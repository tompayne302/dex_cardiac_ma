---
title: "Dexmedetomidine in cardiac surgery meta-analysis"
author: 
  - name: Tom Payne
    affiliation: University of Sydney
date: today
---

```{r setup, include=FALSE}

library(tidyverse)
library(ggplot2)
library(ggpubr)
library(gridExtra)
library(ggrepel)
library(knitr)
library(kableExtra)
library(metafor)
library(lme4)
library(numDeriv)
library(BiasedUrn)
library(glue)
library(gt)
library(meta)
library(gridBase)
library(grid)
library(bayesmeta)
library(dplyr)
library(brms)
library(tidybayes)
library(rjags)
library(ggridges)
library(ggdist)
library(patchwork)
library(janitor)
library(rjags)
library(dmetar)

knitr::opts_chunk$set(echo = F, message = F, warning = F, error = T, 
                      fig.height = 3, out.width = "90%", 
                      dev = "png", dpi = 300, cache = T)

### Set filepaths 
import_path_mac <- '/Users/thomaspayne/Documents/MPhil/cardiac_dex_ma/'
export_path_mac <- '/Users/thomaspayne/Documents/MPhil/cardiac_dex_ma/'

data <- read.csv(paste0(export_path_mac, "Data Extraction Sheet_final-updated 14June23.csv"))

## Change a value for delirium duration that was a data entry error
data$dex_hospitial_days_mean[data$dex_hospitial_days_mean == "123"] <- "13"


## Create a column for total in dex and control group, and order by year
dat <- data %>%
  clean_names() %>%
  filter(!is.na(year)) %>%
  mutate_all(~ str_replace_all(., "\n", "")) %>%
  mutate_all(~ifelse(. == "N/A", "NA", .)) %>%
  mutate(across(contains(c("mortality", "brady", "hypo", "arrhythmia", "age_mean")), as.numeric)) %>%
  rename(dex_n = dex_number_participants,
         dex_del = dex_delirium,
        dex_no_del = dex_delirium_without,
        control_n = control_number_participants,
        control_del = control_delirium_incidence_1,
        control_no_del = control_delirium_incidence_without,
        age_60 = total_age_incl_lower_60,
        dex_dose = dex_maintenance_dose_quantity) %>%
  mutate(control_del_rate = as.numeric(control_del)/as.numeric(control_n),
         control_del_rate_frac = paste0(control_del, "/", control_n),
         dex_del_rate_frac = paste0(dex_del, "/", dex_n),
         dex_mortality = ifelse(is.na(dex_mortality_extended_1), dex_mortality_hospital_1,
                                ifelse(is.na(dex_mortality_hospital_1), dex_mortality_extended_1,
                                       dex_mortality_extended_1 + dex_mortality_hospital_1)),
         dex_no_mortality = ifelse(is.na(dex_mortality_extended_without), dex_mortality_hospital_without,
                                ifelse(is.na(dex_mortality_hospital_without), dex_mortality_extended_without,
                                       dex_mortality_extended_without + dex_mortality_hospital_without)),
         control_mortality = ifelse(is.na(control_mortality_extended_1), control_mortality_hospital_1,
                                ifelse(is.na(control_mortality_hospital_1), control_mortality_extended_1,
                                       control_mortality_extended_1 + control_mortality_hospital_1)),
         control_no_mortality = ifelse(is.na(control_mortality_extended_without), control_mortality_hospital_without,
                                ifelse(is.na(control_mortality_hospital_without), control_mortality_extended_without,
                                       control_mortality_extended_without + control_mortality_hospital_without)),
         control_mortality_rate = control_mortality/control_no_mortality,
         dex_bradycardia = ifelse(is.na(dex_brady_postop_1), dex_brady_intraop_1,
                                ifelse(is.na(dex_brady_intraop_1), dex_brady_postop_1,
                                       dex_brady_postop_1 + dex_brady_intraop_1)),
         dex_no_bradycardia = ifelse(is.na(dex_brady_postop_without), dex_brady_intraop_without,
                                ifelse(is.na(dex_brady_intraop_without), dex_brady_postop_without,
                                       dex_brady_postop_without + dex_brady_intraop_without)),
         control_bradycardia = ifelse(is.na(control_brady_postop_1), control_brady_intraop_1,
                                ifelse(is.na(control_brady_intraop_1), control_brady_postop_1,
                                       control_brady_postop_1 + control_brady_intraop_1)),
         control_no_bradycardia = ifelse(is.na(control_brady_postop_without), control_brady_intraop_without,
                                ifelse(is.na(control_brady_intraop_without), control_brady_postop_without,
                                       control_brady_postop_without + control_brady_intraop_without)),
         control_bradycardia_rate = control_bradycardia/control_no_bradycardia,
         dex_hypotension = ifelse(is.na(dex_hypo_postop_1), dex_hypo_intraop_1,
                                ifelse(is.na(dex_hypo_intraop_1), dex_hypo_postop_1,
                                       dex_hypo_postop_1 + dex_hypo_intraop_1)),
         dex_no_hypotension = ifelse(is.na(dex_hypo_postop_without), dex_hypo_intraop_without,
                                ifelse(is.na(dex_hypo_intraop_without), dex_hypo_postop_without,
                                       dex_hypo_postop_without + dex_hypo_intraop_without)),
         control_hypotension = ifelse(is.na(control_hypo_postop_1), control_hypo_intraop_1,
                                ifelse(is.na(control_hypo_intraop_1), control_hypo_postop_1,
                                       control_hypo_postop_1 + control_hypo_intraop_1)),
         control_no_hypotension = ifelse(is.na(control_hypo_postop_without), control_hypo_intraop_without,
                                ifelse(is.na(control_hypo_intraop_without), control_hypo_postop_without,
                                       control_hypo_postop_without + control_hypo_intraop_without)),
         control_hypotension_rate = control_hypotension/control_no_hypotension,
         dex_arrhythmia = ifelse(is.na(dex_arrhythmia_af_1), dex_arrhythmia_other_1,
                                ifelse(is.na(dex_arrhythmia_other_1), dex_arrhythmia_af_1,
                                       dex_arrhythmia_af_1 + dex_arrhythmia_other_1)),
         dex_no_arrhythmia = ifelse(is.na(dex_arrhythmia_af_without), dex_arrhythmia_other_without,
                                ifelse(is.na(dex_arrhythmia_other_without), dex_arrhythmia_af_without,
                                       dex_arrhythmia_af_without + dex_arrhythmia_other_without)),
         control_arrhythmia = ifelse(is.na(control_arrhythmia_af_1), control_arrhythmia_other_1,
                                ifelse(is.na(control_arrhythmia_other_1), control_arrhythmia_af_1,
                                       control_arrhythmia_af_1 + control_arrhythmia_other_1)),
         control_no_arrhythmia = ifelse(is.na(control_arrhythmia_af_without), control_arrhythmia_other_without,
                                ifelse(is.na(control_arrhythmia_other_without), control_arrhythmia_af_without,
                                       control_arrhythmia_af_without + control_arrhythmia_other_without)),
          control_arrhythmia_rate = control_arrhythmia/control_no_arrhythmia,
         mean_age = (dex_age_mean + control_age_mean)/2) %>%
  mutate(dex_dose = case_when(
    dex_dose == "high" ~ "High",
    dex_dose == "low" ~ "Low",
    TRUE ~ dex_dose  # Keep other values unchanged
  )) %>%
  mutate_at(vars(dex_del, dex_n, dex_no_del, control_del, control_n, control_no_del,
                 dex_delirium_duration_days_mean, dex_delirium_duration_days_sd,
                 control_delirium_duration_days_mean, control_delirium_duration_days_sd,
                 dex_timetoextubation_mean, dex_timetoextubation_sd,
                 control_timetoextubation_mean, control_timetoextubation_sd,
                 dex_icu_days_mean, dex_icu_days_sd, control_icu_days_mean, control_icu_days_sd,
                 dex_hospitial_days_mean, dex_hospitial_days_sd, control_hospitial_days_mean, control_hospitial_days_sd), as.numeric) %>%
  rename_with(~ gsub("hospitial", "hospital", .), contains("hospitial"))

# Now we need to join the risk of bias data
rob <- read.csv(paste0(export_path_mac, "rob.csv"))

rob <- rob %>%
  clean_names() %>%
  filter(!author == "") %>%
  rename(d1 = domain_1_randomisation_process,
         d2 = domain_2_deviations_from_intended_interventions,
         d3 = domain_3_missing_outcome_data,
         d4 = domain_4_measurement_of_outcome,
         d5 = domain_5_selection_of_reported_result,
         overall = overall_r_o_b)

dat <- merge(dat, rob, by = "author", all.x = TRUE) %>%
  mutate_at(vars(d1:overall), ~as.factor(trimws(.))) %>%
  mutate_at(vars(d1:overall), ~recode(., "Low" = "+", "High" = "x", "Some concerns" = "–")) %>%
  mutate_at(vars(d1:overall), as.factor) 

## Delirium incidence outcome
## First, calculate logORs and variance for delirium
dat_del_na_removed <- dat[!is.na(dat$dex_del)&!is.na(dat$control_del),]
IVdat_del_bayesmeta <- escalc(measure="OR", ai=dex_del, bi=dex_no_del, 
                ci=control_del, di=control_no_del, data=dat_del_na_removed, slab=paste(author,year, sep=", "))

  
## First we need to make sure that any duplicate author names are changes
# Create a new column to store the updated author names
IVdat_del_bayesmeta$author_updated <- IVdat_del_bayesmeta$author

# Identify the rows where the author name is duplicated
duplicated_rows <- duplicated(IVdat_del_bayesmeta$author) | duplicated(IVdat_del_bayesmeta$author, fromLast = TRUE)

# Create a suffix vector for duplicated author names
suffix_vec <- rep("", length(IVdat_del_bayesmeta$author))
suffix_vec[duplicated(IVdat_del_bayesmeta$author)] <- ave(IVdat_del_bayesmeta$author, IVdat_del_bayesmeta$author, FUN = function(x) {
  letters[seq_along(x)]
})

# Add suffixes to the duplicated author names
IVdat_del_bayesmeta$author[duplicated_rows] <- paste0(IVdat_del_bayesmeta$author[duplicated_rows], "_", suffix_vec[duplicated_rows])

IVdat_del <- IVdat_del_bayesmeta %>%
  mutate(sei = sqrt(vi),
         author1 = paste(author, year, sep = " ")) %>%
  arrange(year)


## Conduct the primary analysis 
### Proper, uninformative/vague prior for mu: N(0,16))
### Proper, uninformative/vague prior for tau: HN(0,16)
## The above vague priors are based on those given in Albuquerque et al. (JAMA 2022)

priors_primary <- brms::prior(normal(0,0.82), class = b, coef = "Intercept") +
            brms::prior(cauchy(0,0.5), class = sd)

priors_metareg <- brms::prior(normal(0, 1), class = b, coef = "Intercept") +
            brms::prior(cauchy(0,0.5), class = sd) +
            brms::prior(normal(0,0.82), class = b)

  # Run the brms model
m.brm <- brm(yi | se(sei) ~ 0 + Intercept + (1 | author1),
             data = IVdat_del,
             prior = priors_primary,
              iter = 4000,
              backend = "cmdstanr", 
              cores = parallel::detectCores(),
              chains = 4,
              seed = 123)

m.brm_excluding_decade <- update(m.brm,
                                 newdata = IVdat_del %>% filter(!author == "Turan"))

## Create metaregression models
## All use the same priors as above. Common heterogeneity parameters estimated

## Construct metaregression model for dose

m.brm_dose_diff <- update(m.brm, formula. = ~ . + dex_dose, newdata = IVdat_del, prior = priors_metareg)
m.brm_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_del, prior = priors_metareg)

## Metaregression for mean age (dichotomous, <60 or >60)
m.brm_age_diff <- update(m.brm, formula. = ~ . + age_60, newdata = IVdat_del, prior = priors_metareg)
m.brm_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_del, prior = priors_metareg)

## Then, do metaregression for mean age (continuous)
m.brm_age_cont <- update(m.brm, formula. = ~ . + mean_age, newdata = IVdat_del, prior = priors_metareg)

## Surgery type
m.brm_dose_timing <- update(m.brm, formula. = ~ . - Intercept + dex_dose_timing, newdata = IVdat_del, prior = priors_metareg)

# Secondary outcomes
## Mortality outcome
dat_mortality_na_removed <- dat[!is.na(dat$dex_mortality)&!is.na(dat$control_mortality),]
IVdat_mortality <- escalc(measure="OR", ai=dex_mortality, bi=dex_no_mortality, 
                ci=control_mortality, di=control_no_mortality, data=dat_mortality_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
            mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_mortality <- update(m.brm, newdata = IVdat_mortality)

## Bradycardia outcome
dat_bradycardia_na_removed <- dat[!is.na(dat$dex_bradycardia)&!is.na(dat$control_bradycardia),]
IVdat_bradycardia <- escalc(measure="OR", ai=dex_bradycardia, bi=dex_no_bradycardia, 
                ci=control_bradycardia, di=control_no_bradycardia, data=dat_bradycardia_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
                    mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_bradycardia <-  update(m.brm, newdata = IVdat_bradycardia)

## hypotension outcome
dat_hypotension_na_removed <- dat[!is.na(dat$dex_hypotension)&!is.na(dat$control_hypotension),]
IVdat_hypotension <- escalc(measure="OR", ai=dex_hypotension, bi=dex_no_hypotension, 
                ci=control_hypotension, di=control_no_hypotension, data=dat_hypotension_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
            mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_hypotension <-  update(m.brm, newdata = IVdat_hypotension)

## arrythmia outcome
dat_arrhythmia_na_removed <- dat[!is.na(dat$dex_arrhythmia)&!is.na(dat$control_arrhythmia),]
IVdat_arrhythmia <- escalc(measure="OR", ai=dex_arrhythmia, bi=dex_no_arrhythmia, 
                ci=control_arrhythmia, di=control_no_arrhythmia, data=dat_arrhythmia_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
            mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_arrhythmia <-  update(m.brm, newdata = IVdat_arrhythmia)

## delirium duration outcome
dat_del_duration_na_removed <- dat[!is.na(dat$dex_delirium_duration_days_mean)&!is.na(dat$control_delirium_duration_days_mean)
                                   &!is.na(dat$dex_delirium_duration_days_sd)&!is.na(dat$control_delirium_duration_days_sd),]

IVdat_del_duration <- escalc(measure="MD", m1i=dex_delirium_duration_days_mean, sd1i=dex_delirium_duration_days_sd, n1i = dex_del,
                m2i=control_delirium_duration_days_mean, sd2i=control_delirium_duration_days_sd, n2i = control_del,
                data=dat_del_duration_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
            mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_del_duration <-  update(m.brm, newdata = IVdat_del_duration)

## time to extubation outcome
dat_extub_time_na_removed <- dat[!is.na(dat$dex_timetoextubation_mean)&!is.na(dat$control_timetoextubation_mean)
                                 &!is.na(dat$dex_timetoextubation_sd)&!is.na(dat$control_timetoextubation_sd),]
IVdat_extub_time <- escalc(measure="MD", m1i=dex_timetoextubation_mean, sd1i=dex_timetoextubation_sd, n1i = dex_n,
                m2i=control_timetoextubation_mean, sd2i=control_timetoextubation_sd, n2i = control_n,
                data=dat_extub_time_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
            mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_extub_time <-  update(m.brm, newdata = IVdat_extub_time)

## hospital stay outcome
dat_hosp_stay_na_removed <- dat[!is.na(dat$dex_hospital_days_mean)&!is.na(dat$control_hospital_days_mean)
                                 &!is.na(dat$dex_hospital_days_sd)&!is.na(dat$control_hospital_days_sd),]
IVdat_hosp_stay <- escalc(measure="MD", m1i=dex_hospital_days_mean, sd1i=dex_hospital_days_sd, n1i = dex_n,
                m2i=control_hospital_days_mean, sd2i=control_hospital_days_sd, n2i = control_n,
                data=dat_hosp_stay_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
            mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_hosp_stay <-  update(m.brm, newdata = IVdat_hosp_stay)

## ICU stay outcome
dat_icu_stay_na_removed <- dat[!is.na(dat$dex_icu_days_mean)&!is.na(dat$control_icu_days_mean)
                                 &!is.na(dat$dex_icu_days_sd)&!is.na(dat$control_icu_days_sd),]
IVdat_icu_stay <- escalc(measure="MD", m1i=dex_icu_days_mean, sd1i=dex_icu_days_sd, n1i = dex_n,
                m2i=control_icu_days_mean, sd2i=control_icu_days_sd, n2i = control_n,
                data=dat_icu_stay_na_removed, 
                slab=paste(author,year, sep=", ")) %>%
            mutate(sei = sqrt(vi),
                   author1 = paste(author, year, sep = " "))

m.brm_icu_stay <-  update(m.brm, newdata = IVdat_icu_stay)

# Excluding high RoB
m.brm_low_rob <- update(m.brm, newdata = IVdat_del %>% filter(!overall == "x"))

m.brm_age_low_rob <-  update(m.brm_age, newdata = IVdat_del %>% filter(!overall == "x"))
m.brm_dose_low_rob <-  update(m.brm_dose, newdata = IVdat_del %>% filter(!overall == "x"))

## Now to run the sensitivity analysis models
## Create priors for informative and uniform
informative_priors <- brms::prior(normal(0,0.82), class = Intercept) +
            brms::prior(cauchy(0,0.5), class = sd)

vague_priors <- brms::prior(normal(0,4), class = Intercept) +
            brms::prior(cauchy(0,4), class = sd)

turner_etal_priors <- brms::prior(normal(0,0.82), class = Intercept) +
            brms::prior(lognormal(-2.49,1.52), class = sd)

# Now we need to create a series of functions to avoid repeating code for each subgroup
## First, the brm function

m.brm_informative <- brm(yi | se(sei) ~ 1 + (1 | author1),
               data = IVdat_del,
               prior = informative_priors,
               iter = 4000,
               backend = "cmdstanr", 
               cores = parallel::detectCores(),
                chains = 4,
                seed = 123)

m.brm_vague <- update(m.brm_informative, prior = vague_priors)

m.brm_turneretal <- update(m.brm_informative, prior = turner_etal_priors)

```

# Introduction

At long last the data analysis is here! Tess, before diving into this I would read [Kruschke and Liddell](https://link.springer.com/article/10.3758/s13423-016-1221-4) if you're not across the whole frequentist vs. Bayesian debate. Most of what is below will not make sense in the absence of an understanding of the difference between frequentist and Bayesian approaches to data analysis.

# Overall forest plot

Below in @fig-forest-all-studies and @fig-forest-low-rob I show the forest plot of all studies, and excluding studies at high risk of bias, respectively.

The purple lines show the assumed distribution of with-in study effects; that is, in meta-analysis we assume that each study's observed effect $\hat{\theta}$ is an estimate of the true effect in that trial population ${\theta}$, and the uncertainty in this estimate is modeled with a normal distribution with mean ${\theta}$ and standard deviation ${\sigma}$: $N({\theta}, {\sigma})$.

The blue curves show the posterior shrinkage estimates of each study; that is, each study's effect is 'shrunk' towards the mean when viewed in light of the other data. [Kruschke and Liddell](https://link.springer.com/article/10.3758/s13423-016-1221-4) explain this best:

> "Hierarchical models are especially useful because the low-level and high-level parameters are estimated simultaneously and are mutually constraining. When data from many low-level units inform the high-level distribution, the high-level distribution constrains the low-level parameters to be mutually consistent. This causes the more extreme low-level cases to be"shrunken" toward the mode(s) of the group. Shrinkage helps prevent false alarms caused by random conspiracies of rogue outlying data. Essentially, the data from the other individuals are acting as simultaneous prior information to rein in estimates of outlying individuals."

The orange line shows the 95%CrI for the prediction interval: the range of values that would be likely to be observed be a future study of this research question. Note this is different to the *credible interval*, which is the range of plausible values for the **mean** effect size. Prediction intervals are perhaps the best summary of heterogeneity because they provide a practical interpretation: what is the range of plausible values I might observe in different patient populations?

On the right hand side we have the risk of bias assessments for each study: the five domains, and the overall assessment. '+' indicates 'Low risk', '-' indicates 'Some concerns', and 'x' indicates 'High risk'.

::: {#forest-plots .panel-tabset style="colour: pink"}
## All studies

```{r}
#| label: fig-forest-all-studies
#| fig-width: 15
#| fig-height: 7
#| fig-cap: |
#|   Forest plot of all studies.

# Calculate the re-weighted estimated of each study
study.draws <- spread_draws(m.brm, r_author1[author1, ], b_Intercept) %>%
    mutate(b_Intercept = r_author1 + b_Intercept)

  # Establish the pooled result
pooled.effect.draws <- spread_draws(m.brm, b_Intercept) %>%
    mutate(author1 = "Pooled Effect")

  # Combine the pooled result with study estimates, then play around with the lexicon (bmrs mucks this up)
forest.data <- bind_rows(study.draws,
                           pooled.effect.draws) %>%
    ungroup() %>%
    mutate(author1 = str_replace_all(author1, "[.]", " "),
           author1 = reorder(author1, b_Intercept))
  
  # Calculate median qi based on author1
forest.data.summary1 <- group_by(forest.data, author1) %>%
    median_qi(b_Intercept) 
  
  # Join the two dataframes based on similar author names
forest.data.summary2 <- forest.data.summary1 %>%
    mutate(author_word = str_extract(author1, "^\\S+")) %>%
    left_join(IVdat_del %>% 
                mutate(author_word = str_extract(author1, "^\\S+")) %>%
                select(author1, yi, vi, control_del_rate_frac, dex_del_rate_frac, dex_del,
                       dex_n, control_del, control_n, d1:overall, author_word), 
              by = "author_word") %>%
    select(-author_word, -author1.y) %>%
    mutate(dex_del_rate_frac = ifelse(author1.x == "Pooled Effect",
                                    paste0(sum(IVdat_del$dex_del), "/", sum(IVdat_del$dex_n)),
                                    dex_del_rate_frac),
         control_del_rate_frac = ifelse(author1.x == "Pooled Effect",
                                        paste0(sum(IVdat_del$control_del), "/", sum(IVdat_del$control_n)),
                                        control_del_rate_frac))
  
forest.data.summary <- forest.data.summary2 %>% 
    rename(author1 = author1.x)

# Create the prediction interval
nd = data.frame(author1 = "new", sei = 0)
  
pred_summ <- brms::posterior_predict(object = m.brm,
                          newdata = nd,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_df <- pred_summ |> 
             data.frame() |>
             ggdist::median_hdi(.width = c(.66, .8, .95)) |>
            rename(coeff = 1)

pred_df$author1 <- as.factor(c("Pooled Effect"))
pred_df <- pred_df[c(7, 1:3)] %>%
  rename(pi_median = coeff,
         pi_lower = .lower,
         pi_upper = .upper)

# Extract tau and 95%CI
tau_posterior = 
    m.brm |> 
    tidybayes::tidy_draws() |> 
    ggdist::median_hdi(sd_author1__Intercept)
  
  ## Create the values for shrinkage and actual effect estimates
res_plot_pre <- forest.data.summary %>%
  mutate(weighted_effect = paste0(sprintf('%.2f', exp(b_Intercept)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']'),
         unweighted_effect = paste0(sprintf('%.2f', exp(yi)), 
                  ' [', sprintf('%.2f', exp(yi - 1.96*sqrt(vi))),
                  ', ', sprintf('%.2f', exp(yi + 1.96*sqrt(vi))), ']')) %>%
   mutate(unweighted_effect = ifelse(unweighted_effect == "NA [NA, NA]", 
                                   paste0("τ = ", 
                                          sprintf('%.2f', tau_posterior$sd_author1__Intercept), 
                                          " [", sprintf('%.2f', tau_posterior$.lower), ", ",
                                          sprintf('%.2f', tau_posterior$.upper), "]"),
                                          unweighted_effect))


new_row <- data.frame(author1 = as.factor("Study"),
                      weighted_effect = "[95%CrI]",
                      unweighted_effect = "[95%CrI]",
                      dex_del_rate_frac = "Delirium (n/total)",
                      control_del_rate_frac = "Delirium (n/total)",
                      d1 = "D1",
                      d2 = "D2",
                      d3 = "D3",
                      d4 = "D4",
                      d5 = "D5",
                      overall = "Overall")

res_plot <- bind_rows(res_plot_pre, new_row) %>%
          mutate(color = ifelse(author1 == "Pooled Effect", "grey60", "black"),
                 font = ifelse(author1 == "Pooled Effect", "bold", "plain"))

## Add an extra row to res_plot_control because we need more space at the top
new_row_res_plot <- data.frame(author1 = as.factor(""),
                      weighted_effect = "Shrinkage OR",
                      unweighted_effect = "Observed OR",
                      dex_del_rate_frac = "Dexmedetomidine",
                      control_del_rate_frac = "Control",
                      d1 = "",
                      d2 = "",
                      d3 = "",
                      d4 = "",
                      d5 = "",
                      overall = "",
                      color = "black",
                      font = "plain")

res_plot_overall <- bind_rows(res_plot, new_row_res_plot)

# Now - plotting!
p_forest_overall <- ggplot(aes(exp(b_Intercept), 
           relevel(author1, "Pooled Effect", after=Inf),  alpha = 0.9), 
       data = forest.data) +
  # Add vertical lines for pooled effect and CI
  geom_vline(xintercept = exp(fixef(m.brm)[1, 1]), 
             color = "grey60", size = 1) +
  geom_vline(xintercept = exp(fixef(m.brm)[1, 3:4]), 
             color = "grey60", linetype = 2) +
  geom_vline(xintercept = 1, color = "black", 
             size = 0.7) +
  # Add densities
  stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
    stat_slab(aes(xdist = exp(distributional::dist_normal(mean = yi, 
                          sd = sqrt(vi)))),
                      data = forest.data.summary,
              fill = NA, color = "purple",
              slab_linewidth = 0.5, alpha = 0.6) +
    geom_pointintervalh(aes(xmin = exp(pi_lower), 
                          xmax = exp(pi_upper), 
                          x = exp(pi_median)),
                      position = position_nudge(y = -0.3),
                      data = pred_df, 
                      col = "darkorange",
                      size = 6) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,19)) +
  annotate("text", x = 0.35, y =18.5, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 3, y = 18.5,  label = "Favours\ncontrol") +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Odds ratio (log scale)") +
  ylab(NULL) +
  guides(alpha = "none")

p_estimates_overall <- ggplot(aes(y = relevel(author1, "Pooled Effect", after=Inf), color = color), 
                   data = res_plot_overall) +
  geom_text(aes(x = 0, label = weighted_effect), hjust = 0,
            fontface = ifelse(grepl("Shrinkage OR|\\[95%CrI\\]", res_plot_overall$weighted_effect), "bold", "plain")) +
  geom_text(aes(x = 1, label = unweighted_effect), hjust = 0, 
            fontface = ifelse(grepl("Observed OR|\\[95%CrI\\]", res_plot_overall$unweighted_effect), "bold", "plain")) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 2), ylim=c(1,19))

# Map the fill and color aesthetics to the new column in the ggplot call
p_studies_overall <- ggplot(aes(y = relevel(author1, "Pooled Effect", after=Inf), color = color), 
                          data = res_plot_overall) +
  geom_text(aes(x = 0, label = author1), hjust = 0, 
            fontface = ifelse(res_plot_overall$author1 == "Study", "bold",
                              ifelse(res_plot_overall$author1 == "Pooled Effect", "italic","plain"))) +
  geom_text(aes(x = 2, label = dex_del_rate_frac), hjust = 0, 
            fontface = ifelse(res_plot_overall$dex_del_rate_frac == "Delirium (n/total)" | res_plot_overall$dex_del_rate_frac == "Dexmedetomidine", "bold", "plain")) +
  geom_text(aes(x = 4, label = control_del_rate_frac), hjust = 0, 
            fontface = ifelse(res_plot_overall$control_del_rate_frac == "Delirium (n/total)" | res_plot_overall$control_del_rate_frac == "Control", "bold", "plain")) +
  scale_color_identity() + # Use the specified colors directly for text color
  theme_void() +
  coord_cartesian(xlim = c(0, 6))



rob_plot <- ggplot(aes(y = relevel(author1, "Pooled Effect", after=Inf)), data = res_plot_overall) +
  geom_text(aes(x = 0, label = d1, color = ifelse(d1 == "+", "green",
                                           ifelse(d1 == "–", "yellow3", 
                                           ifelse(d1 == "x", "red", 
                                           "black")))), hjust = 0,
            fontface = ifelse(res_plot_overall$d1 == "D1", "bold", "plain"),
            size = ifelse(res_plot_overall$d1 == "D1", 4, 8)) +
  geom_text(aes(x = 0.5, label = d2, color = ifelse(d2 == "+", "green",
                                           ifelse(d2 == "–", "yellow3", 
                                           ifelse(d2 == "x", "red", 
                                           "black")))), hjust = 0,
            fontface = ifelse(res_plot_overall$d2 == "D2", "bold", "plain"),
            size = ifelse(res_plot_overall$d2 == "D2", 4, 8)) +
    geom_text(aes(x = 1, label = d3, color = ifelse(d3 == "+", "green",
                                           ifelse(d3 == "–", "yellow3", 
                                           ifelse(d3 == "x", "red", 
                                           "black")))), hjust = 0,
            fontface = ifelse(res_plot_overall$d3 == "D3", "bold", "plain"),
            size = ifelse(res_plot_overall$d3 == "D3", 4, 8)) +
    geom_text(aes(x = 1.5, label = d4, color = ifelse(d4 == "+", "green",
                                           ifelse(d4 == "–", "yellow3", 
                                           ifelse(d4 == "x", "red", 
                                           "black")))), hjust = 0,
            fontface = ifelse(res_plot_overall$d4 == "D4", "bold", "plain"),
            size = ifelse(res_plot_overall$d4 == "D4", 4, 8)) +
    geom_text(aes(x = 2, label = d5, color = ifelse(d5 == "+", "green",
                                           ifelse(d5 == "–", "yellow3", 
                                           ifelse(d5 == "x", "red", 
                                           "black")))), hjust = 0,
            fontface = ifelse(res_plot_overall$d5 == "D5", "bold", "plain"),
            size = ifelse(res_plot_overall$d5 == "D5", 4, 8)) +
   geom_text(aes(x = 2.5, label = overall, color = ifelse(overall == "+", "green",
                                           ifelse(overall == "–", "yellow3", 
                                           ifelse(overall == "x", "red", 
                                           "black")))), hjust = 0,
            fontface = ifelse(res_plot_overall$overall == "Overall", "bold", "plain"),
            size = ifelse(res_plot_overall$overall == "Overall", 4, 8)) +
  scale_color_manual(values = c("black", "green", "red", "yellow3"),
                     labels = c("Domain", "Low", "High", "Some concerns"),
                     name = "Legend") +
  theme_void() +
  theme(legend.position = "none") +
  coord_cartesian(xlim = c(0, 3))

library(patchwork)

layout <- c(
  area(t = 0, l = 0, b = 12, r = 30),
  area(t = 0, l = 30, b = 12, r = 47), 
  area(t = 0, l = 49, b = 12, r = 70),
  area(t = 0, l = 70, b = 12, r = 90))

p_studies_overall + p_forest_overall + p_estimates_overall + rob_plot + plot_layout(design = layout)


```

## Excluding studies at high risk of bias

```{r}
#| label: fig-forest-low-rob
#| fig-width: 15
#| fig-height: 6
#| fig-cap: |
#|   Forest plot excluding studies at high risk of bias.



# Calculate the re-weighted estimated of each study
study.draws <- spread_draws(m.brm_low_rob, r_author1[author1, ], b_Intercept) %>%
    mutate(b_Intercept = r_author1 + b_Intercept)

  # Establish the pooled result
pooled.effect.draws <- spread_draws(m.brm_low_rob, b_Intercept) %>%
    mutate(author1 = "Pooled Effect")

  # Combine the pooled result with study estimates, then play around with the lexicon (bmrs mucks this up)
forest.data <- bind_rows(study.draws,
                           pooled.effect.draws) %>%
    ungroup() %>%
    mutate(author1 = str_replace_all(author1, "[.]", " "),
           author1 = reorder(author1, b_Intercept))
  
  # Calculate median qi based on author1
forest.data.summary1 <- group_by(forest.data, author1) %>%
    median_qi(b_Intercept) 
  
  # Join the two dataframes based on similar author names
forest.data.summary2 <- forest.data.summary1 %>%
    mutate(author_word = str_extract(author1, "^\\S+")) %>%
    left_join(IVdat_del %>%
                filter(!overall == "x") %>%
                mutate(author_word = str_extract(author1, "^\\S+")) %>%
                select(author1, yi, vi, control_del_rate_frac, dex_del_rate_frac, dex_del,
                       dex_n, control_del, control_n, d1:overall, author_word), 
              by = "author_word") %>%
    select(-author_word, -author1.y) %>%
    mutate(dex_del_rate_frac = ifelse(author1.x == "Pooled Effect",
                                    paste0(sum(IVdat_del$dex_del), "/", sum(IVdat_del$dex_n)),
                                    dex_del_rate_frac),
         control_del_rate_frac = ifelse(author1.x == "Pooled Effect",
                                        paste0(sum(IVdat_del$control_del), "/", sum(IVdat_del$control_n)),
                                        control_del_rate_frac))
  
forest.data.summary <- forest.data.summary2 %>% 
    rename(author1 = author1.x)

# Create the prediction interval
nd = data.frame(author1 = "new", sei = 0)
  
pred_summ <- brms::posterior_predict(object = m.brm_low_rob,
                          newdata = nd,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_df <- pred_summ |> 
             data.frame() |>
             ggdist::median_hdi(.width = c(.66, .8, .95)) |>
            rename(coeff = 1)

pred_df$author1 <- as.factor(c("Pooled Effect"))
pred_df <- pred_df[c(7, 1:3)] %>%
  rename(pi_median = coeff,
         pi_lower = .lower,
         pi_upper = .upper)

# Extract tau and 95%CI
tau_posterior = 
    m.brm_low_rob |> 
    tidybayes::tidy_draws() |> 
    ggdist::median_hdi(sd_author1__Intercept)
  
  ## Create the values for shrinkage and actual effect estimates
res_plot_pre <- forest.data.summary %>%
  mutate(weighted_effect = paste0(sprintf('%.2f', exp(b_Intercept)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']'),
         unweighted_effect = paste0(sprintf('%.2f', exp(yi)), 
                  ' [', sprintf('%.2f', exp(yi - 1.96*sqrt(vi))),
                  ', ', sprintf('%.2f', exp(yi + 1.96*sqrt(vi))), ']')) %>%
   mutate(unweighted_effect = ifelse(unweighted_effect == "NA [NA, NA]", 
                                   paste0("τ = ", 
                                          sprintf('%.2f', tau_posterior$sd_author1__Intercept), 
                                          " [", sprintf('%.2f', tau_posterior$.lower), ", ",
                                          sprintf('%.2f', tau_posterior$.upper), "]"),
                                          unweighted_effect))

new_row <- data.frame(author1 = as.factor("Study"),
                      weighted_effect = "[95%CrI]",
                      unweighted_effect = "[95%CrI]",
                      dex_del_rate_frac = "Delirium (n/total)",
                      control_del_rate_frac = "Delirium (n/total)",
                      d1 = "D1",
                      d2 = "D2",
                      d3 = "D3",
                      d4 = "D4",
                      d5 = "D5",
                      overall = "Overall")

res_plot <- bind_rows(res_plot_pre, new_row) %>%
          mutate(color = ifelse(author1 == "Pooled Effect", "grey60", "black"),
                 font = ifelse(author1 == "Pooled Effect", "bold", "plain"))

## Add an extra row to res_plot_control because we need more space at the top
new_row_res_plot <- data.frame(author1 = as.factor(""),
                      weighted_effect = "Shrinkage OR",
                      unweighted_effect = "Observed OR",
                      dex_del_rate_frac = "Dexmedetomidine",
                      control_del_rate_frac = "Control",
                      d1 = "",
                      d2 = "",
                      d3 = "",
                      d4 = "",
                      d5 = "",
                      overall = "",
                      color = "black",
                      font = "plain")

res_plot_overall <- bind_rows(res_plot, new_row_res_plot)

# Now - plotting!
p_forest_overall <- ggplot(aes(exp(b_Intercept), 
           relevel(author1, "Pooled Effect", after=Inf),  alpha = 0.9), 
       data = forest.data) +
  # Add vertical lines for pooled effect and CI
  geom_vline(xintercept = exp(fixef(m.brm_low_rob)[1, 1]), 
             color = "grey60", size = 1) +
  geom_vline(xintercept = exp(fixef(m.brm_low_rob)[1, 3:4]), 
             color = "grey60", linetype = 2) +
  geom_vline(xintercept = 1, color = "black", 
             size = 0.7) +
  # Add densities
  stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
    stat_slab(aes(xdist = exp(distributional::dist_normal(mean = yi, 
                          sd = sqrt(vi)))),
                      data = forest.data.summary,
              fill = NA, color = "purple",
              slab_linewidth = 0.5, alpha = 0.6) +
    geom_pointintervalh(aes(xmin = exp(pi_lower), 
                          xmax = exp(pi_upper), 
                          x = exp(pi_median)),
                      position = position_nudge(y = -0.3),
                      data = pred_df, 
                      col = "darkorange",
                      size = 6) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,14)) +
  annotate("text", x = 0.35, y =13.5, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 3, y = 13.5,  label = "Favours\ncontrol") +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Odds ratio (log scale)") +
  ylab(NULL) +
  guides(alpha = "none")

p_estimates_overall <- ggplot(aes(y = relevel(author1, "Pooled Effect", after=Inf), color = color), 
                   data = res_plot_overall) +
  geom_text(aes(x = 0, label = weighted_effect), hjust = 0,
            fontface = ifelse(grepl("Shrinkage OR|\\[95%CrI\\]", res_plot_overall$weighted_effect), "bold", "plain")) +
  geom_text(aes(x = 1, label = unweighted_effect), hjust = 0, 
            fontface = ifelse(grepl("Observed OR|\\[95%CrI\\]", res_plot_overall$unweighted_effect), "bold", "plain")) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 2))

# Map the fill and color aesthetics to the new column in the ggplot call
p_studies_overall <- ggplot(aes(y = relevel(author1, "Pooled Effect", after=Inf), color = color), 
                          data = res_plot_overall) +
  geom_text(aes(x = 0, label = author1), hjust = 0, 
            fontface = ifelse(res_plot_overall$author1 == "Study", "bold",
                              ifelse(res_plot_overall$author1 == "Pooled Effect", "italic","plain"))) +
  geom_text(aes(x = 2, label = dex_del_rate_frac), hjust = 0, 
            fontface = ifelse(res_plot_overall$dex_del_rate_frac == "Delirium (n/total)" | res_plot_overall$dex_del_rate_frac == "Dexmedetomidine", "bold", "plain")) +
  geom_text(aes(x = 4, label = control_del_rate_frac), hjust = 0, 
            fontface = ifelse(res_plot_overall$control_del_rate_frac == "Delirium (n/total)" | res_plot_overall$control_del_rate_frac == "Control", "bold", "plain")) +
  scale_color_identity() + # Use the specified colors directly for text color
  theme_void() +
  coord_cartesian(xlim = c(0, 6))

rob_plot <- ggplot(aes(y = relevel(author1, "Pooled Effect", after=Inf)), data = res_plot_overall) +
  geom_text(aes(x = 0, label = d1, color = ifelse(d1 == "+", "green",
                                           ifelse(d1 == "–", "yellow3", 
                                           "black"))), hjust = 0,
            fontface = ifelse(res_plot_overall$d1 == "D1", "bold", "plain"),
            size = ifelse(res_plot_overall$d1 == "D1", 4, 8)) +
  geom_text(aes(x = 0.5, label = d2, color = ifelse(d2 == "+", "green",
                                           ifelse(d2 == "–", "yellow3",
                                           "black"))), hjust = 0,
            fontface = ifelse(res_plot_overall$d2 == "D2", "bold", "plain"),
            size = ifelse(res_plot_overall$d2 == "D2", 4, 8)) +
    geom_text(aes(x = 1, label = d3, color = ifelse(d3 == "+", "green",
                                           ifelse(d3 == "–", "yellow3", 
                                           "black"))), hjust = 0,
            fontface = ifelse(res_plot_overall$d3 == "D3", "bold", "plain"),
            size = ifelse(res_plot_overall$d3 == "D3", 4, 8)) +
    geom_text(aes(x = 1.5, label = d4, color = ifelse(d4 == "+", "green",
                                           ifelse(d4 == "–", "yellow3", 
                                           "black"))), hjust = 0,
            fontface = ifelse(res_plot_overall$d4 == "D4", "bold", "plain"),
            size = ifelse(res_plot_overall$d4 == "D4", 4, 8)) +
    geom_text(aes(x = 2, label = d5, color = ifelse(d5 == "+", "green",
                                           ifelse(d5 == "–", "yellow3", 
                                           "black"))), hjust = 0,
            fontface = ifelse(res_plot_overall$d5 == "D5", "bold", "plain"),
            size = ifelse(res_plot_overall$d5 == "D5", 4, 8)) +
   geom_text(aes(x = 2.5, label = overall, color = ifelse(overall == "+", "green",
                                           ifelse(overall == "–", "yellow3", 
                                           "black"))), hjust = 0,
            fontface = ifelse(res_plot_overall$overall == "Overall", "bold", "plain"),
            size = ifelse(res_plot_overall$overall == "Overall", 4, 8)) +
  scale_color_manual(values = c("black", "green", "yellow3"),
                     labels = c("Domain", "Low", "Some concerns"),
                     name = "Legend") +
  theme_void() +
  theme(legend.position = "none") +
  coord_cartesian(xlim = c(0, 3))

library(patchwork)

layout <- c(
  area(t = 0, l = 0, b = 12, r = 30),
  area(t = 0, l = 30, b = 12, r = 47), 
  area(t = 0, l = 49, b = 12, r = 70),
  area(t = 0, l = 70, b = 12, r = 90))

p_studies_overall + p_forest_overall + p_estimates_overall + rob_plot + plot_layout(design = layout)

```
:::

# Posterior probability plots

Now let's look at the posteriors a bit more closely. We are essentially zooming in on the 'Pooled Effect' curves from the above forest plots.

For 𝛕, I present dotted lines to represent traditional cutoffs for heterogeneity: Low (𝛕 \< 0.1), reasonable (0.1 \< 𝛕 \< 0.5), fairly high (0.5 \< 𝛕 \< 1.0), and farily extreme (𝛕 \> 1.0).

I also present our primary priors for 𝝁 and 𝛕, $N(0, 0.82)$ and $Cauchy(0, 0.5)$, respectively.

@fig-posteriors-all shows the curves, with tabs for all studies and excluding studies at high risk of bias.

::: {#posterior-probs .panel-tabset style="colour: purple"}
## All studies

```{r}
#| label: fig-posteriors-all
#| fig-width: 10
#| fig-height: 7
#| fig-cap: |
#|   Posteriors of A) mean effect and B) heterogeneity across all studies.


post.samples <- as_draws_df(m.brm, c("b_Intercept", "sd_author1__Intercept"))

mu_prior_plot = data.frame(prior = exp(distributional::dist_normal(mean = 0, sd = 0.82)))

mu_df <- m.brm %>%
  spread_draws(b_Intercept) %>%
  median_qi(.width = c(.66, .8, .95))

mu <- ggplot(aes(x = exp(b_Intercept), alpha = 1), data = post.samples) +
  stat_slab(aes(fill = after_stat(level)),
                .width = c(.66, .80, .95, 1), position = "dodgejust") +
  scale_fill_brewer(na.translate = FALSE, name = "Probability") +
  geom_pointintervalh(aes(xmin = exp(.lower), 
                          xmax = exp(.upper), 
                          x = exp(b_Intercept)),
                      data = mu_df, 
                      col = "black", alpha = 1) +
  stat_slab(aes(xdist = prior), data = mu_prior_plot, fill = NA, color = "grey",  inherit.aes = FALSE) +
  scale_x_log10(breaks = c(0.25, 0.5, 1, 2,4), expand = c(0, 0)) + 
  ggdist::scale_thickness_shared() +
  coord_cartesian(xlim=c(0.25, 4), ylim = c(0,1)) +
  geom_vline(xintercept = 1, linetype = "dashed") +
  annotate("text", x = 0.35, y = 0.8, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 2, y = 0.8,  label = "Favours\ncontrol") +
  annotate("text", x = 2, y = 0.2, label = "Mean effect prior", fontface = "bold", colour = "grey30", angle=-8) +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Odds ratio (log scale)", title = "A: Mean effect") +
  ylab(NULL) +
  guides(alpha = "none")

tau_prior_plot = data.frame(prior = distributional::dist_cauchy(location = 0, scale = 0.5))

tau <- ggplot(aes(x = sd_author1__Intercept, alpha = 1), data = post.samples) +
  stat_slab(aes(fill = after_stat(level)),
                .width = c(.66, .80, .95,  1), position = "dodgejust") +
  stat_pointinterval(.width = c(.66, .8, .95), alpha = 1) +
  stat_slab(aes(xdist = prior), 
                data = tau_prior_plot, fill = NA, color = "grey", inherit.aes = FALSE) +
  ggdist::scale_thickness_shared() +
  scale_fill_brewer(na.translate = FALSE, name = "Probability") +
  geom_vline(xintercept = 0.1, linetype = "dashed") +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  geom_vline(xintercept = 1, linetype = "dashed") +
  annotate("text", x = 0.05, y = 0.98, label = "Low", fontface = "bold") +
  annotate("text", x = 0.3, y = 0.98, label = "Reasonable", fontface = "bold") +
  annotate("text", x = 0.75, y = 0.98, label = "Fairly high", fontface = "bold") +
  annotate("text", x = 1.1, y = 0.98, label = "Fairly extreme", fontface = "bold") +
  annotate("text", x = 1, y = 0.1, label = "Heterogeneity prior", fontface = "bold", colour = "grey30", angle = -2) +
  scale_x_continuous(breaks = c(0, 0.1, 0.25, 0.5, 0.75, 1, 1.2), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0, 1.2), ylim = c(0,1)) +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Log odds ratio", title = "B: Heterogeneity") +
  ylab(NULL) +
  guides(alpha = "none")

grid.arrange(mu, tau, ncol = 1)

```

## Excluding studies at high risk of bias

```{r}
#| label: fig-posteriors-low-rob
#| fig-width: 10
#| fig-height: 7
#| fig-cap: |
#|   Posteriors of A) mean effect and B) heterogeneity when excluding studies at high risk of bias.


post.samples <- as_draws_df(m.brm_low_rob, c("b_Intercept", "sd_author1__Intercept"))

mu_prior_plot = data.frame(prior = exp(distributional::dist_normal(mean = 0, sd = 0.82)))

mu_df <- m.brm_low_rob %>%
  spread_draws(b_Intercept) %>%
  median_qi(.width = c(.66, .8, .95))

mu <- ggplot(aes(x = exp(b_Intercept), alpha = 1), data = post.samples) +
  stat_slab(aes(fill = after_stat(level)),
                .width = c(.66, .80, .95, 1), position = "dodgejust") +
  scale_fill_brewer(na.translate = FALSE, name = "Probability") +
  geom_pointintervalh(aes(xmin = exp(.lower), 
                          xmax = exp(.upper), 
                          x = exp(b_Intercept)),
                      data = mu_df, 
                      col = "black", alpha = 1) +
  stat_slab(aes(xdist = prior), data = mu_prior_plot, fill = NA, color = "grey",  inherit.aes = FALSE) +
  scale_x_log10(breaks = c(0.25, 0.5, 1, 2,4), expand = c(0, 0)) + 
  ggdist::scale_thickness_shared() +
  coord_cartesian(xlim=c(0.25, 4), ylim = c(0,1)) +
  geom_vline(xintercept = 1, linetype = "dashed") +
  annotate("text", x = 0.35, y = 0.8, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 2, y = 0.8,  label = "Favours\ncontrol") +
  annotate("text", x = 2, y = 0.2, label = "Mean effect prior", fontface = "bold", colour = "grey30", angle=-8) +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Odds ratio (log scale)", title = "A: Mean effect") +
  ylab(NULL) +
  guides(alpha = "none")

tau_prior_plot = data.frame(prior = distributional::dist_cauchy(location = 0, scale = 0.5))

tau <- ggplot(aes(x = sd_author1__Intercept, alpha = 1), data = post.samples) +
  stat_slab(aes(fill = after_stat(level)),
                .width = c(.66, .80, .95,  1), position = "dodgejust") +
  stat_pointinterval(.width = c(.66, .8, .95), alpha = 1) +
  stat_slab(aes(xdist = prior), 
                data = tau_prior_plot, fill = NA, color = "grey", inherit.aes = FALSE) +
  ggdist::scale_thickness_shared() +
  scale_fill_brewer(na.translate = FALSE, name = "Probability") +
  geom_vline(xintercept = 0.1, linetype = "dashed") +
  geom_vline(xintercept = 0.5, linetype = "dashed") +
  geom_vline(xintercept = 1, linetype = "dashed") +
  annotate("text", x = 0.05, y = 0.98, label = "Low", fontface = "bold") +
  annotate("text", x = 0.3, y = 0.98, label = "Reasonable", fontface = "bold") +
  annotate("text", x = 0.75, y = 0.98, label = "Fairly high", fontface = "bold") +
  annotate("text", x = 1.1, y = 0.98, label = "Fairly extreme", fontface = "bold") +
  annotate("text", x = 1, y = 0.1, label = "Heterogeneity prior", fontface = "bold", colour = "grey30", angle = -2) +
  scale_x_continuous(breaks = c(0, 0.1, 0.25, 0.5, 0.75, 1, 1.2), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0, 1.2), ylim = c(0,1)) +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Log odds ratio", title = "B: Heterogeneity") +
  ylab(NULL) +
  guides(alpha = "none")

grid.arrange(mu, tau, ncol = 1)

```
:::

# Probability of benefit calculations

Now to calculate the probability of dexmedetomidine having certain effect sizes. In order to do this we need to transform the odds ratio into an interpretable effect size: the number needed to treat (NNT).

To go from the OR ➜ NTT, you must first use the OR to calculate the risk in the treatment group (R~t~), as shown in @eq-1.

$$
R_t = \frac{R_c \times OR}{R_c(OR - 1) + 1}
$$ {#eq-1}

Where R~t~ is the risk in the treatment group and R~c~ is the risk in the control group. For R~c~, we use the median control group rate across all studies, with a different median rate for each subgroup.

Then, we can calculate the risk difference (R~d~) in @eq-2:

$$
R_d = R_t - R_c
$$ {#eq-2}

We are then able to calculate the NNT in @eq-3:

$$
NNT = \frac{100}{R_d}
$$ {#eq-3}

The typical workflow is all well and good but we can't use it for our purposes. What we need is to calculate the required odds ratio for a given NNT and then estimate the probability of our data suggesting the odds ratio is greater or lower than that value.

So we rearrange @eq-1 to obtain @eq-4:

$$
OR = \frac{R_t(R_c - 1)}{R_c(R_t - 1))}
$$ {#eq-4}

We are able to calculate R~t~ in the equation above using @eq-5:

$$
R_t = R_c - \frac{1}{NNT}
$$ {#eq-5}

These calculations are shown below in @tbl-benefit-probs-all and @tbl-benefit-probs-low_rob, with tabs for all studies and excluding studies at high risk of bias.

::: {#prob-benefit .panel-tabset style="color: pink"}
## All studies

```{r}
#| label: tbl-benefit-probs-all
#| tbl-cap: Probability of various NNTs, including all studies

Rc_overall <- median(IVdat_del$control_del_rate)

nnt_50_Rt_overall <- Rc_overall - (100/50)/100
nnt_50_or_overall <- (nnt_50_Rt_overall*(Rc_overall - 1))/(Rc_overall*(nnt_50_Rt_overall - 1))

nnt_25_Rt_overall <- Rc_overall - (100/25)/100
nnt_25_or_overall <- (nnt_25_Rt_overall*(Rc_overall - 1))/(Rc_overall*(nnt_25_Rt_overall - 1))

nnt_10_Rt_overall <- Rc_overall - (100/10)/100
nnt_10_or_overall <- (nnt_10_Rt_overall*(Rc_overall - 1))/(Rc_overall*(nnt_10_Rt_overall - 1))

probs_overall = 
  m.brm |> 
  tidy_draws() |> 
  summarise(any_benefit_overall = 100*mean(b_Intercept < log(1)),
            rrr_nnt_50_overall = 100*mean(b_Intercept < log(nnt_50_or_overall)),
            rrr_nnt_25_overall = 100*mean(b_Intercept < log(nnt_25_or_overall)),
            rrr_nnt_10_overall = 100*mean(b_Intercept < log(nnt_10_or_overall)),
            )

# First, age below 60
Rc_age_below_60 <- median(IVdat_del[IVdat_del$age_60=="No" &
                                               !is.na(IVdat_del$age_60),]$control_del_rate)
nnt_50_Rt_age_below_60 <- Rc_age_below_60 - (100/50)/100
nnt_50_or_age_below_60 <- nnt_50_Rt_age_below_60*(Rc_age_below_60 - 1)/Rc_age_below_60*(nnt_50_Rt_age_below_60 - 1)

nnt_25_Rt_age_below_60 <- Rc_age_below_60 - (100/25)/100
nnt_25_or_age_below_60 <- nnt_25_Rt_age_below_60*(Rc_age_below_60 - 1)/Rc_age_below_60*(nnt_25_Rt_age_below_60 - 1)

nnt_10_Rt_age_below_60 <- Rc_age_below_60 - (100/10)/100
nnt_10_or_age_below_60 <- nnt_10_Rt_age_below_60*(Rc_age_below_60 - 1)/Rc_age_below_60*(nnt_10_Rt_age_below_60 - 1)

probs_age_below_60 = 
  spread_draws(m.brm_age, b_age_60No) |> 
  summarise(any_benefit_age_below_60 = 100*mean(b_age_60No < log(1)),
            rrr_nnt_50_age_below_60 = 100*mean(b_age_60No < log(nnt_50_or_age_below_60)),
            rrr_nnt_25_age_below_60 = 100*mean(b_age_60No < log(nnt_25_or_age_below_60)),
            rrr_nnt_10_age_below_60 = 100*mean(b_age_60No < log(nnt_10_or_age_below_60)),
            )

# then, age above 60
Rc_age_above_60 <- median(IVdat_del[IVdat_del$age_60=="Yes" &
                                               !is.na(IVdat_del$age_60),]$control_del_rate)
nnt_50_Rt_age_above_60 <- Rc_age_above_60 - (100/50)/100
nnt_50_or_age_above_60 <- nnt_50_Rt_age_above_60*(Rc_age_above_60 - 1)/Rc_age_above_60*(nnt_50_Rt_age_above_60 - 1)

nnt_25_Rt_age_above_60 <- Rc_age_above_60 - (100/25)/100
nnt_25_or_age_above_60 <- nnt_25_Rt_age_above_60*(Rc_age_above_60 - 1)/Rc_age_above_60*(nnt_25_Rt_age_above_60 - 1)

nnt_10_Rt_age_above_60 <- Rc_age_above_60 - (100/10)/100
nnt_10_or_age_above_60 <- nnt_10_Rt_age_above_60*(Rc_age_above_60 - 1)/Rc_age_above_60*(nnt_10_Rt_age_above_60 - 1)

probs_age_above_60 = 
  spread_draws(m.brm_age, b_age_60Yes) |> 
  summarise(any_benefit_age_above_60 = 100*mean(b_age_60Yes < log(1)),
            rrr_nnt_50_age_above_60 = 100*mean(b_age_60Yes < log(nnt_50_or_age_above_60)),
            rrr_nnt_25_age_above_60 = 100*mean(b_age_60Yes < log(nnt_25_or_age_above_60)),
            rrr_nnt_10_age_above_60 = 100*mean(b_age_60Yes < log(nnt_10_or_age_above_60)),
            )

## Now dose
# High dose
Rc_high_dose <- median(IVdat_del[IVdat_del$dex_dose=="High" &
                                               !is.na(IVdat_del$dex_dose),]$control_del_rate)

nnt_50_Rt_high_dose <- Rc_high_dose - (100/50)/100
nnt_50_or_high_dose <- nnt_50_Rt_high_dose*(Rc_high_dose - 1)/Rc_high_dose*(nnt_50_Rt_high_dose - 1)

nnt_25_Rt_high_dose <- Rc_high_dose - (100/25)/100
nnt_25_or_high_dose <- nnt_25_Rt_high_dose*(Rc_high_dose - 1)/Rc_high_dose*(nnt_25_Rt_high_dose - 1)

nnt_10_Rt_high_dose <- Rc_high_dose - (100/10)/100
nnt_10_or_high_dose <- nnt_10_Rt_high_dose*(Rc_high_dose - 1)/Rc_high_dose*(nnt_10_Rt_high_dose - 1)

probs_high_dose = 
  spread_draws(m.brm_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_high_dose = 100*mean(b_dex_doseHigh < log(1)),
            rrr_nnt_50_high_dose = 100*mean(b_dex_doseHigh < log(nnt_50_or_high_dose)),
            rrr_nnt_25_high_dose = 100*mean(b_dex_doseHigh < log(nnt_25_or_high_dose)),
            rrr_nnt_10_high_dose = 100*mean(b_dex_doseHigh < log(nnt_10_or_high_dose)),
            )

# Low dose
Rc_low_dose <- median(IVdat_del[IVdat_del$dex_dose=="Low" &
                                               !is.na(IVdat_del$dex_dose),]$control_del_rate)
nnt_50_Rt_low_dose <- Rc_low_dose - (100/50)/100
nnt_50_or_low_dose <- nnt_50_Rt_low_dose*(Rc_low_dose - 1)/Rc_low_dose*(nnt_50_Rt_low_dose - 1)

nnt_25_Rt_low_dose <- Rc_low_dose - (100/25)/100
nnt_25_or_low_dose <- nnt_25_Rt_low_dose*(Rc_low_dose - 1)/Rc_low_dose*(nnt_25_Rt_low_dose - 1)

nnt_10_Rt_low_dose <- Rc_low_dose - (100/10)/100
nnt_10_or_low_dose <- nnt_10_Rt_low_dose*(Rc_low_dose - 1)/Rc_low_dose*(nnt_10_Rt_low_dose - 1)

probs_low_dose = 
  spread_draws(m.brm_dose, b_dex_doseLow) |> 
  summarise(any_benefit_low_dose = 100*mean(b_dex_doseLow < log(1)),
            rrr_nnt_50_low_dose = 100*mean(b_dex_doseLow < log(nnt_50_or_low_dose)),
            rrr_nnt_25_low_dose = 100*mean(b_dex_doseLow < log(nnt_25_or_low_dose)),
            rrr_nnt_10_low_dose = 100*mean(b_dex_doseLow < log(nnt_10_or_low_dose)),
            )

prob_tbl1 <- as.data.frame(rbind("Overall" = c("Overall", "Overall", Rc_overall, probs_overall),
     "Age below 60" = c("Age below 60", "Age", Rc_age_below_60, probs_age_below_60),
     "Age above 60" = c("Age above 60", "Age", Rc_age_above_60,probs_age_above_60),
     "High dose" = c("High dose", "Dose", Rc_high_dose, probs_high_dose),
     "Low dose" = c("Low dose", "Dose", Rc_low_dose, probs_low_dose))) %>%
  mutate_at(vars(3:7), as.numeric) %>%
  rename(`Subgroup` = V1,
         `group` = V2,
          `median_rate` = V3) %>%
  mutate(`median_rate` = `median_rate` * 100) %>%
  mutate_at(vars(3:7), ~sprintf("%.1f", .))

colnames(prob_tbl1) <- c("Subgroup", "Group", "Median control group rate (%)", "Probability of any benefit (%)", "Probability of NNT = 50 (%)",
                        "Probability of NNT = 25 (%)", "Probability of NNT = 10 (%)")

prob_tbl1 %>%
  gt(rowname_col = "Subgroup", groupname_col = "Group") %>%
  tab_style(
    style = list(
      cell_fill(color = "grey"),
      cell_text(weight = "bold")
      ),
    locations = cells_row_groups()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels())
```

## Excluding studies at high risk of bias

```{r}
#| label: tbl-benefit-probs-low_rob
#| tbl-cap: Probability of various NNTs, excluding studies at high risk of bias.

Rc_overall <- median(IVdat_del$control_del_rate)

nnt_50_Rt_overall <- Rc_overall - (100/50)/100
nnt_50_or_overall <- (nnt_50_Rt_overall*(Rc_overall - 1))/(Rc_overall*(nnt_50_Rt_overall - 1))

nnt_25_Rt_overall <- Rc_overall - (100/25)/100
nnt_25_or_overall <- (nnt_25_Rt_overall*(Rc_overall - 1))/(Rc_overall*(nnt_25_Rt_overall - 1))

nnt_10_Rt_overall <- Rc_overall - (100/10)/100
nnt_10_or_overall <- (nnt_10_Rt_overall*(Rc_overall - 1))/(Rc_overall*(nnt_10_Rt_overall - 1))

probs_overall = 
  m.brm_low_rob |> 
  tidy_draws() |> 
  summarise(any_benefit_overall = 100*mean(b_Intercept < log(1)),
            rrr_nnt_50_overall = 100*mean(b_Intercept < log(nnt_50_or_overall)),
            rrr_nnt_25_overall = 100*mean(b_Intercept < log(nnt_25_or_overall)),
            rrr_nnt_10_overall = 100*mean(b_Intercept < log(nnt_10_or_overall)),
            )

# First, age below 60
Rc_age_below_60 <- median(IVdat_del[IVdat_del$age_60=="No" &
                                               !is.na(IVdat_del$age_60),]$control_del_rate)
nnt_50_Rt_age_below_60 <- Rc_age_below_60 - (100/50)/100
nnt_50_or_age_below_60 <- nnt_50_Rt_age_below_60*(Rc_age_below_60 - 1)/Rc_age_below_60*(nnt_50_Rt_age_below_60 - 1)

nnt_25_Rt_age_below_60 <- Rc_age_below_60 - (100/25)/100
nnt_25_or_age_below_60 <- nnt_25_Rt_age_below_60*(Rc_age_below_60 - 1)/Rc_age_below_60*(nnt_25_Rt_age_below_60 - 1)

nnt_10_Rt_age_below_60 <- Rc_age_below_60 - (100/10)/100
nnt_10_or_age_below_60 <- nnt_10_Rt_age_below_60*(Rc_age_below_60 - 1)/Rc_age_below_60*(nnt_10_Rt_age_below_60 - 1)

probs_age_below_60 = 
  spread_draws(m.brm_age_low_rob, b_age_60No) |> 
  summarise(any_benefit_age_below_60 = 100*mean(b_age_60No < log(1)),
            rrr_nnt_50_age_below_60 = 100*mean(b_age_60No < log(nnt_50_or_age_below_60)),
            rrr_nnt_25_age_below_60 = 100*mean(b_age_60No < log(nnt_25_or_age_below_60)),
            rrr_nnt_10_age_below_60 = 100*mean(b_age_60No < log(nnt_10_or_age_below_60)),
            )

# then, age above 60
Rc_age_above_60 <- median(IVdat_del[IVdat_del$age_60=="Yes" &
                                               !is.na(IVdat_del$age_60),]$control_del_rate)
nnt_50_Rt_age_above_60 <- Rc_age_above_60 - (100/50)/100
nnt_50_or_age_above_60 <- nnt_50_Rt_age_above_60*(Rc_age_above_60 - 1)/Rc_age_above_60*(nnt_50_Rt_age_above_60 - 1)

nnt_25_Rt_age_above_60 <- Rc_age_above_60 - (100/25)/100
nnt_25_or_age_above_60 <- nnt_25_Rt_age_above_60*(Rc_age_above_60 - 1)/Rc_age_above_60*(nnt_25_Rt_age_above_60 - 1)

nnt_10_Rt_age_above_60 <- Rc_age_above_60 - (100/10)/100
nnt_10_or_age_above_60 <- nnt_10_Rt_age_above_60*(Rc_age_above_60 - 1)/Rc_age_above_60*(nnt_10_Rt_age_above_60 - 1)

probs_age_above_60 = 
  spread_draws(m.brm_age_low_rob, b_age_60Yes) |> 
  summarise(any_benefit_age_above_60 = 100*mean(b_age_60Yes < log(1)),
            rrr_nnt_50_age_above_60 = 100*mean(b_age_60Yes < log(nnt_50_or_age_above_60)),
            rrr_nnt_25_age_above_60 = 100*mean(b_age_60Yes < log(nnt_25_or_age_above_60)),
            rrr_nnt_10_age_above_60 = 100*mean(b_age_60Yes < log(nnt_10_or_age_above_60)),
            )

## Now dose
# High dose
Rc_high_dose <- median(IVdat_del[IVdat_del$dex_dose=="High" &
                                               !is.na(IVdat_del$dex_dose),]$control_del_rate)

nnt_50_Rt_high_dose <- Rc_high_dose - (100/50)/100
nnt_50_or_high_dose <- nnt_50_Rt_high_dose*(Rc_high_dose - 1)/Rc_high_dose*(nnt_50_Rt_high_dose - 1)

nnt_25_Rt_high_dose <- Rc_high_dose - (100/25)/100
nnt_25_or_high_dose <- nnt_25_Rt_high_dose*(Rc_high_dose - 1)/Rc_high_dose*(nnt_25_Rt_high_dose - 1)

nnt_10_Rt_high_dose <- Rc_high_dose - (100/10)/100
nnt_10_or_high_dose <- nnt_10_Rt_high_dose*(Rc_high_dose - 1)/Rc_high_dose*(nnt_10_Rt_high_dose - 1)

probs_high_dose = 
  spread_draws(m.brm_dose_low_rob, b_dex_doseHigh) |> 
  summarise(any_benefit_high_dose = 100*mean(b_dex_doseHigh < log(1)),
            rrr_nnt_50_high_dose = 100*mean(b_dex_doseHigh < log(nnt_50_or_high_dose)),
            rrr_nnt_25_high_dose = 100*mean(b_dex_doseHigh < log(nnt_25_or_high_dose)),
            rrr_nnt_10_high_dose = 100*mean(b_dex_doseHigh < log(nnt_10_or_high_dose)),
            )

# Low dose
Rc_low_dose <- median(IVdat_del[IVdat_del$dex_dose=="Low" &
                                               !is.na(IVdat_del$dex_dose),]$control_del_rate)
nnt_50_Rt_low_dose <- Rc_low_dose - (100/50)/100
nnt_50_or_low_dose <- nnt_50_Rt_low_dose*(Rc_low_dose - 1)/Rc_low_dose*(nnt_50_Rt_low_dose - 1)

nnt_25_Rt_low_dose <- Rc_low_dose - (100/25)/100
nnt_25_or_low_dose <- nnt_25_Rt_low_dose*(Rc_low_dose - 1)/Rc_low_dose*(nnt_25_Rt_low_dose - 1)

nnt_10_Rt_low_dose <- Rc_low_dose - (100/10)/100
nnt_10_or_low_dose <- nnt_10_Rt_low_dose*(Rc_low_dose - 1)/Rc_low_dose*(nnt_10_Rt_low_dose - 1)

probs_low_dose = 
  spread_draws(m.brm_dose_low_rob, b_dex_doseLow) |> 
  summarise(any_benefit_low_dose = 100*mean(b_dex_doseLow < log(1)),
            rrr_nnt_50_low_dose = 100*mean(b_dex_doseLow < log(nnt_50_or_low_dose)),
            rrr_nnt_25_low_dose = 100*mean(b_dex_doseLow < log(nnt_25_or_low_dose)),
            rrr_nnt_10_low_dose = 100*mean(b_dex_doseLow < log(nnt_10_or_low_dose)),
            )

prob_tbl1 <- as.data.frame(rbind("Overall" = c("Overall", "Overall", Rc_overall, probs_overall),
     "Age below 60" = c("Age below 60", "Age", Rc_age_below_60, probs_age_below_60),
     "Age above 60" = c("Age above 60", "Age", Rc_age_above_60,probs_age_above_60),
     "High dose" = c("High dose", "Dose", Rc_high_dose, probs_high_dose),
     "Low dose" = c("Low dose", "Dose", Rc_low_dose, probs_low_dose))) %>%
  mutate_at(vars(3:7), as.numeric) %>%
  rename(`Subgroup` = V1,
         `group` = V2,
          `median_rate` = V3) %>%
  mutate(`median_rate` = `median_rate` * 100) %>%
  mutate_at(vars(3:7), ~sprintf("%.1f", .))

colnames(prob_tbl1) <- c("Subgroup", "Group", "Median control group rate (%)", "Probability of any benefit (%)", "Probability of NNT = 50 (%)",
                        "Probability of NNT = 25 (%)", "Probability of NNT = 10 (%)")

prob_tbl1 %>%
  gt(rowname_col = "Subgroup", groupname_col = "Group") %>%
  tab_style(
    style = list(
      cell_fill(color = "grey"),
      cell_text(weight = "bold")
      ),
    locations = cells_row_groups()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels())
```
:::

# Sensitivity using other priors

Now to see if our results are sensitive to the choice of prior for 𝝁 and 𝛕.

@tbl-prior-sensitivity below shows the results using three different sets of priors. First is the informative prior, which is the ${\mu} ∽ N(0, 0.82)$ and ${\tau} ∽ Cauchy(0, 0.5)$ prior we used for our primary analysis in @fig-forest-all-studies. The second is the vague prior, which is ${\mu} ∽ N(0, 4)$ and ${\tau} ∽ Cauchy(0, 4)$. This essentially imparts no information on the posterior estimate.

Finally, we have the `Turner et al.` prior. Basically, in 2015 a bunch of people got together and calculated a long list of possible priors for ${\tau}$ depending on the outcome you're looking at and the type of intervention. We're looking at 'pharmacological vs. placebo/control' and the outcome type closet to our purpose is 'infection/onset of new disease'. This combination specifies ${\tau} ∽ LN(-2.49, 1.51)$ as a reasonable prior distribution. For the mean effect we will use the informative prior ${\mu} ∽ N(0, 0.82)$.

As you can see, the choice of prior doesn't really make a difference.

```{r}
#| label: tbl-prior-sensitivity
#| tbl-cap: Sensitivity analysis using weakly informative, vague, and Turner et al. priors for this meta-analysis. 

# First make a function to extract 95% CrI's
CrI <- function(model){
  model |> 
    brms::fixef(summary = F) |>
    ggdist::median_hdi() |>
    mutate(estimate = paste0(sprintf('%.2f', exp(y)), 
                  ' [', sprintf('%.2f', exp(ymin)),
                  ', ', sprintf('%.2f', exp(ymax)), ']')) |>
    dplyr::select(estimate)
}

## The create a function for prediction intervals:

predictions = function(model){
  
  nd = data.frame(author1 = "new", sei = 0)
  
  set.seed(123)
  
  brms::posterior_predict(object = model,
                          newdata = nd,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") |> 
    data.frame() |>
    ggdist::median_hdi() |> 
    rename(y = 1) |>
    mutate(estimate = paste0(sprintf('%.2f', exp(y)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']')) |>
    dplyr::select(estimate)
}

## Create a function for tau
#Create the text for tau
tau_text <- function(model) {
  
  posterior = 
    model |> 
    tidybayes::tidy_draws() |> 
    ggdist::median_hdi(sd_author1__Intercept) |>
    mutate(estimate = paste0(sprintf('%.2f', sd_author1__Intercept), 
                  ' [', sprintf('%.2f', .lower),
                  ', ', sprintf('%.2f', .upper), ']')) |>
    dplyr::select(estimate)
}


inform_row <- cbind(CrI(m.brm_informative), predictions(m.brm_informative), tau_text(m.brm_informative))
vague_row <- cbind(CrI(m.brm_vague), predictions(m.brm_vague), tau_text(m.brm_vague))
turneretal_row <- cbind(CrI(m.brm_turneretal), predictions(m.brm_turneretal), tau_text(m.brm_turneretal))

combined_tbl <- rbind(inform_row, vague_row, turneretal_row)

## Put row names in (corresponding to each model)
combined_tbl$model <- c("Informative",  "Vague", "Turner et al.")

## rearrange table to put models as first row
combined_tbl <- combined_tbl[c(4, 1:3)]

## Combine the dataframe into a gt()
combined_tbl %>%
  gt(rowname_col = "model") %>%
  tab_stubhead(label = "Statistical Model") %>%
  cols_label(
    estimate = "Odds ratio (median [95%CrI])",
    estimate.1 = "Odds ratio (median [95%CrI])",
    estimate.2 = "Median [95%CrI])") %>%
  tab_spanner(
    label = "\u03C4",
    columns = estimate.2) %>%
  tab_spanner(
    label = "Credible interval",
    columns = estimate) %>%
  tab_spanner(
    label = "Prediction interval",
    columns = estimate.1) %>%
  tab_footnote(
    footnote = paste0("\u03BC prior: ", informative_priors$prior[[1]], "; \u03C4 prior: ", informative_priors$prior[[2]]),
    locations = cells_stub(rows = model == "Informative"),
    placement = "right") %>%
  tab_footnote(
    footnote = paste0("\u03BC prior: ", vague_priors$prior[[1]], "; \u03C4 prior: ", vague_priors$prior[[2]]),
    locations = cells_stub(rows = model == "Vague"),
    placement = "right") %>%
  tab_footnote(
    footnote = paste0("\u03BC prior: ", turner_etal_priors$prior[[1]], "; \u03C4 prior: ", turner_etal_priors$prior[[2]]),
    locations = cells_stub(rows = model == "Turner et al."),
    placement = "right") %>%
  opt_footnote_marks(
    marks = "standard")
```

# Fully Bayesian sequential analysis

This analysis uses the methods of [Spence et al](https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.7052?saml_referrer).

Up to this point, our Bayesian methods have used the `brms` package, which interfaces with external program `Stan` using the `cmdstanr` package. However, the following code interfaces with `JAGS` using the `rjags` package. Both `JAGS` and `Stan` use Markov Chain Monte Carlo sampling to perform fully Bayesian statistical inference. However, JAGS relies on Gibbs sampling while Stan relies on the No-U-Turn Sampler (NUTS), so the nuts (pun intended) and bolts are a little different. However, the effect on the pooled result is probably very small.

I have made some explanatory plots below to help explain how this Bayesian sequential method works.

Firstly, we are required to specify ${\epsilon}_e$, ${\epsilon}_f$, and ${\delta}$.

${\epsilon}_e$ represents the threshold value for the effect. As shown in @fig-seq-ma-explanation-plots, when the $(1 - {\epsilon}_e) \text{ HPD CrI}$ for the log odds ratio excludes 0 (no effect), the sequential analysis suggests stopping for benefit. Spence et al. suggest that ${\epsilon}_e = 0.006$ represents a good empirical choice to maintain approximate Bayesian equivalents of ${\alpha} = 0.05$ and ${\beta} = 0.1$. If we use ${\epsilon}_e = 0.006$, then we stop for benefit when our 99.4% HPD CrI excludes 0. Note that this imparts no information on the magnitude of the actual effect size (it could be a very small effect with massive certainty), which is a drawback of this method.

${\epsilon}_f$ represents the threshold value for futility. Spence et al. suggest that ${\epsilon}_f = 0.01$ represents a good empirical choice to maintain approximate Bayesian equivalents of ${\alpha} = 0.05$ and ${\beta} = 0.1$. With ${\epsilon}_f = 0.01$, we are saying that the probability of our effect being more than ${\delta}$ is less than 1%.

${\delta}$ is the minimal relevant effect size. 'Futility' indicates the probability that the intervention can achieve the minimum clinically relevant effect size ${\delta}$ is very small. For this analysis I have specified ${\delta}$ as a log OR of -0.18, which is equivalent to a Cohen's d of 0.1. I will discuss this further below but this is calculated in @eq-6:

$$
\text{ Cohen's d} = log(OR) × \frac{{\pi}}{\sqrt{3}}
$$ {#eq-6}

## Explanatory plots

```{r}
#| label: fig-seq-ma-explanation-plots
#| fig-width: 10
#| fig-height: 3
#| fig-cap: |
#|   Explanation of the fully Bayesian sequential method described by Spence et al. A) Shows when we stop for effect, B) when we stop for futility.

library(PNWColors)

stop_effect <- ggplot(data = data.frame(x = c(-10, 10)), aes(x)) + #Empty plot
  
  # Normal(0, 4)
  stat_function(fun = dnorm, n = 1000,
              args = list(mean = -0.3, sd = 0.1), linetype=1, size = 1.2,
              aes(colour = "4")) +
  geom_vline(xintercept = c(-0.5,  -0.1),
             linetype = 2,
             color = pnw_palette("Starfish", 3, type = "discrete")[1]) +
  geom_segment(aes(x = -0.5, y = 4.5, xend = -0.1, yend = 4.5),
             color = "black",
             size = 0.7,
             arrow = arrow(length = unit(0.04, "npc"), type = "closed")) +
  geom_segment(aes(x = -0.1, y = 4.5, xend = -0.5, yend = 4.5),
             color = "black",
             size = 0.7,
             arrow = arrow(length = unit(0.04, "npc"), type = "closed")) +             
  geom_text(aes(x = -0.3, y = 5, label = "99.4% HPD CrI"),
            size = 4,
            vjust = -1) +
  scale_y_continuous(breaks = seq(0, 6, 3),
                     limits = c(0, 6),
                     expand = c(0, 0)) + # remove gap between X and Y axis
  scale_x_continuous(breaks = 0,
                     limits = c(-0.8, 0.2),
                     expand = c(0, 0)) +
  coord_cartesian(x = c(-0.8, 0.2)) +
  labs(x = "\nMean effect size (log odds ratio)",
       y = "Density\n", title = "A") +
  theme_classic() +
geom_ribbon(data = data.frame(x = seq(-0.8, 0.2, length.out = 1000),
                                y = dnorm(seq(-0.8, 0.2, length.out = 1000), 
                                          mean = -0.3, sd = 0.1)),
              aes(x = x, ymin = 0, ymax = y), 
              fill = "lightblue", alpha = 0.5) +
geom_ribbon(data = data.frame(x = seq(-0.8, -0.6, length.out = 1000),
                                y = dnorm(seq(-0.8, -0.6, length.out = 1000), 
                                          mean = -0.3, sd = 0.1)),
              aes(x = x, ymin = 0, ymax = y), 
              fill = "blue", alpha = 0.5) +
  guides(colour = FALSE) +
  theme(
    plot.margin = margin(20,20,0,20),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    legend.position = 'top',
    legend.text = element_text(size=12),
    legend.title = element_text(size=14),
    legend.key= element_blank(),
    panel.background = element_blank()
    )

stop_futility <- ggplot(data = data.frame(x = c(-10, 10)), aes(x)) + #Empty plot
  stat_function(fun = dnorm, n = 1000,
              args = list(mean = -0.02, sd = 0.1), linetype=1, size = 1.2,
              aes(colour = "4")) +
  geom_vline(xintercept = c(log(0.8),  -log(0.8)),
             linetype = 2,
             color = pnw_palette("Starfish", 3, type = "discrete")[1]) +
  scale_y_continuous(breaks = seq(0, 6, 3),
                     limits = c(0, 6),
                     expand = c(0, 0)) + # remove gap between X and Y axis
  scale_x_continuous(breaks = c(log(0.8), 0, -log(0.8)),
                     labels = function(x) round(as.numeric(x), 2),
                     expand = c(0, 0)) +
  coord_cartesian(x = c(-0.6, 0.6)) +
  labs(x = "\nMean effect size (log odds ratio)",
       y = "Density\n", title = "B") +
  theme_classic() +
geom_ribbon(data = data.frame(x = seq(-0.4, 0.4, length.out = 1000),
                                y = dnorm(seq(-0.4, 0.4, length.out = 1000), 
                                          mean = -0.02, sd = 0.1)),
              aes(x = x, ymin = 0, ymax = y), 
              fill = "lightblue", alpha = 0.5) +
  geom_ribbon(data = data.frame(x = seq(-1, log(0.8), length.out = 1000),
                                y = dnorm(seq(-1, log(0.8), length.out = 1000), 
                                          mean = -0.02, sd = 0.1)),
              aes(x = x, ymin = 0, ymax = y), 
              fill = "blue", alpha = 0.5) +
  geom_ribbon(data = data.frame(x = seq(-log(0.8), 1, length.out = 1000),
                                y = dnorm(seq(-log(0.8), 1, length.out = 1000), 
                                          mean = -0.02, sd = 0.1)),
              aes(x = x, ymin = 0, ymax = y), 
              fill = "blue", alpha = 0.5) +
  geom_text(aes(x = 0.4, y = 0.75, label = "P(θ > δ) < 1%"),
            size = 4,
            vjust = -1) +
  geom_text(aes(x = -0.4, y = 0.75, label = "P(θ < −δ) < 1%"),
            size = 4,
            vjust = -1) +
  guides(colour = FALSE) +
  theme(
    plot.margin = margin(20,20,0,20),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    legend.position = 'top',
    legend.text = element_text(size=12),
    legend.title = element_text(size=14),
    legend.key= element_blank(),
    panel.background = element_blank()
    )

grid.arrange(stop_effect, stop_futility, ncol =2)
```

## Analysis

::: panel-tabset
## All studies

```{r}
#| label: sequential-analysis-all-studies

### 6. R code to perform fully Bayesian sequential meta-analysis with a
###  log-normal heterogeneity prior on real data from Cochrane Library
# Priors: N(0, var) for theta, halfnormal(scale) for tau.sq
# Input:
#   data with 'yearofstudy', 'events1', 'total1', 'events2' & 'total2' columns
#   delta.OR = minimally relevant clinical effect size on odds ratio scale
#   epsilon.e, epsilon.f = vectors of posterior decision thresholds
#   var, log.mean, log.var = prior parameters
#   iterations, chains, burn, thin = MCMC settings
# Requires JAGS download: http://mcmc-jags.sourceforge.net
#   and `rjags' R package: https://cran.r-project.org/web/packages/rjags/

### Define Bayesian model
write("model {for (i in 1:n) { 
      yi[i] ~ dnorm(theta.i[i], 1 / vi[i])
      theta.i[i] ~ dnorm(theta, 1/tsq)} 
      theta  ~ dnorm(mu, prec)
      tsq ~ dnorm(0, 1/0.25) T(0,)}", "BayesMA.bug")

Bayesian.seqMA <- function(yi, vi, delta, 
                           epsilon.e=0.006,epsilon.f=0.010, var=0.6724,iterations=1E5, chains=3, 
                           burn=5000, thin=10){
  i <- 1; result <- "Uncertain"; diag <- c()
                           while(result == "Uncertain" & i <= length(yi)) {
                             Y <- yi[1:i]
                             v <- vi[1:i]
                      BayesMA.jags <- jags.model("BayesMA.bug", n.chains=chains, 
                                                 quiet=TRUE,list(yi=Y, vi=v, n=i, mu=0, prec=1 / var),
                                                 inits=list(theta=runif(1, -2, 2),tsq= 1/runif(1, 0, 0.5)))
                      update(BayesMA.jags,burn, progress.bar="none") 
                      BayesMA.sims <- coda.samples(BayesMA.jags, c("theta", "tsq"),n.iter=iterations, thin=thin,progress.bar="none")
                      diag[i] <- gelman.diag(BayesMA.sims)$mpsrf
                      all.chains <- as.mcmc(do.call(rbind, BayesMA.sims))
                      theta.HPD.eps <- HPDinterval(all.chains, 1 - epsilon.e)[1, ]
                      if (theta.HPD.eps[[1]] > 0) result <- "Harm"
                      if (theta.HPD.eps[[2]] < 0) result <- "Benefit"
                      if (quantile(all.chains[, 1], 1 - epsilon.f)[[1]] < abs(delta) &
                          quantile(all.chains[, 1], epsilon.f)[[1]] > -abs(delta)) {
                        result <- "Futility"}
                      studies <- i
                      i<-i+1}
                           if (sum(diag > 1.2) > 0) {cat("Warning: problem with convergence")}
                           theta.median <- median(all.chains[, 1])
                           theta.HPD.95 <- HPDinterval(all.chains)[1, ]
                           tsq.median <- median(all.chains[, 2])
                           tsq.HPD <- HPDinterval(all.chains)[2, ]
                           list(result=result, studies=studies,theta=theta.median, 
                                theta.hpd=theta.HPD.95,tau.sq=tsq.median, 
                                tau.sq.hpd=tsq.HPD)}

Bayesian.seqMA(yi=IVdat_del[,132], vi=IVdat_del[,133], delta=-0.18)
                             
```

## Excluding studies at high risk of bias

```{r}
#| label: sequential-analysis-low-rob
IVdat_del_low_rob <- IVdat_del %>%
  filter(!overall == "x") 
Bayesian.seqMA(yi=IVdat_del_low_rob[,132], vi=IVdat_del_low_rob[,133], delta=-0.18)
                             
```
:::

## Semi-Bayesian sequential analysis

Now for our semi-Bayesian sequential analysis. This uses the methods of [Higgins et al](https://pubmed.ncbi.nlm.nih.gov/21472757/). This method is 'semi-Bayesian' because it used restricted Whitehead monitoring boundaries with an (approximate) Bayesian approach to updating heterogeneity, but ${\mu}$ is still estimated using frequentist methods.

In the dataframe below, when $stopping = 0$, the benefit is `uncertain`, but when $stopping = 1$, the conclusion is `benefit`.

This method requires us to specify some inputs: ${\mu}_R$, $H$, and $V_{max}$.

${\mu}_R$ is the desired log odds ratio. For this, I have used the MCID of $logOR = 0.18$, which I discussed above in @eq-6.

$±H$ is the horizontal boundaries of the O'Brien-Fleming stopping boundary. To specify an ${\alpha}$ of 0.05 and ${\beta}$ of 0.1, $H$ is set to 7.461.

$V_{max}$ is the maximum vertical boundary of the O'Brien-Fleming stopping boundary. To specify an ${\alpha}$ of 0.05 and ${\beta}$ of 0.1, $V_{max}$ is set to 11.079.

As can be seen below in @tbl-semibayes-all, this method suggests the benefit is uncertain with the current data.

::: panel-tabset
## All studies

```{r}
#| label: tbl-semibayes-all
#| tbl-cap: Semi-Bayesian sequential method.

# An R function to perform a sequential meta-analysis using the approximate
# Semi-Bayes method to update estimates of the heterogeneity variance.
# Input:
# y, var: effect estimate and variance from each trial
# Vmax, H: vertical and horizontal boundaries of O’Brian and Fleming design
# eta, lambda: parameters of IG(eta,lambda) prior.
# Requires ’rmeta’ package (http://cran.r-project.org/web/packages/rmeta/)

## Desired log odds ratio reduction to be detected 0.18, with alpha = 0.05, beta = 0.1
logor <- 0.18
H_logor <- 7.461/logor
Vmax_logor <- 11.079/(logor^2)

seq.ma.appbayes <- function(y,var,Vmax,H,eta,lambda){
# set up output data
library(rmeta)
yout <- varout <- Zout <- Vout <- tausqout <- lowerout <- upperout <- stopping <- rep(0,length(y))
Vlast <- 1 / var[1]
# loop for each trial in the sequential meta-analysis
for (i in 1:length(y)){
# Compute approximate Bayes estimate of heterogeneity variance
# from DerSimonian-Laird estimate and prior distribution
    tau.sq.DL <- meta.summaries(y[1:i],sqrt(var[1:i]),method="random")$tau2
    tausqout[i] <- tau.sq.AB <- max((2 * lambda + i * tau.sq.DL)/(2 * eta + i - 2), 0)
# Compute cumulative Z and V and estimates of y and var(y)
    Vout[i] <- cumul.V <- sum(1/(var[1:i] + tau.sq.AB))
    Zout[i] <- cumul.Z <- sum(y[1:i]/(var[1:i] + tau.sq.AB))
    yout[i] <- cumul.Z / cumul.V
    varout[i] <- 1 / cumul.V
# Compute "Christmas tree" correction
    Vdiff <- max(0, cumul.V - Vlast)
    corrected.upper <- H - 0.583 * sqrt(Vdiff)
    corrected.lower <- -H + 0.583 * sqrt(Vdiff)
    Vlast <- cumul.V
# Compute repeated confidence intervals
    lowerout[i] <- (cumul.Z - corrected.upper)/cumul.V
    upperout[i] <- (cumul.Z - corrected.lower)/cumul.V
# Determime if a stopping boundary is crossed
# 1 = boundary crossed , 0 = not crossed

    stopping[i] <- ifelse((i>2) & (lowerout[i]>0 | upperout[i]<0 | cumul.V>Vmax), 1, 0)
    }
# output results
    out <- data.frame(y=yout,var=varout,Z=Zout,V=Vout,lower=lowerout,
                      upper=upperout,tausq=tausqout,stopping=stopping)
    return(out)
}

df <- data.frame(seq.ma.appbayes(y = IVdat_del[,132], var = IVdat_del[,133], 
                Vmax = Vmax_logor, H = H_logor, eta = 1.5, lambda=1))

gt(df)

```

## Excluding studies at high risk of bias

```{r}
#| label: tbl-semi-bayes-low-rob
#| tbl-cap: Semi-Bayesian sequential method for this meta-analysis, excluding studies at high risk of bias.

# An R function to perform a sequential meta-analysis using the approximate
# Semi-Bayes method to update estimates of the heterogeneity variance.
# Input:
# y, var: effect estimate and variance from each trial
# Vmax, H: vertical and horizontal boundaries of O’Brian and Fleming design
# eta, lambda: parameters of IG(eta,lambda) prior.
# Requires ’rmeta’ package (http://cran.r-project.org/web/packages/rmeta/)

## Desired log odds ratio reduction to be detected 0.18, with alpha = 0.05, beta = 0.1
logor <- 0.18
H_logor <- 7.461/logor
Vmax_logor <- 11.079/(logor^2)

seq.ma.appbayes <- function(y,var,Vmax,H,eta,lambda){
# set up output data
library(rmeta)
yout <- varout <- Zout <- Vout <- tausqout <- lowerout <- upperout <- stopping <- rep(0,length(y))
Vlast <- 1 / var[1]
# loop for each trial in the sequential meta-analysis
for (i in 1:length(y)){
# Compute approximate Bayes estimate of heterogeneity variance
# from DerSimonian-Laird estimate and prior distribution
    tau.sq.DL <- meta.summaries(y[1:i],sqrt(var[1:i]),method="random")$tau2
    tausqout[i] <- tau.sq.AB <- max((2 * lambda + i * tau.sq.DL)/(2 * eta + i - 2), 0)
# Compute cumulative Z and V and estimates of y and var(y)
    Vout[i] <- cumul.V <- sum(1/(var[1:i] + tau.sq.AB))
    Zout[i] <- cumul.Z <- sum(y[1:i]/(var[1:i] + tau.sq.AB))
    yout[i] <- cumul.Z / cumul.V
    varout[i] <- 1 / cumul.V
# Compute "Christmas tree" correction
    Vdiff <- max(0, cumul.V - Vlast)
    corrected.upper <- H - 0.583 * sqrt(Vdiff)
    corrected.lower <- -H + 0.583 * sqrt(Vdiff)
    Vlast <- cumul.V
# Compute repeated confidence intervals
    lowerout[i] <- (cumul.Z - corrected.upper)/cumul.V
    upperout[i] <- (cumul.Z - corrected.lower)/cumul.V
# Determime if a stopping boundary is crossed
# 1 = boundary crossed , 0 = not crossed

    stopping[i] <- ifelse((i>2) & (lowerout[i]>0 | upperout[i]<0 | cumul.V>Vmax), 1, 0)
    }
# output results
    out <- data.frame(y=yout,var=varout,Z=Zout,V=Vout,lower=lowerout,
                      upper=upperout,tausq=tausqout,stopping=stopping)
    return(out)
}

df <- data.frame(seq.ma.appbayes(y = IVdat_del_low_rob[,132], var = IVdat_del_low_rob[,133], 
                Vmax = Vmax_logor, H = H_logor, eta = 1.5, lambda=1))

gt(df)
```
:::

## Cumulative meta-analysis plots

The above analysis give good information but do not present the results succinctly to a reader. Therefore, below I've performed cumulative meta-analyses to show the results of the sequential analyses. These plots show the posterior of the pooled effect after adding the next study sequentially. As such, the confidence intervals should shrink with increasing studies towards the 'true' population mean. @fig-cumulative_forest-all-studies shows the findings.

```{r}
#| label: fig-cumulative_forest-all-studies
#| fig.height: 6
#| fig.width: 10
#| cache: TRUE
#| results: 'hide'
#| message: FALSE
#| warning: FALSE
#| errors: FALSE

new_dat <- dat[!is.na(dat$control_del)&!is.na(dat$dex_del),] %>%
  arrange(year)

## We need to change duplicated author1's like we did in the setup chunk
new_dat$author_updated <- new_dat$author
duplicated_rows <- duplicated(new_dat$author) | duplicated(new_dat$author, fromLast = TRUE)
suffix_vec <- rep("", length(new_dat$author))
suffix_vec[duplicated(new_dat$author)] <- ave(new_dat$author, new_dat$author, FUN = function(x) {
  letters[seq_along(x)]
})
new_dat$author[duplicated_rows] <- paste0(new_dat$author[duplicated_rows], "_", suffix_vec[duplicated_rows])

cumulative_meta_analysis_function <- function(new_dat) {
  results_list <- list()
  for (i in 1:nrow(new_dat)) {
    metareg_df <- new_dat[1:i,] 

    escalc_df <- escalc(measure = "OR", ai = dex_del, bi = dex_no_del, 
                        ci = control_del, di = control_no_del, data = metareg_df) %>%
        mutate(sei = sqrt(vi),
         author1 = paste(author, year, sep = " "))
    
    bm_result <- brm(yi | se(sei) ~ 0 + Intercept + (1 | author1),
             data = escalc_df,
             prior = priors_primary,
              iter = 4000,
              backend = "cmdstanr", 
              cores = parallel::detectCores(),
              chains = 4,
              seed = 123)

    results_list[[i]] <- bm_result
  }
  return(results_list)
}

results_list <- cumulative_meta_analysis_function(new_dat)

cum_ma_1 <- results_list[[1]]
cum_ma_2 <- results_list[[2]]
cum_ma_3 <- results_list[[3]]
cum_ma_4 <- results_list[[4]]
cum_ma_5 <- results_list[[5]]
cum_ma_6 <- results_list[[6]]
cum_ma_7 <- results_list[[7]]
cum_ma_8 <- results_list[[8]]
cum_ma_9 <- results_list[[9]]
cum_ma_10 <- results_list[[10]]
cum_ma_11 <- results_list[[11]]
cum_ma_12 <- results_list[[12]]
cum_ma_13 <- results_list[[13]]
cum_ma_14 <- results_list[[14]]
cum_ma_15 <- results_list[[15]]
cum_ma_16 <- results_list[[16]]


cumul_mas <- list(cum_ma_1, cum_ma_2, cum_ma_3, cum_ma_4, cum_ma_5, cum_ma_6, 
               cum_ma_7, cum_ma_8, cum_ma_9, cum_ma_10, cum_ma_11, cum_ma_12,
               cum_ma_13, cum_ma_14, cum_ma_15, cum_ma_16)

OR_fn <- function(m.brm, subgroup) {
    spread_draws(m.brm, b_Intercept) %>%
        mutate(outcome = subgroup)
}

draws_ma <- rbind(OR_fn(cum_ma_1, paste0("+ ", cum_ma_1$data[1,4])), OR_fn(cum_ma_2, paste0("+ ", cum_ma_2$data[2,4])), 
                   OR_fn(cum_ma_3, paste0("+ ", cum_ma_3$data[3,4])), OR_fn(cum_ma_4, paste0("+ ", cum_ma_4$data[4,4])),
                   OR_fn(cum_ma_5, paste0("+ ", cum_ma_5$data[5,4])), OR_fn(cum_ma_6, paste0("+ ", cum_ma_6$data[6,4])),
                   OR_fn(cum_ma_7, paste0("+ ", cum_ma_7$data[7,4])), OR_fn(cum_ma_8, paste0("+ ", cum_ma_8$data[8,4])),
                   OR_fn(cum_ma_9, paste0("+ ", cum_ma_9$data[9,4])), OR_fn(cum_ma_10, paste0("+ ", cum_ma_10$data[10,4])),
                  OR_fn(cum_ma_11, paste0("+ ", cum_ma_11$data[11,4])), OR_fn(cum_ma_12, paste0("+ ", cum_ma_12$data[12,4])),
                  OR_fn(cum_ma_13, paste0("+ ", cum_ma_13$data[13,4])),OR_fn(cum_ma_14, paste0("+ ", cum_ma_14$data[14,4])),
                  OR_fn(cum_ma_15, paste0("+ ", cum_ma_15$data[15,4])), OR_fn(cum_ma_16, paste0("+ ", cum_ma_16$data[16,4]))) %>%
  mutate(outcome = factor(outcome, levels = c("+ Maldonado 2009", "+ Shehabi 2009","+ Park 2014","+ Djaiani 2016","+ Liu 2016","+ Li 2017","+ Shu 2017",
                                      "+ Subramaniam 2019","+ Soh 2020","+ Turan 2020","+ Dong 2021","+ Likhvantsev 2021","+ Momeni 2021",
                                      "+ Chitnis 2022", "+ Qu 2023","+ Wang 2023")))

# Now for the low risk of bias
new_dat_low_rob <- dat[!is.na(dat$control_del)&!is.na(dat$dex_del),] %>%
  arrange(year) %>%
  filter(!overall == "x")

results_list_low_rob <- cumulative_meta_analysis_function(new_dat_low_rob)

cum_ma_1_low_rob <- results_list_low_rob[[1]]
cum_ma_2_low_rob <- results_list_low_rob[[2]]
cum_ma_3_low_rob <- results_list_low_rob[[3]]
cum_ma_4_low_rob <- results_list_low_rob[[4]]
cum_ma_5_low_rob <- results_list_low_rob[[5]]
cum_ma_6_low_rob <- results_list_low_rob[[6]]
cum_ma_7_low_rob <- results_list_low_rob[[7]]
cum_ma_8_low_rob <- results_list_low_rob[[8]]
cum_ma_9_low_rob <- results_list_low_rob[[9]]
cum_ma_10_low_rob <- results_list_low_rob[[10]]
cum_ma_11_low_rob <- results_list_low_rob[[11]]

cumul_mas_low_rob <- list(cum_ma_1_low_rob, cum_ma_2_low_rob, cum_ma_3_low_rob, 
                          cum_ma_4_low_rob, cum_ma_5_low_rob, cum_ma_6_low_rob, 
               cum_ma_7_low_rob, cum_ma_8_low_rob, cum_ma_9_low_rob, cum_ma_10_low_rob, 
               cum_ma_11_low_rob)

draws_ma_low_rob <- rbind(OR_fn(cum_ma_1_low_rob, paste0("+ ", cum_ma_1_low_rob$data[1,4])), 
                          OR_fn(cum_ma_2_low_rob, paste0("+ ", cum_ma_2_low_rob$data[2,4])), 
                   OR_fn(cum_ma_3_low_rob, paste0("+ ", cum_ma_3_low_rob$data[3,4])), 
                   OR_fn(cum_ma_4_low_rob, paste0("+ ", cum_ma_4_low_rob$data[4,4])),
                   OR_fn(cum_ma_5_low_rob, paste0("+ ", cum_ma_5_low_rob$data[5,4])), 
                   OR_fn(cum_ma_6_low_rob, paste0("+ ", cum_ma_6_low_rob$data[6,4])),
                   OR_fn(cum_ma_7_low_rob, paste0("+ ", cum_ma_7_low_rob$data[7,4])), 
                   OR_fn(cum_ma_8_low_rob, paste0("+ ", cum_ma_8_low_rob$data[8,4])),
                   OR_fn(cum_ma_9_low_rob, paste0("+ ", cum_ma_9_low_rob$data[9,4])), 
                   OR_fn(cum_ma_10_low_rob, paste0("+ ", cum_ma_10_low_rob$data[10,4])),
                  OR_fn(cum_ma_11_low_rob, paste0("+ ", cum_ma_11_low_rob$data[11,4]))) %>%
  mutate(outcome = factor(outcome, levels = c("+ Shehabi 2009", "+ Djaiani 2016", "+ Li 2017",
                                      "+ Subramaniam 2019","+ Soh 2020","+ Turan 2020","+ Dong 2021","+ Likhvantsev 2021","+ Momeni 2021",
                                      "+ Qu 2023","+ Wang 2023")))

all <- ggplot(aes(exp(b_Intercept), 
           y = fct_relevel(outcome, rev)), 
       data = draws_ma) +
  geom_vline(xintercept = 1, color = "black", 
             size = 0.7) +
  stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,18)) +
  annotate("text", x = 0.35, y =17.5, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 3, y = 17.5,  label = "Favours\ncontrol") +
  geom_segment(aes(x = 3.5, y = 1, xend = 1.5, yend = 1),
             color = "black",
             size = 0.2,
             arrow = arrow(length = unit(0.02, "npc"), type = "closed")) +             
  annotate("text", x = 3, y = 2,  label = "Benefit not certain\n(Semi-Bayesian)", colour = "red") +
  geom_segment(aes(x = 3.5, y = 10, xend = 1.5, yend = 10),
             color = "black",
             size = 0.2,
             arrow = arrow(length = unit(0.02, "npc"), type = "closed")) +             
  annotate("text", x = 3, y = 10,  label = "Benefit certain\n(Fully Bayesian)", colour = "darkgreen") +
  theme_light() +
  labs(x="Odds ratio (log scale)", title = "A: All studies") +
  ylab(NULL) +
  guides(alpha = "none")

low_rob <- ggplot(aes(exp(b_Intercept), 
           y = fct_relevel(outcome, rev)), 
       data = draws_ma_low_rob) +
  geom_vline(xintercept = 1, color = "black", 
             size = 0.7) +
  stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,13)) +
  annotate("text", x = 0.35, y =12.5, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 3, y = 12.5,  label = "Favours\ncontrol") +
  geom_segment(aes(x = 3.5, y = 1, xend = 1.5, yend = 1),
             color = "black",
             size = 0.2,
             arrow = arrow(length = unit(0.02, "npc"), type = "closed")) +             
  annotate("text", x = 3, y = 1,  label = "Benefit not certain\n(Semi- + fully Bayes)", colour = "red") +
  theme_light() +
  labs(x="Odds ratio (log scale)", title = "B: Excluding studies at high risk of bias") +
  ylab(NULL) +
  guides(alpha = "none")

grid.arrange(all, low_rob, ncol = 2)

```

# Bayesian re-analysis of DECADE

Now to re-analyse the findings of the DECADE trial.

For this we will use the `brms` package to do logistic regression (outcome: `delirium`, predictor: `group`).

::: callout-note
They additionally control for cardiac history in the DECADE trial (given this was 'unmatched' at baseline between the two groups as per their definition of 'unmatched') but we do not have these data, so we can't do this. The effect on the results appears minimal.
:::

A logit-binomial regression is used: binomial family with logit link function. The output of this analysis is a log odds ratio - which is convenient for us, because the output of our meta-analysis is a log odds ratio, meaning we don't have to do any interim conversions. Turan et al. report relative risks with the log link function in their GLM but this is not really best practice. See [Doi et al.'s 2022 paper](https://www.jclinepi.com/article/S0895-4356(21)00241-9/fulltext). For our purposes it's appropriate to use odds ratios, especially because the odds ratio is portable across different baseline risks (unlike the RR). To my mind this is critical for meta-analysis, because we want to apply our mean effect size across different populations.

We need to consider priors for the 'group' coefficient of the logistic regression.

::: callout-note
You'll notice there is a discrepancy between the prior family and the likelihood family; our prior is a normal distribution but the regression family uses a binomial (or Bernoulli) distribution. The normal distribution should approximate the binomial distribution with 794 data points, so it's appropriate that we use normally distributed priors.
:::

There are two approaches to informative priors here: reference priors, and data-derived priors. For these analyses I borrow various ideas from [Albuquerque and Brophy](https://www.sciencedirect.com/science/article/pii/S016752732200328X#f0015), [Zampieri et al.](https://ccforum.biomedcentral.com/articles/10.1186/s13054-022-04120-y#Sec12), [Goligher et al.](https://jamanetwork.com/journals/jama/article-abstract/2709620), and [Andersen-Ranberg et al](https://pubmed.ncbi.nlm.nih.gov/36971791/).

First, the data-derived priors.

Here, we run a Bayesian meta-analysis excluding the DECADE trial. We use a weakly informative prior for the mean effect, ${\mu}$, which is a normal distribution with mean 0 and standard deviation 0.82: $N(0, 0.82)$. We also use a weakly informative prior for heterogeneity SD, ${\tau}$, which is a Cauchy distribution with a 0.5 scale parameter: $Cauchy(0, 0.50)$.

The code below is for the meta-analysis. It's the same code that is used for all the other meta-analytic models in this document. `IVdat_del` is an `escalc()` object with effect sizes and variances for studies in the meta-analysis, excluding the DECADE trial.

``` r
priors_meta_analysis <- brms::prior(normal(0,0.82), class = b, coef = "Intercept") +
                        brms::prior(cauchy(0,0.5), class = sd)
            
meta_analysis_brm <- brms::brm(yi | se(sei) ~ 0 + Intercept + (1 | author1),
              data = IVdat_del,
              prior = priors_meta_analysis,
              iter = 4000,
              backend = "cmdstanr", 
              cores = parallel::detectCores(),
              chains = 4,
              seed = 123)
```

The normal-normal hierarchical model for meta-analysis (which we are using here) assumes that each individual study's effects are normally distributed (the first 'normal'), but also that the overall effects across different populations follows a normal distribution with mean ${\mu}$ and standard deviation ${\tau}$ (the second 'normal'). This has the notation: $N({\mu}, {\tau}^2)$. This is why ${\tau}$ is a measure of heterogeneity; it quantifies variability in effects across patient populations.

Now, what I have done below is make the prior for the `group` coefficient in logistic regression for the DECADE trial: $N({\mu}, {\tau}^2)$.

::: callout-note
This is imperfect for a few reasons. Firstly, I said that ${\tau}$ is the SD of the true population mean effect. So there is no reason we should *a priori* expect the SD of DECADE's observed effect to be equal to ${\tau}$ from our meta-analysis (because the DECADE trial was performed on just one of these theoretical 'populations'). The other option is that we could use the estimated standard error of the mean effect from the meta-analysis (but this is actually pretty similar to the ${\tau}$ estimate).
:::

I didn't specify a prior for the ${\sigma}$ parameter in `brms`, instead using the default vague prior. I set the Intercept prior (the odds of delirium when all predictor variables are held at their reference value) to $N(0, 1)$.

So the logistic regression model for DECADE's results that I call in `brms` is as follows:

``` r
priors_regression <- brms::prior(normal(𝛍,𝛕), coef = "groupdexmedetomidine") +
                     brms::prior(normal(0,1), class = b)

logstic_regression.brm <- brms::brm(delirium ~ 0 + Intercept + group,
              data = df,
              family = bernoulli(link = "logit"),
              prior = priors_regression,
              iter = 4000,
              backend = "cmdstanr", 
              cores = parallel::detectCores(),
              chains = 4,
              seed = 123)
```

We also want to present analyses downweighting the effect of the meta-analysis prior. This is done to account for perceived issues with the meta-analysis - such as publication bias (which we have assessed to be very significant), and the inclusion of low-quality studies. To do this, I artificially inflated the standard deviation parameter ${\tau}$. I multiplied ${\tau}$ by 2 to obtain the 50% downweighting, and multiplied ${\tau}$ by 4 to get the 75% downweighting.

With all that in mind, I have the following data-derived priors for DECADE's results, which are shown further down in @fig-priors-plots:

-   Meta-analysis prior with the full weight: $N({\mu}, {\tau})$

-   Meta-analysis prior with the half weight: $N({\mu}, {\tau} × 2)$

-   Meta-analysis prior with the quarter weight: $N({\mu}, {\tau} × 4)$

Now for the 'reference' priors.

Reference priors are meant to reflect various prior attitudes towards the research question: from very sceptical to very optimistic. References priors are informed by a consideration of the minimal clinically important difference (MCID). I am going to use the MCID in two ways, which I describe in detail below:

1.  Inform the *specification of reference priors* for the DECADE trial's results.

2.  Aid with *interpretation of the clinical significance* of the posterior estimates of the DECADE trial's results.

The MCID is the threshold value for which we would consider an effect meaningful. This is a little difficult with odds ratios because most people can't intuitively think of what a 'meaningful' odds ratio would be. For example if you picked a 30% odds reduction, this would decrease the rate of delirium from 40% to 32% (almost certainly meaningful) or from 5% to 3.6% (potentially not meaningful).

One way to determine the MCID is to consider the effect on the Cohen's d (standardised mean difference (SMD)) scale. This is a standardised scale where a Cohen's d of 0.2, 0.5, and 0.8 (somewhat arbitrarily) correspond to a small, medium, and large effect size, respectively.

[Kruschke](https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf) suggests that in the absence of better information, an SMD of 0.1 is a good starting point for the MCID. This is half of what would constitute a 'small' effect size according to Cohen. We can (approximately) convert the SMD to the log odds ratio using the formula:

$$
\text{SMD} = log(OR) × \frac{{\pi}}{\sqrt{3}}
$$

This results in an MCID of 0.18 on the log odds ratio scale. This is equivalent to an odds ratio of 0.84, or a 16% reduction in the odds of delirium. Using the control delirium rate in the DECADE trial of 12%, this means we would consider a 2% absolute risk reduction (to a rate of 10% in the dexmedetomidine group) as a 'meaningful' reduction.

This also allows us to specify the **region of practical equivalence** (ROPE). This is the range of values which, from a practical perspective, are equivalent to the null (zero effect). The decision rules for the ROPE are best described by [Kruschke](https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf):

> "A parameter value is declared to be not credible, or rejected, if its entire ROPE lies outside the 95% highest density interval (HDI) of the posterior distribution of that parameter ... Notice that when the HDI excludes the ROPE, we are not rejecting all values within the ROPE; we are rejecting only the null value."

So, if our posterior 95% CrI does not overlap with the ROPE we would conclude that dexmedetomidine has a non-zero effect on delirium.

And we are also provided with the flip side of this analysis:

> "A parameter value is declared to be accepted for practical purposes if that value's ROPE completely contains the 95% HDI of the posterior of that parameter."

So, if our posterior 95% CrI falls completely with the ROPE, we would conclude that dexmedetomidine has no effect on delirium.

And finally, what is the 95%CrI and ROPE overlap?

> "When the HDI and ROPE overlap, with the ROPE not completely containing the HDI, then neither of the above decision rules is satisfied, and we withhold a decision. This means merely that the current data are insufficient to yield a clear decision one way or the other, according to the stated decision criteria."

The ROPE relates to the MCID as the upper and lower limits of the ROPE will be the values for the MCID. The specification of the limits of the ROPE is obviously subject to considerable debate. [Kruschke](https://nyu-cdsc.github.io/learningr/assets/kruschke_bayesian_in_R.pdf) tells us:

> "In some applications, the ROPE can be specified with respect to the magnitude of conventionally"small" effect sizes for the domain of research ... But just as the labeling of effect sizes as "small" depends on conventional practice, the setting of a ROPE must be made in the context of current theory, measurement abilities, and the practical purpose of the decision."

So we've chosen a logOR change of 0.18 as our 'small' effect size. That means our ROPE is $-0.18 < {\mu} < 0.18$ on the log odds ratio scale. But we need to check if our conclusions are robust to difference limits for the ROPE; from what I said above, it is clear that wide ROPEs will favour accepting the null while narrow ROPEs will favour rejection. So, I will also consider an MCID of $0.18 × 1.5 = 0.27$, i.e., 1.5 times greater than the original MCID, which would make our ROPE $-0.27 < {\mu} < 0.27$.

So that covers the mean for our reference priors. But what about the prior variance? For this I consider 'RCT equivalents'. I stole this idea from [Goligher et al.](https://jamanetwork.com/journals/jama/article-abstract/2709620)'s Bayesian re-analysis in JAMA.

Basically, for the reference prior I pretend that there has been a theoretical prior RCT that reported a certain effect size (corresponding to the MCID stuff above) and enrolled a certain number of participants. We can calculate the standard error of the log odds ratio of that trial using the formula:

$$
se_i = \sqrt{\frac{1}{a_i} + \frac{1}{b_i} + \frac{1}{c_i} + \frac{1}{d_i}}
$$ {#eq-logor-var}

Where a, b, c, and d correspond to the 2x2 table from the *i*th two-group RCT analysing a binary outcome:

|                 | Delirium present | Delirium absent |
|-----------------|------------------|-----------------|
| Control         | a                | b               |
| Dexmedetomidine | c                | d               |

For priors that are 'strongly sceptical' or 'strongly optimistic', I hypothesised a 1000-participant RCT showing a mean effect (on the log odds ratio scale) of: ${\theta} = 1.5 × MCID = 0.27$. For priors that are 'sceptical' or 'optimistic', I hypothesised a 500-participant RCT showing a mean effect (on the log odds ratio scale) of: ${\theta} = MCID = 0.18$.

::: callout-note
Note that the my choices for numbers of participants (and thus the variance) in these theoretical past trials are almost completely arbitrary. They are larger than the theoretical RCTs discussed by [Goligher et al.](https://jamanetwork.com/journals/jama/article-abstract/2709620) I'm open to any ideas about what the corrolary of a 'very sceptical' or 'very optimistic' attitude would be in 'RCT participant' equivalents.
:::

Now for calculating the standard error of the hypothetical trials. To start off, I used the formula I mentioned earlier for calculating the treatment group risk $R_t$ for each hypothetical RCT.

$$
R_t = \frac{R_c × OR}{R_c × (OR - 1) + 1}
$$

In the above equation, for $R_c$ I used the control group delirium rate in DECADE of 12% for all calculations.

Once we have $R_t$ and $R_c$, we can multiply these by the number of participants in that arm of the hypothetical RCT. So, for an RCT of $n$ participants, there would be $\frac{n}{2}$ people in each arm, so our 2x2 table would be:

|                 | Delirium present        | Delirium absent             |
|-----------------|-------------------------|-----------------------------|
| Control         | $a = R_c × \frac{n}{2}$ | $b = (1-R_c) × \frac{n}{2}$ |
| Dexmedetomidine | $c = R_t × \frac{n}{2}$ | $d = (1-R_t) × \frac{n}{2}$ |

Then I calculate the standard error using the above formula in @eq-logor-var with the corresponding values for a, b, c, and d.

::: callout-note
What should be mentioned here is that we are calculating the **standard error**, not the **standard deviation**. I'm not really sure how we could calculate a standard deviation of the odds ratio from a 2x2 table. One option is that we could use the estimated error of the `group` coefficient from the logistic regression for the theoretical RCT. It's not clear what they do in [Goligher et al.](https://jamanetwork.com/journals/jama/article-abstract/2709620)'s analysis. Open to ideas here.
:::

With all that in mind, I have used the following reference priors for the DECADE's results:

-   Vague: $N(0, 10)$. This is an essentially flat prior which imparts no information on the posterior estimate.
-   Strongly skeptical: $N(0.27, 0.186)$. Equivalent to an RCT enrolling 1000 people and showing a 31% increase in the odds of delirium ($MCID × 1.5$) with dexmedetomidine.
-   Skeptical: $N(0.18, 0.267)$. Equivalent to an RCT enrolling 500 people and showing a 20% increase in the odds of delirium ($MCID$) with dexmedetomidine.
-   Neutral: $N(0, 0.355)$. 95% of the probability density lies between an odds ratio of 0.5 to 2.0. This is a reasonable prior to reflect the attitude of someone who thinks dexmedetomidine is equally likely to have a beneficial or harmful effect, and smaller effects are more likely than bigger ones.
-   Optimistic: $N(-0.18, 0.287)$. Equivalent to an RCT enrolling 500 people and showing a 16% reduction in the odds of delirium ($MCID$) with dexmedetomidine.
-   Strongly optimistic: $N(-0.27, 0.206)$. Equivalent to an RCT enrolling 1000 people and showing a 24% reduction in the odds of delirium ($MCID × 1.5$) with dexmedetomidine.

## Prior plots

@fig-priors-plots shows the distributions of the priors. The top two panels show the distribution of the (A) reference priors, and (B) meta-analysis priors. The bottom forest plot shows the posterior distribution of DECADE's results for each prior The grey region denotes the ROPE. The purple lines show the DECADE's findings when analysed alone. The grey lines represent the prior. Regions of the posterior that suggest benefit (OR \< 1) are shaded in blue and those that suggest harm (OR \> 1) are shaded in red. The probabilities of any harm or benefit, and the harm or benefit exceeding the MCID, are shown for each prior.

```{r}
#| label: fig-priors-plots
#| fig-width: 12
#| fig-height: 10
#| cache: TRUE
#| results: 'hide'
#| message: FALSE
#| warning: FALSE
#| errors: FALSE
#| fig-cap: |
#|   Plots of the distributions of the reference and meta-analysis priors.

## Create a dataframe with the DECADE trial's results
group <- c(rep("dexmedetomidine", 398), rep("placebo", 396))
delirium <- c(rep(1, 67), rep(0, 331), rep(1, 46), rep(0, 350))
df <- data.frame(group, delirium)

df$group <- factor(df$group, levels = c("placebo", "dexmedetomidine"))

## First create our MA priors
## start with the fully weighted MA prior
median_mu_excl_decade <- m.brm_excluding_decade %>%
  spread_draws(b_Intercept) %>%
  median_qi() %>%
  dplyr::select(b_Intercept)
#  -0.516

median_tau_excl_decade <- m.brm_excluding_decade %>%
  spread_draws(sd_author1__Intercept) %>%
  median_qi() %>%
  dplyr::select(sd_author1__Intercept)
# 0.170

## Make a function to extract the posterior estimates

draws_fn <- function(brm) {
  brm %>%
  spread_draws(b_groupdexmedetomidine) %>%
  median_qi() %>% 
  mutate(estimate = paste0(sprintf('%.2f', exp(b_groupdexmedetomidine)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']'))
}

# Also we need a function to extract the draws for the forest plot
forest_draws_fn <- function(brm, prior) {
  brm %>%
  spread_draws(b_groupdexmedetomidine) %>%
    dplyr::select(b_groupdexmedetomidine) %>%
    mutate(subgroup = prior)
    
}

## and a function to calculate probability of benefit
benefit_fn <- function(brm) {
  brm %>% 
  tidy_draws() %>% 
  summarise(prob_benefit = sprintf('%.1f', 100*mean(b_groupdexmedetomidine < 0)),
            prob_harm = sprintf('%.1f', 100*mean(b_groupdexmedetomidine > 0)),
            prob_mcid_harm = sprintf('%.1f', 100*mean(b_groupdexmedetomidine > 0.18)),
            prob_mcid_benefit = sprintf('%.1f', 100*mean(b_groupdexmedetomidine < -0.18)))
}


full_weight_ma_priors <- brms::prior(normal(-0.516, 0.170), coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)

ma_full_weight.brm <- brm(delirium ~ 0 + Intercept + group,
             data = df,
             family = bernoulli(link = "logit"),
             prior = full_weight_ma_priors,
              iter = 4000,
              backend = "cmdstanr", 
              cores = parallel::detectCores(),
              chains = 4,
              seed = 123)

ma_full_weight_draws <- draws_fn(ma_full_weight.brm)

ma_full_weight_forest_draws <- forest_draws_fn(ma_full_weight.brm, "MA (100% weight)")
ma_full_weight_benefit <- benefit_fn(ma_full_weight.brm)

## Now for the 50% weight (get this by multiplying SD by 2)

half_weight_ma_priors <- brms::prior(normal(-0.516, 0.170*2), class = b, coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)

ma_half_weight.brm <- update(ma_full_weight.brm, prior = half_weight_ma_priors)

ma_half_weight_draws <- draws_fn(ma_half_weight.brm)
ma_half_weight_forest_draws <- forest_draws_fn(ma_half_weight.brm, "MA (50% weight)")
ma_half_weight_benefit <- benefit_fn(ma_half_weight.brm)


## Now for the 25% weight (get this by multiplying SD by 4)

quarter_weight_ma_priors <- brms::prior(normal(-0.516, 0.170*4), class = b, coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)

ma_quarter_weight.brm <- update(ma_full_weight.brm, prior = quarter_weight_ma_priors)

ma_quarter_weight_draws <- draws_fn(ma_quarter_weight.brm)
ma_quarter_weight_forest_draws <- forest_draws_fn(ma_quarter_weight.brm, "MA (25% weight)")
ma_quarter_weight_benefit <- benefit_fn(ma_quarter_weight.brm)

## Now for our reference priors
# Start with vague with SD = 10
vague_priors <- brms::prior(normal(0,10), class = b, coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)

vague.brm <- update(ma_full_weight.brm, prior = vague_priors)

vague_draws <- draws_fn(vague.brm)
vague_forest_draws <- forest_draws_fn(vague.brm, "Vague")
vague_benefit <- benefit_fn(vague.brm)


# Now very skeptical

## We want an RCT of 100 people showing a 0.27 logOR increase in delirium (1.5*MCID of Cohen's D = 0.1)
# First, to calculate the SE of the RCT we need to calculate the risk in treatment and control groups using formula: Rt = Rc*OR/Rc*(OR - 1) + 1
rt <- (0.12*exp(0.27))/(0.12*(exp(0.27) - 1) + 1)
## [1] 0.1515584

## So the risk would increased from 12% to 15.156%. 
## To obtain a prior RCT of n = 1000 showing no effect using the base rate of 12% delirium from DECADE
## We need to use the formula: SE = sqrt(1/a + 1/b + 1/c + 1d)

se_very_skeptical <- sqrt((1/60) + (1/440) + (1/76) + (1/424))
## 0.1856227

very_skeptical_priors <- brms::prior(normal(0.27, 0.1856227), coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)
  
very_skeptical.brm <- update(ma_full_weight.brm, prior = very_skeptical_priors)

very_skeptical_draws <- draws_fn(very_skeptical.brm)
very_skeptical_forest_draws <- forest_draws_fn(very_skeptical.brm, "Very skeptical")
very_skeptical_benefit <- benefit_fn(very_skeptical.brm)


## Now for skeptical, assuming a prior RCT of n = 500 
# First, to calculate the SE of the RCT we need to calculate the risk in treatment and control groups using formula: Rt = Rc*OR/Rc*(OR - 1) + 1
rt <- (0.12*exp(0.18))/(0.12*(exp(0.18) - 1) + 1)
## [1] 0.1403447

## So the risk would increased from 12% to 14%. 

se_skeptical <- sqrt((1/30) + (1/220) + (1/35) + (1/215))
## 0.2666484

skeptical_priors <- brms::prior(normal(0.18, 0.2666484), coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)
  
skeptical.brm <- update(ma_full_weight.brm, prior = skeptical_priors)

skeptical_draws <- draws_fn(skeptical.brm)
skeptical_forest_draws <- forest_draws_fn(skeptical.brm, "Skeptical")
skeptical_benefit <- benefit_fn(skeptical.brm)

## Now, for neutral priors
# neutral prior is defined so that 0.95 of the probability mass ranges from an odds ratio between 0.5 and 2.0. 
# As done in https://ccforum.biomedcentral.com/articles/10.1186/s13054-022-04120-y#Sec12
se_neutral <- 0.355
neutral_priors <- brms::prior(normal(0, 0.355), coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)
  
neutral.brm <- update(ma_full_weight.brm, prior = neutral_priors)

neutral_draws <- draws_fn(neutral.brm)
neutral_forest_draws <- forest_draws_fn(neutral.brm, "Neutral")
neutral_benefit <- benefit_fn(neutral.brm)


## Now for optimistic - we want an RCT of 500 people showing a -0.18 logOR reduction
# First, to calculate the SE of the RCT we need to calculate the risk in treatment and control groups using formula: Rt = Rc*OR/Rc*(OR - 1) + 1
rt <- (0.12*exp(-0.18))/(0.12*(exp(-0.18) - 1) + 1)
## [1] 0.1022537

# So our trial will have 12% event rate in control and 10% event rate in DEX group

se_optimistic <- sqrt((1/30) + (1/220) + (1/25) + (1/225))

## [1] 0.2869203

optimistic_priors <- brms::prior(normal(-0.18, 0.2869203), coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)
  
optimistic.brm <- update(ma_full_weight.brm, prior = optimistic_priors)
optimistic_draws <- draws_fn(optimistic.brm)
optimistic_forest_draws <- forest_draws_fn(optimistic.brm, "Optimistic")
optimistic_benefit <- benefit_fn(optimistic.brm)

## Now for very optimistic - an RCT of 1000 people showing a -0.27 logOR reduction
# First, to calculate the SE of the RCT we need to calculate the risk in treatment and control groups using formula: Rt = Rc*OR/Rc*(OR - 1) + 1
rt <- (0.12*exp(-0.27))/(0.12*(exp(-0.27) - 1) + 1)
## [1] 0.09428264

# So our trial will have 12% event rate in control and 9.4% event rate in DEX group

se_very_optimistic <- sqrt((1/60) + (1/440) + (1/47) + (1/453))
## [1] 0.2059696

very_optimistic_priors <- brms::prior(normal(-0.27, 0.2059696), coef = "groupdexmedetomidine") +
                brms::prior(normal(0,1), class = b)
  
very_optimistic.brm <- update(ma_full_weight.brm, prior = very_optimistic_priors)

very_optimistic_draws <- draws_fn(very_optimistic.brm)
very_optimistic_forest_draws <- forest_draws_fn(very_optimistic.brm, "Very optimistic")
very_optimistic_benefit <- benefit_fn(very_optimistic.brm)

## Now let's make the plots
# First let's graph the priors
reference_priors <- ggplot(data = data.frame(x = c(-10, 10)), aes(x)) +
  geom_vline(xintercept = 1, color = "black", size = 1) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = 0, sd = 10)), color = "vague", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = 0.27, sd = se_very_skeptical)), color = "very skeptical", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = 0.18, sd = se_skeptical)), color = "skeptical", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = 0, sd = se_neutral)), color = "neutral", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = -0.18, sd = se_optimistic)), color = "optomistic", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = -0.27, sd = se_very_optimistic)), color = "very optomistic", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +
  coord_cartesian(xlim = c(0.1, 7), ylim = c(1, 2)) +
  ggdist::scale_thickness_shared() +
  theme_light() +
  theme(
    axis.text.y = element_blank(),
    legend.position = c(0.95, 0.9),  
    legend.justification = c(1, 1),
    legend.background = element_rect(fill = "lightgrey", color = "black")) +
  labs(x = "Odds ratio", title = "A: Reference priors for logistic regression") +
  ylab(NULL) +
  geom_vline(xintercept = c(0.84, 1.20), linetype = "dotted", color = "grey30") +
  annotate("rect", xmin = 0.84, xmax = 1.20, ymin = -Inf, ymax = Inf, fill = "grey", alpha = 0.2) +
  scale_color_manual(values = RColorBrewer::brewer.pal(6, "Set2"),
                     breaks = c("vague", "very skeptical", "skeptical", "neutral", "optomistic", "very optomistic"),
                     labels = c("Vague", "Very skeptical", 
                                "Skeptical", "Neutral", "Optimistic", 
                                "Very optimistic"),
                     name = "Prior") +
  guides(color = guide_legend(override.aes = list(linetype = "solid", shape = NA)))

ma_priors <- ggplot(data = data.frame(x = c(-10, 10)), aes(x)) +
  geom_vline(xintercept = 1, color = "black", size = 1) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = -0.516, sd = 0.170)), color = "metafull", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = -0.516, sd = (0.170*2))), color = "metahalf", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = -0.516, sd = (0.170*4))), color = "metaquarter", y = 1),
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +
  coord_cartesian(xlim = c(0.1, 7), ylim = c(1, 2)) +
  ggdist::scale_thickness_shared() +
  theme_light() +
  theme(
    axis.text.y = element_blank(),
    legend.position = c(0.8, 0.9),  
    legend.justification = c(1, 1),
    legend.background = element_rect(fill = "lightgrey", color = "black")) +
  labs(x = "Odds ratio", title = "B: Meta-analysis priors for logistic regression") +
  ylab(NULL) +
  geom_vline(xintercept = c(0.84, 1.20), linetype = "dotted", color = "grey30") +
  annotate("rect", xmin = 0.84, xmax = 1.20, ymin = -Inf, ymax = Inf, fill = "grey", alpha = 0.2) +
  scale_color_manual(values = RColorBrewer::brewer.pal(3, "Set1"),
                     breaks = c("metafull", "metahalf", "metaquarter"),
                     labels = c("Full weight", 
                                "50% weight", 
                                "25% weight"),
                     name = "Prior") +
  guides(color = guide_legend(override.aes = list(linetype = "solid", shape = NA)))

# Now let's plot the posterior distributions of the odds ratios for each of these priors
# First let's make a dataframe called tabdat with all of our things of interest
tabdat_pre <- data.frame(subgroup = factor(c("Vague", "Very skeptical", 
                                "Skeptical", "Neutral", "Optimistic", 
                                "Very optimistic", "MA (100% weight)", "MA (50% weight)", "MA (25% weight)")),
                     decade_posterior_mean = IVdat_del$yi[IVdat_del$author1 == "Turan 2020"],
                     decade_posterior_sd = IVdat_del$sei[IVdat_del$author1 == "Turan 2020"],
                     prior_mean = c(0, 0.27, 0.18, 0, -0.18, -0.27, -0.516, -0.516, -0.516),
                     prior_sd = c(10, se_very_skeptical, se_skeptical, se_neutral,
                                  se_optimistic, se_very_optimistic, 0.170, 0.170*2, 0.170*4),
                     posterior_estimate = c(vague_draws$estimate, very_skeptical_draws$estimate, skeptical_draws$estimate,
                                            neutral_draws$estimate, optimistic_draws$estimate, very_optimistic_draws$estimate, 
                                            ma_full_weight_draws$estimate, ma_half_weight_draws$estimate, ma_quarter_weight_draws$estimate),
                     p_benefit = c(vague_benefit$prob_benefit, very_skeptical_benefit$prob_benefit, skeptical_benefit$prob_benefit,
                                            neutral_benefit$prob_benefit, optimistic_benefit$prob_benefit, very_optimistic_benefit$prob_benefit, 
                                            ma_full_weight_benefit$prob_benefit, ma_half_weight_benefit$prob_benefit, ma_quarter_weight_benefit$prob_benefit),
                     p_harm = c(vague_benefit$prob_harm, very_skeptical_benefit$prob_harm, skeptical_benefit$prob_harm,
                                            neutral_benefit$prob_harm, optimistic_benefit$prob_harm, very_optimistic_benefit$prob_harm, 
                                            ma_full_weight_benefit$prob_harm, ma_half_weight_benefit$prob_harm, ma_quarter_weight_benefit$prob_harm),
                     p_mcid_harm = c(vague_benefit$prob_mcid_harm, very_skeptical_benefit$prob_mcid_harm, skeptical_benefit$prob_mcid_harm,
                                            neutral_benefit$prob_mcid_harm, optimistic_benefit$prob_mcid_harm, very_optimistic_benefit$prob_mcid_harm, 
                                            ma_full_weight_benefit$prob_mcid_harm, ma_half_weight_benefit$prob_mcid_harm, ma_quarter_weight_benefit$prob_mcid_harm),
                     p_mcid_benefit = c(vague_benefit$prob_mcid_benefit, very_skeptical_benefit$prob_mcid_benefit, skeptical_benefit$prob_mcid_benefit,
                                            neutral_benefit$prob_mcid_benefit, optimistic_benefit$prob_mcid_benefit, very_optimistic_benefit$prob_mcid_benefit, 
                                            ma_full_weight_benefit$prob_mcid_benefit, ma_half_weight_benefit$prob_mcid_benefit, ma_quarter_weight_benefit$prob_mcid_benefit)) %>%
                    mutate(prior_mean_exp = as.numeric(sprintf('%.2f', exp(prior_mean))),
                          prior_sd = as.numeric(sprintf('%.2f', prior_sd)),
                          prior_mean_sd = paste0(sprintf('%.2f',prior_mean_exp), " ± ", prior_sd),
                           p_benefit = paste0(p_benefit, "%"),
                          p_harm = paste0(p_harm, "%"),
                          p_mcid_benefit = paste0(p_mcid_benefit, "%"),
                          p_mcid_harm = paste0(p_mcid_harm, "%"))

# Make a table which I will need to make the geom_pointintervals
draws_df <- rbind(vague_draws, very_skeptical_draws, skeptical_draws, neutral_draws,
                                            optimistic_draws, very_optimistic_draws, 
                                            ma_full_weight_draws, ma_half_weight_draws, ma_quarter_weight_draws)
draws_df <- cbind(subgroup = factor(c("Vague", "Very skeptical", 
                                "Skeptical", "Neutral", "Optimistic", 
                                "Very optimistic", "MA (100% weight)", "MA (50% weight)", "MA (25% weight)")),
                  draws_df) %>%
  mutate(subgroup = factor(subgroup, levels = c("Prior belief","Vague", "Very skeptical", 
                                "Skeptical", "Neutral", "Optimistic", 
                                "Very optimistic", "MA (100% weight)", "MA (50% weight)", "MA (25% weight)")))

new_row <- data.frame(subgroup = factor("Prior belief"),
                      decade_posterior_mean = "",
                     decade_posterior_sd = "",
                     prior_mean = "",
                     prior_mean_exp = "",
                     prior_sd = "",
                     prior_mean_sd = "Prior mean±SD",
                      posterior_estimate = "Posterior 95%CrI",
                     p_benefit = "P(any benefit)",
                     p_mcid_benefit = "P(benefit >MCID)",
                     p_harm = "P(any harm)",
                     p_mcid_harm = "P(harm >MCID)")

tabdat <- rbind(new_row, tabdat_pre) %>%
  mutate(subgroup = factor(subgroup, levels = c("Prior belief","Vague", "Very skeptical", 
                                "Skeptical", "Neutral", "Optimistic", 
                                "Very optimistic", "MA (100% weight)", "MA (50% weight)", "MA (25% weight)")))
  

# Now let's make a forest dataframe with our data for posterior estimates
# First we need to combine all our forest draws into one long dataframe

forest.data <- rbind(vague_forest_draws, very_skeptical_forest_draws, skeptical_forest_draws, neutral_forest_draws,
                     optimistic_forest_draws, very_optimistic_forest_draws, ma_full_weight_forest_draws, 
                     ma_half_weight_forest_draws, ma_quarter_weight_forest_draws) %>%
  mutate(subgroup = factor(subgroup, levels = c("Vague", "Very skeptical", "Skeptical", "Neutral", "Optimistic", 
                                "Very optimistic", "MA (100% weight)", "MA (50% weight)", "MA (25% weight)")))

forest <- ggplot(aes(exp(b_groupdexmedetomidine), 
           y = fct_relevel(subgroup, rev)), 
       data = forest.data) +
  geom_vline(xintercept = 1, color = "black", 
             size = 0.7) +
  stat_slab(aes(fill = after_stat(x < 1)), slab_colour = "black") +
  geom_pointintervalh(aes(xmin = exp(.lower), 
                          xmax = exp(.upper), 
                          x = exp(b_groupdexmedetomidine)),
                      data = draws_df, 
                      col = "black", alpha = 1, position = position_nudge(y = -0.1)) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = decade_posterior_mean, sd = decade_posterior_sd)),
                y = fct_relevel(subgroup, rev)), 
            data = tabdat_pre, color = "purple", fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = prior_mean, sd = prior_sd)),
                y = fct_relevel(subgroup, rev)), 
            data = tabdat_pre, color = "grey30", fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.25, 4), ylim=c(1,10)) +
  annotate("text", x = 0.5, y =10, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 2, y = 10,  label = "Favours\ncontrol") +
  ggdist::scale_thickness_shared() +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Odds ratio (log scale)") +
  ylab(NULL) +
  theme(legend.position = "none")  +
  guides(alpha = "none") +
  geom_vline(xintercept = c(0.84, 1.20), linetype = "dotted", color = "grey30") +
  annotate("rect", xmin = 0.84, xmax = 1.20, ymin = -Inf, ymax = Inf, fill = "grey", alpha = 0.2)

prior_details <- ggplot(aes(y = fct_relevel(subgroup, rev)), 
                   data = tabdat) +
  geom_text(aes(x = 0, label = subgroup), hjust = 0,
                        fontface = ifelse(tabdat$subgroup == "Prior belief", "bold", "plain")) +
  geom_text(aes(x = 2, label = prior_mean_sd), hjust = 0, 
            fontface = ifelse(tabdat$prior_mean_sd == "Prior mean±SD", "bold", "plain")) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 4))

posteriors <- ggplot(aes(y = fct_relevel(subgroup, rev)), 
                   data = tabdat) +
  geom_text(aes(x = 0, label = posterior_estimate), hjust = 0,
                        fontface = ifelse(tabdat$posterior_estimate == "Posterior 95%CrI", "bold", "plain")) +
  geom_text(aes(x = 1, label = p_benefit), hjust = 0,
                        fontface = ifelse(tabdat$p_benefit == "P(any benefit)", "bold", "plain")) +
  geom_text(aes(x = 1.8, label = p_mcid_benefit), hjust = 0,
                        fontface = ifelse(tabdat$p_mcid_benefit == "P(benefit >MCID)", "bold", "plain")) +
  geom_text(aes(x = 2.8, label = p_harm), hjust = 0,
                        fontface = ifelse(tabdat$p_harm == "P(any harm)", "bold", "plain")) +
  geom_text(aes(x = 3.6, label = p_mcid_harm), hjust = 0,
                        fontface = ifelse(tabdat$p_mcid_harm == "P(harm >MCID)", "bold", "plain")) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 4.2))

library(patchwork)

layout <- c(
  patchwork::area(t = 0, l = 0, b = 15, r = 30),
  patchwork::area(t = 0, l = 32, b = 15, r = 60), 
  patchwork::area(t = 16, l = 0, b = 42, r = 15),
  patchwork::area(t = 16, l = 15, b = 42, r = 28), 
  patchwork::area(t = 16, l = 29, b = 42, r = 60))

reference_priors + ma_priors + prior_details + forest  + posteriors + plot_layout(design = layout)

```

## ARR table

Below I provide direct probability statements for the DECADE trial's results given each priors I specified above. I consider absolute risk reductions (ARRs) of up to 8% or more and absolute risk increases (ARIs) of up to 8% or more. All ARRs in @tbl-arr below use the control group rate for delirium incidence from the DECADE trial (12%). So, an ARR of 8% would be a reduction from 12% in the control group to 4% in the dexmedetomidine group, and an ARI of 8% would be a increase from 12% in the control group to 20% in the dexmedetomidine group.

```{r}
#| label: tbl-arr
#| tbl-cap: Explanations for priors for DECADE and the corresponding probabilities of changes in absolute risk of delirium.

# First we need to calculate the ARRs
# To do this we need to logORs that are required for each ARR
# Let's start with the likelihood of a 8% decrease in delirium incidence
Rc_decade <- 0.12

arr_8_Rt <- Rc_decade - 0.08
arr_8_or <- (arr_8_Rt*(Rc_decade - 1))/(Rc_decade*(arr_8_Rt - 1))
arr_4_Rt <- Rc_decade - 0.04
arr_4_or <- (arr_4_Rt*(Rc_decade - 1))/(Rc_decade*(arr_4_Rt - 1))
arr_2_Rt <- Rc_decade - 0.02
arr_2_or <- (arr_2_Rt*(Rc_decade - 1))/(Rc_decade*(arr_2_Rt - 1))
ari_2_Rt <- Rc_decade + 0.02
ari_2_or <- (ari_2_Rt*(Rc_decade - 1))/(Rc_decade*(ari_2_Rt - 1))
ari_4_Rt <- Rc_decade + 0.04
ari_4_or <- (ari_4_Rt*(Rc_decade - 1))/(Rc_decade*(ari_4_Rt - 1))
ari_8_Rt <- Rc_decade + 0.08
ari_8_or <- (ari_8_Rt*(Rc_decade - 1))/(Rc_decade*(ari_8_Rt - 1))

probs_fn <- function(brm) {
      brm %>% 
      tidy_draws() %>% 
      summarise(arr_8 = 100*mean(b_groupdexmedetomidine < log(arr_8_or)),
            arr_4 = 100*mean(b_groupdexmedetomidine < log(arr_4_or)),
            arr_2 = 100*mean(b_groupdexmedetomidine < log(arr_2_or)),
            arr_below_0 = 100*mean(b_groupdexmedetomidine < log(1)),
            arr_above_0 = 100*mean(b_groupdexmedetomidine > log(1)),
            ari_2 = 100*mean(b_groupdexmedetomidine > log(ari_2_or)),
            ari_4 = 100*mean(b_groupdexmedetomidine > log(ari_4_or)),
            ari_8 = 100*mean(b_groupdexmedetomidine > log(ari_8_or)))
}

probs <- rbind(probs_fn(vague.brm), probs_fn(very_skeptical.brm), probs_fn(skeptical.brm),
               probs_fn(neutral.brm), probs_fn(optimistic.brm), probs_fn(very_optimistic.brm),
               probs_fn(ma_full_weight.brm), probs_fn(ma_half_weight.brm), probs_fn(ma_quarter_weight.brm)) %>%
  mutate(across(where(is.numeric), ~sprintf("%.1f", .)))

tabdat <- data.frame(prior = factor(c("Vague", "Very skeptical", 
                                "Skeptical", "Neutral", "Optimistic", 
                                "Very optimistic", "MA (100% weight)", "MA (50% weight)", "MA (25% weight)")),
                     prior_equivalent = c("No information imposed on posterior estimate", 
                                          "Equivalent to a hypothetical n = 1000 RCT showing a 31% increase in odds of delirium",
                                          "Equivalent to a hypothetical n = 500 RCT showing a 20% increase in odds of delirium",
                                          "95% of the density lies between an odds ratio of 0.5 to 2.0",
                                          "Equivalent to a hypothetical n = 500 RCT showing a 14% decrease in odds of delirium",
                                           "Equivalent to a hypothetical n = 1000 RCT showing a 24% decrease in odds of delirium",
                                          "Meta-analysis of n = 1654 participants across 13 trials given it's full weight as a prior",
                                          "Meta-analysis of n = 1654 participants across 13 trials given 50% weight as a prior",
                                          "Meta-analysis of n = 1654 participants across 13 trials given 25% weight as a prior"),
                     rationale = c("Does not favour one prior belief over another", 
                                          "This effect size is 1.5 times the MCID for harm (logOR of 0.27)",
                                          "This effect size is the MCID for harm (logOR of 0.18)",
                                          "Plausible values for the effect are likely, with values closer to the null most likely",
                                          "This effect size is the MCID for benefit (logOR of -0.18)",
                                           "This effect size is the 1.5 times MCID for benefit (logOR of -0.27)",
                                   "This is analgous to the result of a standard meta-analysis of all the 14 studies, including the DECADE trial",
                                   "This downweights the effect of the meta-analysis by 50% to account for percieved issues with the included trials",
                                   "This downweights the effect of the meta-analysis by 75% to account for percieved issues with the included trials"),
                     probs)

colnames(tabdat) <- c("Prior", "Prior equivalent", "Rationale for prior", "ARR >8%", "ARR >4%",
                        "ARR >2%", "ARR >0%", "ARI >0%", "ARI >2%", "ARI >4%", "ARI >8%")

tabdat %>%
    gt(rowname_col = "Prior") %>%
   tab_row_group(
    label = "Meta-analysis-derived priors",
    rows = starts_with("MA")) %>%
  row_group_order(groups = c(NA, "Meta-analysis-derived priors")) %>%
  tab_options(row_group.default_label = "Reference priors") %>%
  tab_spanner(
    label = "Posterior probability that the change in absolute risk from DECADE is above/below a certain threshold",
    columns = starts_with("AR")) %>%
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_row_groups(groups = everything())) %>%
  tab_style(
    style = list(
      cell_fill(color = "grey"),
      cell_text(weight = "bold")
      ),
    locations = cells_row_groups())
                     
```

## Shrinkage OR estimation + ECDFs

Now let's zoom on the relative distributions of the mean effect when we include and exclude the DECADE trial from the meta-analysis. For this we will use empirical cumulative distribution function (ECDF) plots.

I consider four density functions: meta-analysis including DECADE, meta-analysis excluding DECADE, DECADE alone, and the DECADE shrinkage estimate from the meta-analysis.

The x-axis is common to the upper and lower graphs so one is able to read off the probability of any odds ratio for any of the four curves. @fig-ecdfs shows the results. The grey region denotes the ROPE ($log(OR) = ± 0.18$).

```{r}
#| label: fig-ecdfs
#| fig-width: 10
#| fig-height: 10
#| fig-cap: |
#|   Empirical cumulative distribution function (ECDF) plots including and excluding DECADE.

## First, the MA with the DECADE trial

post.samples_decade <- as_draws_df(m.brm, c("b_Intercept", "sd_author1__Intercept"))

study.draws_decade <- spread_draws(m.brm, r_author1[author1,], b_Intercept) %>% 
  mutate(b_Intercept = r_author1 + b_Intercept)

pooled.effect.draws_decade <- spread_draws(m.brm, b_Intercept) %>% 
  mutate(author1 = "Pooled Effect")

forest.data_decade <- bind_rows(study.draws_decade, 
                         pooled.effect.draws_decade) %>% 
   ungroup() %>%
   mutate(author1 = str_replace_all(author1, "[.]", " ")) %>% 
   mutate(author1 = reorder(author1, b_Intercept)) %>%
   filter(author1 == c("Turan 2020", "Pooled Effect"))

## Now, without DECADE
## First, the MA with the DECADE trial
post.samples_no_decade <- as_draws_df(m.brm_excluding_decade, c("b_Intercept", "sd_author1__Intercept"))

study.draws_no_decade <- spread_draws(m.brm_excluding_decade, r_author1[author1,], b_Intercept) %>% 
  mutate(b_Intercept = r_author1 + b_Intercept)

pooled.effect.draws_no_decade <- spread_draws(m.brm_excluding_decade, b_Intercept) %>% 
  mutate(author1 = "Pooled Effect")

forest.data_no_decade <- bind_rows(study.draws_no_decade, 
                         pooled.effect.draws_no_decade) %>% 
   ungroup() %>%
   mutate(author1 = str_replace_all(author1, "[.]", " ")) %>% 
   mutate(author1 = reorder(author1, b_Intercept)) %>%
   filter(author1 == c("Pooled Effect"))

## Now to make the plot combining the 4 curves of interest
decade_posterior_mean <-  IVdat_del$yi[IVdat_del$author1 == "Turan 2020"]
decade_posterior_sd <- IVdat_del$sei[IVdat_del$author1 == "Turan 2020"]

ma_curves <- ggplot(data = data.frame(x = c(-10, 10)), aes(x)) +
    stat_slab(aes(x = exp(b_Intercept), color = "pooled_incl_decade"), data = forest.data_decade %>% filter(author1 == "Pooled Effect"),
                      slab_linewidth = 0.9, fill = NA, alpha = 0.7) +
   stat_slab(aes(x = exp(b_Intercept), color = "decade_shrink"), data = forest.data_decade %>% filter(author1 == "Turan 2020"), 
                    slab_linewidth = 0.9, fill = NA, alpha = 0.7) +
   stat_slab(aes(x = exp(b_Intercept), color = "pooled_excl_decade"), data = forest.data_no_decade, 
                    slab_linewidth = 0.9, fill = NA, alpha = 0.7) +
  stat_slab(aes(xdist = exp(distributional::dist_normal(mean = decade_posterior_mean, sd = decade_posterior_sd)), color = "decade_alone"), 
            fill = NA, slab_linewidth = 0.9, inherit.aes = FALSE) +
  scale_x_log10(breaks = c(0.25, 0.5, 1, 2, 4), expand = c(0, 0)) +
  coord_cartesian(xlim = c(0.25, 4)) +
  ggdist::scale_thickness_shared() +
  geom_vline(xintercept=1) +
  annotate("text", x = 0.35, y = 0.9, label = "Favours\ndexmedetomidine", hjust = 0, size = 4) +
  annotate("text", x = 2, y = 0.9, label = "Favours\ncontrol", hjust = 0, size = 4) +
  theme_light() +
  theme(
    axis.text.y = element_blank(),
    legend.position = "none") +
  labs(x = "Odds ratio") +
  ylab(NULL) +
  geom_vline(xintercept = c(0.84, 1.20), linetype = "dotted", color = "grey30") +
  annotate("rect", xmin = 0.84, xmax = 1.20, ymin = -Inf, ymax = Inf, fill = "grey", alpha = 0.2) +
  scale_color_manual(values = RColorBrewer::brewer.pal(4, "Set1"),
                     breaks = c("pooled_incl_decade", "decade_shrink", "pooled_excl_decade",
                                "decade_alone"),
                     labels = c("Pooled including DECADE", "DECADE shrinkage", 
                                "Pooled excluding DECADE", "DECADE alone"),
                     name = "Posterior") 

## Now for the ECDFs

# Compute density values for the normal curve of DECADE
x <- seq(-2, 2, length.out = 100000)
density <- rnorm(x, mean = IVdat_del[IVdat_del$author == "Turan", "yi"], 
                 sd = IVdat_del[IVdat_del$author == "Turan", "sei"])
df_density <- data.frame(x = x, density = density)

library(scales)

ecdfs <- ggplot(aes(x = exp(b_Intercept), color = "pooled_excl_decade"), data = forest.data_no_decade) +
  stat_ecdf(geom = "step") +
  stat_ecdf(aes(x = exp(b_Intercept), color = "pooled_incl_decade"), data = forest.data_decade %>% filter(author1 == "Pooled Effect"), geom = "step") +
  stat_ecdf(aes(x = exp(b_Intercept), color = "decade_shrink"), data = forest.data_decade %>% filter(author1 == "Turan 2020"), geom = "step") +
  stat_ecdf(aes(x = exp(density), color = "decade_alone"), data = df_density, geom = "step") +
  scale_y_continuous(name = "Probability that mean OR < x", limits = c(0, 1), breaks = seq(0, 1, 0.1), labels = percent_format(),
                     sec.axis = sec_axis(~ 1 - ., name = "Probability that mean OR > x",
                                          breaks = seq(0, 1, 0.1), labels = percent_format())) +
  geom_vline(xintercept = 1) +
  scale_x_log10(breaks = c(0.25, 0.5, 1, 2, 4), expand = c(0, 0)) +
  coord_cartesian(xlim = c(0.25, 4)) +
  theme_light() +
  theme(
    legend.position = c(0.9, 0.5),  
    legend.justification = c(1, 1),
    legend.background = element_rect(fill = "lightgrey", color = "black")) +
  theme(axis.line = element_blank(),           # remove axis lines
        axis.text.x = element_blank(),         # remove x-axis tick labels
        axis.ticks.x = element_blank()) +
  xlab(NULL) +
  geom_vline(xintercept = c(0.84, 1.20), linetype = "dotted", color = "grey30") +
  annotate("rect", xmin = 0.84, xmax = 1.20, ymin = -Inf, ymax = Inf, fill = "grey", alpha = 0.2) +
  scale_color_manual(values = RColorBrewer::brewer.pal(4, "Set1"),
                     breaks = c("pooled_incl_decade", "decade_shrink", "pooled_excl_decade",
                                "decade_alone"),
                     labels = c("Pooled including DECADE", "DECADE shrinkage", 
                                "Pooled excluding DECADE", "DECADE alone"),
                     name = "Posterior") +
  guides(color = guide_legend(override.aes = list(linetype = "solid", shape = NA)))

layout <- c(
  area(t = 0, l = 0, b = 12, r = 30),
  area(t = 13, l = 0, b = 20, r = 30))

ecdfs + ma_curves + plot_layout(design = layout)

```

# Metaregression

Now let's consider how the effect of dexmedetomidine may across populations vary due to various factors. This is called metaregression, which is just the meta-analysis equivalent of multivariable linear regression. By glancing at it, it just looks like subgroup group, which is correct. *Subgroup analysis is just a form of metaregression with a categorical moderator.*

Below I've used similar priors as I used for our primary analysis. Although, here we are interested in the `moderator` coefficient, rather than the intercept coefficient. So my prior for the coefficient of interest is $N(0, 0.82)$ while for the intercept coefficient it is $N(0, 1)$. I retain the same ${\tau}$ priors as in our primary analysis: $Cauchy(0, 0.5)$.

``` r
priors_metareg <- brms::prior(normal(0, 1), class = b, coef = "Intercept") +
                  brms::prior(cauchy(0,0.5), class = sd) +
                  brms::prior(normal(0,0.82), class = b)
```

I also provide a comparison between categorical moderators where there are two levels (dose, age). You can do this with three or more moderators but you need to compare things 1 vs. 1, so the number of comparisons starts rising with more levels per moderator. But this is something we can do if we want to. For the 'difference' plots, it is calculating the difference between: $\text{top plot} - \text{bottom plot}$.

In the plot below @fig-metareg-brms, the blue curve is the posterior distribution of the regression coefficient draws for each outcome. The orange line is the prediction interval.

## Forest plot all subgroups

```{r}
#| label: fig-metareg-brms
#| fig-width: 13
#| fig-height: 8
#| fig-cap: |
#|   Metaregression plots.

# First, age
# Extract the pooled result data
pooled.effect.draws_age_60No <- spread_draws(m.brm_age, b_age_60No) %>%
    mutate(author1 = "Age <60") %>%
  rename(coeff = b_age_60No)

pooled.effect.draws_age_60Yes <- spread_draws(m.brm_age, b_age_60Yes) %>%
    mutate(author1 = "Age >60") %>%
  rename(coeff = b_age_60Yes)

# Combine the pooled result with study estimates, then play around with the lexicon (bmrs mucks this up)
forest.data_age <- bind_rows(pooled.effect.draws_age_60No,
                           pooled.effect.draws_age_60Yes) %>%
    ungroup() %>%
    mutate(author1 = str_replace_all(author1, "[.]", " "))
  
# Calculate median qi based on author1
forest.data.summary_age <- group_by(forest.data_age, author1) %>%
    median_qi(coeff) 

# Now repeat all the above for age difference
  # Extract the pooled result data
pooled.effect.draws_age_diff <- spread_draws(m.brm_age_diff, b_age_60Yes) %>%
    mutate(author1 = "Age Difference") %>%
   rename(coeff = b_age_60Yes)

  # Combine the pooled result with study estimates, then play around with the lexicon (bmrs mucks this up)
forest.data_age_diff <- pooled.effect.draws_age_diff %>%
    ungroup() %>%
    mutate(author1 = str_replace_all(author1, "[.]", " "))

# Combine the two forest dataframes
forest.data_age_comb <- rbind(forest.data_age, forest.data_age_diff) %>%
  mutate(author1 = as.factor(author1))
  
  # Calculate median qi based on author1
forest.data.summary_age_diff <- group_by(forest.data_age_diff, author1) %>%
    median_qi(coeff) 

## Then make a function to calculate the number of studies for each outcome
count_na <- function(column_name, column_value) {
  return(IVdat_del %>% filter(!!sym(column_name) == column_value) %>% nrow())
}

study_number <- c(count_na("age_60", "Yes"), count_na("age_60", "No"), 
                  count_na("age_60", "Yes") + count_na("age_60", "No"),
                  count_na("dex_dose", "High"), count_na("dex_dose", "Low"),
                  count_na("dex_dose", "High") + count_na("dex_dose", "Low"),
                  count_na("dex_dose_timing", "intraop"), count_na("dex_dose_timing", "postop"),
                  count_na("dex_dose_timing", "intraop_and_postop"))
                  
## And finally get the total number of participants for each outcome
## First, delirium in the DEX group
sum_dex_or_control_ratio <- function(column_name, column_value, type) {
  if (type == "dex") {
    column_del <- "dex_del"
    column_n <- "dex_n"
  } else if (type == "control") {
    column_del <- "control_del"
    column_n <- "control_n"
  }
  
  if (length(column_value) == 1) {
    # If a single column value is passed, filter for that value
    metareg_df <- IVdat_del %>%
      filter(!!sym(column_name) == column_value)
    
    return(paste0(sum(metareg_df[[column_del]], na.rm=TRUE), "/",
                  sum(metareg_df[[column_n]], na.rm=TRUE)))
  } else {
    # If multiple column values are passed, filter for all values and combine
    metareg_df <- IVdat_del %>%
      filter(!!sym(column_name) %in% column_value)
    
    return(paste0(sum(metareg_df[[column_del]], na.rm=TRUE), "/",
                  sum(metareg_df[[column_n]], na.rm=TRUE)))
  }
}


dex_delirium_rate <- c(sum_dex_or_control_ratio("age_60", "Yes", "dex"), sum_dex_or_control_ratio("age_60", "No", "dex"),
                       sum_dex_or_control_ratio("age_60", c("Yes", "No"), "dex"),
                  sum_dex_or_control_ratio("dex_dose", "High", "dex"), sum_dex_or_control_ratio("dex_dose", "Low", "dex"),
                  sum_dex_or_control_ratio("dex_dose", c("High", "Low"), "dex"),
                  sum_dex_or_control_ratio("dex_dose_timing", "intraop", "dex"), 
                  sum_dex_or_control_ratio("dex_dose_timing", "postop", "dex"),
                  sum_dex_or_control_ratio("dex_dose_timing", "intraop_and_postop", "dex"))


## Now the rate in the control group
control_delirium_rate <- c(sum_dex_or_control_ratio("age_60", "Yes", "control"), 
                           sum_dex_or_control_ratio("age_60", "No", "control"), 
                           sum_dex_or_control_ratio("age_60", c("Yes", "No"), "control"),
                           sum_dex_or_control_ratio("dex_dose", "High", "control"), 
                           sum_dex_or_control_ratio("dex_dose", "Low", "control"),
                           sum_dex_or_control_ratio("dex_dose", c("High", "Low"), "control"),
                  sum_dex_or_control_ratio("dex_dose_timing", "intraop", "control"), 
                  sum_dex_or_control_ratio("dex_dose_timing", "postop", "control"),
                  sum_dex_or_control_ratio("dex_dose_timing", "intraop_and_postop", "control"))

# Now bind the two age dataframes together
forest.data.summary_age_comb_pre <- rbind(forest.data.summary_age, forest.data.summary_age_diff)

# Calculate the 95% PIs using advice in https://discourse.mc-stan.org/t/prediction-interval-in-meta-analysis-with-brms/21060/3
nd_age_above_60 = data.frame(author1 = "new", age_60 = "Yes", sei = 0)
  
pred_age_above_60_df <- brms::posterior_predict(object = m.brm_age_diff,
                          newdata = nd_age_above_60,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_age_above_60_summ <- pred_age_above_60_df |> 
             data.frame() |>
             ggdist::median_hdi() |>
            rename(coeff = 1)

nd_age_below_60 = data.frame(author1 = "new", age_60 = "No", sei = 0)

pred_age_below_60_df <- brms::posterior_predict(object = m.brm_age_diff,
                          newdata = nd_age_below_60,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_age_below_60_summ <- pred_age_below_60_df |> 
        data.frame() |>
        ggdist::median_hdi() |>
        rename(coeff = 1)

# Cbome the PI's for each dose levele
pred_df_age <- rbind(pred_age_above_60_summ, pred_age_below_60_summ)
na_row <- rep(NA, ncol(pred_df_age)) ## Make an extra row of NAs for dose difference as this is needed for plots
pred_df_age <- rbind(pred_df_age, na_row)
pred_df_age$author1 <- as.factor(c("Age >60", "Age <60", "Age Difference"))
pred_df_age <- pred_df_age[c(7, 1:3)] %>%
  rename(pi_median = coeff,
         pi_lower = .lower,
         pi_upper = .upper)



# Now combine the PI's with the other data
forest.data.summary_age_comb <- left_join(forest.data.summary_age_comb_pre, pred_df_age, by = "author1") %>%
  mutate(author1 = as.factor(author1),
         dex_del_rate_frac = c(sum_dex_or_control_ratio("age_60", "No", "dex"), sum_dex_or_control_ratio("age_60", "Yes", "dex"),
                       sum_dex_or_control_ratio("age_60", c("Yes", "No"), "dex")),
         control_del_rate_frac = c(sum_dex_or_control_ratio("age_60", "No", "control"), 
                           sum_dex_or_control_ratio("age_60", "Yes", "control"), 
                           sum_dex_or_control_ratio("age_60", c("Yes", "No"), "control")),
         study_number = as.character(c(count_na("age_60", "No"), count_na("age_60", "Yes"), 
                  count_na("age_60", "Yes") + count_na("age_60", "No"))),
         unweighted_effect = paste0(sprintf('%.2f', exp(coeff)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']'),
         prediction_int = paste0(sprintf('%.2f', exp(pi_median)), 
                  ' [', sprintf('%.2f', exp(pi_lower)),
                  ', ', sprintf('%.2f', exp(pi_upper)), ']')) %>%
         mutate(prediction_int = ifelse(prediction_int == "NA [NA, NA]", "", prediction_int))


# Now make a dataframe to make the forest plot helpful

new_row <- data.frame(author1 = as.factor("Age"),
                      unweighted_effect = "OR [95%CrI]",
                      dex_del_rate_frac = "Delirium (n/total)",
                      control_del_rate_frac = "Delirium (n/total)",
                      study_number = "No. Studies",
                      prediction_int = "OR [95%CrI]")

res_plot <- bind_rows(forest.data.summary_age_comb, new_row) %>%
          mutate(color = ifelse(author1 == "Age Difference", "grey60", "black"),
                 font = ifelse(author1 == "Age Difference", "bold", "plain"))

## Add an extra row to res_plot_control because we need more space at the top
new_row_res_plot <- data.frame(author1 = as.factor("Subgroup"),
                      unweighted_effect = "Credible interval",
                      dex_del_rate_frac = "DEX",
                      control_del_rate_frac = "Control",
                      study_number = "",
                      prediction_int = "Prediction interval",
                      color = "black",
                      font = "plain")

res_plot_age <- bind_rows(res_plot, new_row_res_plot)

# Now, for dose
# Extract the pooled result data
pooled.effect.draws_dex_doseHigh <- spread_draws(m.brm_dose, b_dex_doseHigh) %>%
    mutate(author1 = "High") %>%
  rename(coeff = b_dex_doseHigh)

pooled.effect.draws_dex_doseLow <- spread_draws(m.brm_dose, b_dex_doseLow) %>%
    mutate(author1 = "Low") %>%
  rename(coeff = b_dex_doseLow)

# Combine the pooled result with study estimates, then play around with the lexicon (bmrs mucks this up)
forest.data_dose <- bind_rows(pooled.effect.draws_dex_doseHigh,
                           pooled.effect.draws_dex_doseLow) %>%
    ungroup() %>%
    mutate(author1 = str_replace_all(author1, "[.]", " "))
  
# Calculate median qi based on author1
forest.data.summary_dose <- group_by(forest.data_dose, author1) %>%
    median_qi(coeff) 

# Now repeat all the above for dose difference
  # Extract the pooled result data
pooled.effect.draws_dose_diff <- spread_draws(m.brm_dose_diff, b_dex_doseLow) %>%
    mutate(author1 = "Dose Difference") %>%
   rename(coeff = b_dex_doseLow)

  # Combine the pooled result with study estimates, then play around with the lexicon (bmrs mucks this up)
forest.data_dose_diff <- pooled.effect.draws_dose_diff %>%
    ungroup() %>%
    mutate(author1 = str_replace_all(author1, "[.]", " "))

# Combine the two forest dataframes
forest.data_dose_comb <- rbind(forest.data_dose, forest.data_dose_diff) %>%
  mutate(author1 = as.factor(author1))
  
  # Calculate median qi based on author1
forest.data.summary_dose_diff <- group_by(forest.data_dose_diff, author1) %>%
    median_qi(coeff) 

# Now bind the two age dataframes together
forest.data.summary_dose_comb_pre <- rbind(forest.data.summary_dose, forest.data.summary_dose_diff) 

# Now create the PIs for dose
nd_dose_high = data.frame(author1 = "new", dex_dose = "High", sei = 0)
  
pred_dose_high_df <- brms::posterior_predict(object = m.brm_dose_diff,
                          newdata = nd_dose_high,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_dose_high_summ <- pred_dose_high_df |> 
             data.frame() |>
             ggdist::median_hdi() |>
            rename(coeff = 1)

nd_dose_low = data.frame(author1 = "new", dex_dose = "Low", sei = 0)

pred_dose_low_df <- brms::posterior_predict(object = m.brm_dose_diff,
                          newdata = nd_dose_low,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_dose_low_summ <- pred_dose_low_df |> 
        data.frame() |>
        ggdist::median_hdi() |>
        rename(coeff = 1)

# Cbome the PI's for each dose level and the difference
pred_df_dose <- rbind(pred_dose_high_summ, pred_dose_low_summ)
na_row <- rep(NA, ncol(pred_df_dose)) ## Make an extra row of NAs for dose difference as this is needed for plots
pred_df_dose <- rbind(pred_df_dose, na_row)
pred_df_dose$author1 <- as.factor(c("High", "Low", "Dose Difference"))
pred_df_dose <- pred_df_dose[c(7, 1:3)] %>%
  rename(pi_median = coeff,
         pi_lower = .lower,
         pi_upper = .upper)

# Now combine PI with previous estimates
forest.data.summary_dose_comb <- left_join(forest.data.summary_dose_comb_pre, pred_df_dose, by = "author1") %>%
  mutate(author1 = as.factor(author1),
         dex_del_rate_frac = c(sum_dex_or_control_ratio("dex_dose", "High", "dex"), sum_dex_or_control_ratio("dex_dose", "Low", "dex"),
                       sum_dex_or_control_ratio("dex_dose", c("Low", "High"), "dex")),
         control_del_rate_frac = c(sum_dex_or_control_ratio("dex_dose", "High", "control"), 
                           sum_dex_or_control_ratio("dex_dose", "Low", "control"), 
                           sum_dex_or_control_ratio("dex_dose", c("Low", "High"), "control")),
         study_number = as.character(c(count_na("dex_dose", "High"), count_na("dex_dose", "Low"), 
                  count_na("dex_dose", "Low") + count_na("dex_dose", "High"))),
         unweighted_effect = paste0(sprintf('%.2f', exp(coeff)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']'),
                  prediction_int = paste0(sprintf('%.2f', exp(pi_median)), 
                  ' [', sprintf('%.2f', exp(pi_lower)),
                  ', ', sprintf('%.2f', exp(pi_upper)), ']')) %>%
          mutate(prediction_int = ifelse(prediction_int == "NA [NA, NA]", "", prediction_int))

new_row <- data.frame(author1 = as.factor("Dose"),
                      unweighted_effect = "",
                      dex_del_rate_frac = "",
                      control_del_rate_frac = "",
                      study_number = "")


res_plot_dose <- bind_rows(forest.data.summary_dose_comb, new_row) %>%
          mutate(color = ifelse(author1 == "Dose Difference", "grey60", "black"),
                 font = ifelse(author1 == "Dose Difference", "bold", "plain"))

## Now repeat the above for dex timing

# Extract the pooled result data
pooled.effect.draws_intraop <- spread_draws(m.brm_dose_timing, b_dex_dose_timingintraop) %>%
    mutate(author1 = "Intraop") %>%
  rename(coeff = b_dex_dose_timingintraop)

pooled.effect.draws_postop <- spread_draws(m.brm_dose_timing, b_dex_dose_timingpostop) %>%
    mutate(author1 = "Postop") %>%
  rename(coeff = b_dex_dose_timingpostop)

pooled.effect.draws_intraop_and_postop <- spread_draws(m.brm_dose_timing, b_dex_dose_timingintraop_and_postop) %>%
    mutate(author1 = "Intraop + postop") %>%
  rename(coeff = b_dex_dose_timingintraop_and_postop)


# Combine the pooled result with study estimates, then play around with the lexicon (bmrs mucks this up)
forest.data_dose_timing <- bind_rows(pooled.effect.draws_intraop, pooled.effect.draws_postop, pooled.effect.draws_intraop_and_postop) %>%
    mutate(author1 = str_replace_all(author1, "[.]", " ")) %>%
    mutate(author1 = as.factor(author1))
  
# Calculate median qi based on author1
forest.data.summary_dose_timing <- group_by(forest.data_dose_timing, author1) %>%
    median_qi(coeff) 

# Now create the PIs for dex dose timing
nd_intraop = data.frame(author1 = "new", dex_dose_timing = "intraop", sei = 0)
  
pred_intraop_df <- brms::posterior_predict(object = m.brm_dose_timing,
                          newdata = nd_intraop,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_intraop_summ <- pred_intraop_df |> 
             data.frame() |>
             ggdist::median_hdi() |>
            rename(coeff = 1)

nd_postop = data.frame(author1 = "new", dex_dose_timing = "postop", sei = 0)
  
pred_postop_df <- brms::posterior_predict(object = m.brm_dose_timing,
                          newdata = nd_postop,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_postop_summ <- pred_postop_df |> 
             data.frame() |>
             ggdist::median_hdi() |>
            rename(coeff = 1)

nd_intraop_and_postop = data.frame(author1 = "new", dex_dose_timing = "intraop_and_postop", sei = 0)
  
pred_intraop_and_postop_df <- brms::posterior_predict(object = m.brm_dose_timing,
                          newdata = nd_intraop_and_postop,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

pred_intraop_and_postop_summ <- pred_intraop_and_postop_df |> 
             data.frame() |>
             ggdist::median_hdi() |>
            rename(coeff = 1)

# Cbome the PI's for each dose level and the difference
pred_df_dose_timing <- rbind(pred_intraop_summ, pred_postop_summ, pred_intraop_and_postop_summ)
pred_df_dose_timing$author1 <- as.factor(c("Intraop", "Postop", "Intraop + postop"))
pred_df_dose_timing <- pred_df_dose_timing[c(7, 1:3)] %>%
  rename(pi_median = coeff,
         pi_lower = .lower,
         pi_upper = .upper)

# Now combine PI with previous estimates
forest.data.summary_dose_timing_comb <- left_join(forest.data.summary_dose_timing, pred_df_dose_timing, by = "author1") %>%
  mutate(author1 = as.factor(author1),
         dex_del_rate_frac = c(sum_dex_or_control_ratio("dex_dose_timing", "intraop", "dex"), 
                               sum_dex_or_control_ratio("dex_dose_timing", "intraop_and_postop", "dex"),
                               sum_dex_or_control_ratio("dex_dose_timing", "postop", "dex")),
         control_del_rate_frac = c(sum_dex_or_control_ratio("dex_dose_timing", "intraop", "control"),
                               sum_dex_or_control_ratio("dex_dose_timing", "intraop_and_postop", "control"),
                               sum_dex_or_control_ratio("dex_dose_timing", "postop", "control")),
         study_number = as.character(c(count_na("dex_dose_timing", "intraop"), 
                                       count_na("dex_dose_timing", "intraop_and_postop"),
                                       count_na("dex_dose_timing", "postop"))),
         unweighted_effect = paste0(sprintf('%.2f', exp(coeff)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']'),
          prediction_int = paste0(sprintf('%.2f', exp(pi_median)), 
                  ' [', sprintf('%.2f', exp(pi_lower)),
                  ', ', sprintf('%.2f', exp(pi_upper)), ']')) %>%
         mutate(prediction_int = ifelse(prediction_int == "NA [NA, NA]", "", prediction_int))

new_row <- data.frame(author1 = as.factor("Dex dose timing"),
                      unweighted_effect = "",
                      dex_del_rate_frac = "",
                      control_del_rate_frac = "",
                      study_number = "")


res_plot_dose_timing <- bind_rows(forest.data.summary_dose_timing_comb, new_row)

## Now make all the plots!
age_forest <- ggplot(aes(exp(coeff), 
           relevel(author1, "Age Difference", after=Inf)), 
       data = forest.data_age_comb) +
  geom_vline(xintercept = 1, color = "black", 
             size = 1) +
  # Add densities
   stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
    geom_pointintervalh(aes(xmin = exp(pi_lower), 
                          xmax = exp(pi_upper), 
                          x = exp(pi_median)),
                      position = position_nudge(y = -0.25),
                      data = pred_df_age, 
                      col = "darkorange",
                      size = 4) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,5)) +
  annotate("text", x = 0.35, y =4.5, label = "Favours\ndexmedetomidine") +
  annotate("text", x = 3, y = 4.5,  label = "Favours\ncontrol") +
  theme_light() +
  theme(axis.line = element_blank(),           # remove axis lines
        axis.text.x = element_blank(),         # remove x-axis tick labels
        axis.ticks.x = element_blank(),        # remove x-axis ticks
        axis.text.y = element_blank()) +
  xlab(NULL) +
  ylab(NULL)

age_estimates <- ggplot(aes(y = relevel(author1, "Age Difference", after=Inf), color = color), 
                   data = res_plot_age) +
  geom_text(aes(x = 0, label = unweighted_effect), hjust = 0,
            fontface = ifelse(res_plot_age$unweighted_effect == "Credible interval" | res_plot_age$unweighted_effect == "OR [95%CrI]", "bold", "plain")) +
  geom_text(aes(x = 2, label = prediction_int), hjust = 0,
            fontface = ifelse(res_plot_age$prediction_int == "Prediction interval" | res_plot_age$prediction_int == "OR [95%CrI]", "bold", "plain")) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 4))

age_studies <- ggplot(aes(y = relevel(author1, "Age Difference", after=Inf), color = color), 
                          data = res_plot_age) +
  geom_text(aes(x = 0, label = author1), hjust = 0, 
            fontface = ifelse(res_plot_age$author1 == "Subgroup" | res_plot_age$author1 == "Age", "bold",
                              ifelse(res_plot_age$author1 == "Age Difference", "italic","plain"))) +
  geom_text(aes(x = 2, label = dex_del_rate_frac), hjust = 0, 
            fontface = ifelse(res_plot_age$dex_del_rate_frac == "Delirium (n/total)" | res_plot_age$dex_del_rate_frac == "DEX", "bold", "plain")) +
  geom_text(aes(x = 4, label = control_del_rate_frac), hjust = 0, 
            fontface = ifelse(res_plot_age$control_del_rate_frac == "Delirium (n/total)" | res_plot_age$control_del_rate_frac == "Control", "bold", "plain")) +
  geom_text(aes(x = 6, label = study_number), hjust = 0, 
            fontface = ifelse(res_plot_age$study_number == "No. Studies", "bold", "plain")) +
  scale_color_identity() + # Use the specified colors directly for text color
  theme_void() +
  coord_cartesian(xlim = c(0, 8))

dose_forest <- ggplot(aes(exp(coeff), 
           relevel(author1, "Dose Difference", after=Inf)), 
       data = forest.data_dose_comb) +
  geom_vline(xintercept = 1, color = "black", 
             size = 1) +
   stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
    geom_pointintervalh(aes(xmin = exp(pi_lower), 
                          xmax = exp(pi_upper), 
                          x = exp(pi_median)),
                      position = position_nudge(y = -0.25),
                      data = pred_df_dose, 
                      col = "darkorange",
                      size = 4) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,4)) +
  theme_light() +
  theme(axis.line = element_blank(),           # remove axis lines
        axis.text.x = element_blank(),         # remove x-axis tick labels
        axis.ticks.x = element_blank(),        # remove x-axis ticks
        axis.text.y = element_blank()) +
  xlab(NULL) +
  ylab(NULL)

dose_estimates <- ggplot(aes(y = relevel(author1, "Dose Difference", after=Inf), color = color), 
                   data = res_plot_dose) +
  geom_text(aes(x = 0, label = unweighted_effect), hjust = 0) +
  geom_text(aes(x = 2, label = prediction_int), hjust = 0) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 4))

dose_studies <- ggplot(aes(y = relevel(author1, "Dose Difference", after=Inf), color = color), 
                          data = res_plot_dose) +
  geom_text(aes(x = 0, label = author1), hjust = 0,
            fontface = ifelse(res_plot_dose$author1 == "Dose", "bold",
                              ifelse(res_plot_dose$author1 == "Dose Difference", "italic","plain"))) +
  geom_text(aes(x = 2, label = dex_del_rate_frac), hjust = 0) +
  geom_text(aes(x = 4, label = control_del_rate_frac), hjust = 0) +
  geom_text(aes(x = 6, label = study_number), hjust = 0) +
  scale_color_identity() + # Use the specified colors directly for text color
  theme_void() +
  coord_cartesian(xlim = c(0, 8))

## Finally, dose_timing
dose_timing_forest <- ggplot(aes(exp(coeff), relevel(author1, "Intraop + postop", after=Inf)), 
       data = forest.data_dose_timing) +
  geom_vline(xintercept = 1, color = "black", 
             size = 1) +
  stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
    geom_pointintervalh(aes(xmin = exp(pi_lower), 
                          xmax = exp(pi_upper), 
                          x = exp(pi_median)),
                      position = position_nudge(y = -0.25),
                      data = pred_df_dose_timing, 
                      col = "darkorange",
                      size = 4) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,4)) +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Odds ratio (log scale)") +
  ylab(NULL)

dose_timing_estimates <- ggplot(aes(y = relevel(author1, "Intraop + postop", after=Inf)), 
                   data = res_plot_dose_timing) +
  geom_text(aes(x = 0, label = unweighted_effect), hjust = 0) +
  geom_text(aes(x = 2, label = prediction_int), hjust = 0) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 4))

dose_timing_studies <- ggplot(aes(y = relevel(author1, "Intraop + postop", after=Inf)), 
                          data = res_plot_dose_timing) +
  geom_text(aes(x = 0, label = author1), hjust = 0, 
            fontface = ifelse(res_plot_dose_timing$author1 == "Dex dose timing", "bold","plain")) +
  geom_text(aes(x = 2, label = dex_del_rate_frac), hjust = 0) +
  geom_text(aes(x = 4, label = control_del_rate_frac), hjust = 0) +
  geom_text(aes(x = 6, label = study_number), hjust = 0) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 8))

layout <- c(
  area(t = 0, l = 0, b = 12, r = 40),
  area(t = 0, l = 38, b = 12, r = 60), 
  area(t = 0, l = 62, b = 12, r = 85),
  area(t = 14, l = 0, b = 24, r = 40),
  area(t = 14, l = 38, b = 24, r = 60), 
  area(t = 14, l = 62, b = 24, r = 85),
  area(t = 26, l = 0, b = 40, r = 40),
  area(t = 26, l = 38, b = 40, r = 60), 
  area(t = 26, l = 62, b = 40, r = 85))

age_studies + age_forest + age_estimates + dose_studies + dose_forest + dose_estimates +
  dose_timing_studies + dose_timing_forest + dose_timing_estimates + plot_layout(design = layout)

```

# Secondary outcomes

Now for the secondary outcomes. For these I used the same priors as in our primary analysis.

## Forest plot

For the 'Mean (SD)' columns for the continuous outcomes, I used the unweighted grand mean (means of all the studys' means and SDs). As such, these numbers as essentially arbitrary (because means from tiny studies are weighted equally to means from massive studies). I just present these numbers to give the reader an idea of the sorts of values that were observed in the included studies.

```{r}
#| label: fig-secondary-plots
#| fig-width: 12
#| fig-height: 8
#| results: 'hide'
#| message: FALSE
#| warning: FALSE
#| errors: FALSE
#| fig-cap: |
#|   Forest plot of the secondary outcomes.

## First let's start of with our binary secondary outcomes
df_mortality <-  spread_draws(m.brm_mortality, b_Intercept) %>%
  mutate(outcome = "Mortality")

df_bradycardia <-  spread_draws(m.brm_bradycardia, b_Intercept) %>%
  mutate(outcome = "Bradycardia")

df_hypotension <-  spread_draws(m.brm_hypotension, b_Intercept) %>%
  mutate(outcome = "Hypotension")

df_arrhythmia <-  spread_draws(m.brm_arrhythmia, b_Intercept) %>%
  mutate(outcome = "Arrhythmia")

forest.data_secondary_dichot <- rbind(df_mortality, df_bradycardia, df_hypotension, df_arrhythmia) %>%
  mutate(outcome = as.factor(outcome))

pred_fn <- function(m.brm, outcome) {
  nd = data.frame(author1 = "new", sei = 0)
  
  pred_summ <- brms::posterior_predict(object = m.brm,
                          newdata = nd,
                          re_formula = NULL,
                          allow_new_levels = TRUE,
                          sample_new_levels = "gaussian") 

  pred_df <- pred_summ |> 
             data.frame() |>
             ggdist::median_hdi(.width = c(.95)) |>
            rename(coeff = 1)

  pred_df$outcome <- as.factor(c(outcome))
  pred_df <- pred_df[c(7, 1:3)] %>%
  rename(pi_median = coeff,
         pi_lower = .lower,
         pi_upper = .upper)
}

# Combine the prediction intervals into a dataframe
pred_df_secondary_dichot <- rbind(pred_fn(m.brm_mortality, "Mortality"), pred_fn(m.brm_bradycardia, "Bradycardia"), pred_fn(m.brm_hypotension, "Hypotension"), 
                           pred_fn(m.brm_arrhythmia, "Arrhythmia"))

# Extract odds ratios and 95% CIs
est_fn_dichot <- function(m.brm_secondary) {
  spread_draws(m.brm_secondary, b_Intercept) %>%
    median_qi() %>%
    mutate(effect = paste0(sprintf('%.2f', exp(b_Intercept)), 
                  ' [', sprintf('%.2f', exp(.lower)),
                  ', ', sprintf('%.2f', exp(.upper)), ']'))
}

tau_fn_secondary <- function(m.brm_secondary) {
  spread_draws(m.brm_secondary, sd_author1__Intercept) %>%
    median_qi() %>%
    mutate(tau = paste0(sprintf('%.2f', sd_author1__Intercept), 
                  ' [', sprintf('%.2f', .lower),
                  ', ', sprintf('%.2f', .upper), ']'))
}

estimates_secondary_dichot <- rbind(est_fn_dichot(m.brm_mortality), est_fn_dichot(m.brm_bradycardia), 
                             est_fn_dichot(m.brm_hypotension), est_fn_dichot(m.brm_arrhythmia))
tau_secondary_dichot <- rbind(tau_fn_secondary(m.brm_mortality), tau_fn_secondary(m.brm_bradycardia), 
                       tau_fn_secondary(m.brm_hypotension), tau_fn_secondary(m.brm_arrhythmia))

comb_estimates_tau_dichot <- cbind(estimates_secondary_dichot, tau_secondary_dichot) %>%
  dplyr::select(effect, tau)

## Then make a function to calculate the number of studies for each outcome
count_na <- function(name) {
  return(dat %>% filter(!is.na(!!sym(paste0("dex_", name)))) %>% nrow())
}
study_number_dichot <- c(count_na("mortality"), count_na("bradycardia"), count_na("hypotension"), count_na("arrhythmia"))

## And finally get the total number of participants for each outcome
sum_dex_ratio <- function(name) {
  return(paste0(sum(dat[!is.na(dat[[paste0("dex_", name)]]),][[paste0("dex_", name)]], na.rm=TRUE), "/",
                sum(dat[!is.na(dat[[paste0("dex_", name)]]),][["dex_n"]], na.rm=TRUE)))
}

sum_control_ratio <- function(name) {
  return(paste0(sum(dat[!is.na(dat[[paste0("control_", name)]]),][[paste0("control_", name)]], na.rm=TRUE), "/",
                sum(dat[!is.na(dat[[paste0("control_", name)]]),][["control_n"]], na.rm=TRUE)))
}

dex_number <- c(sum_dex_ratio("mortality"), sum_dex_ratio("bradycardia"), 
                        sum_dex_ratio("hypotension"), sum_dex_ratio("arrhythmia"))

control_number <- c(sum_control_ratio("mortality"), sum_control_ratio("bradycardia"), 
                        sum_control_ratio("hypotension"), sum_control_ratio("arrhythmia"))

## Combine all these into a dataframe
tabdat_pre <- data.frame("outcome" = as.factor(c("Mortality", "Bradycardia", "Hypotension",
                                "Arrhythmia")),
                     "study_number" = as.character(study_number_dichot),
                     "dex_number" = dex_number,
                     "control_number" = control_number,
                      comb_estimates_tau_dichot)

new_row <- data.frame(outcome = as.factor("Outcome"),
                      effect = "[95%CrI]",
                      tau = "[95%CrI]",
                      dex_number = "n/total",
                      control_number = "n/total",
                      study_number = "No. Studies")

## Add an extra row because we need more space at the top
new_row_tabdat <- data.frame(outcome = as.factor(""),
                      effect = "Odds ratio",
                      tau = "τ",
                      dex_number = "DEX",
                      control_number = "Control",
                      study_number = "")

tabdat_dichot <- bind_rows(tabdat_pre, new_row, new_row_tabdat)

p_forest_secondary_dichot <- ggplot(aes(exp(b_Intercept), 
           relevel(outcome, "Arrhythmia", after=Inf)), 
       data = forest.data_secondary_dichot) +
  geom_vline(xintercept = 1, color = "black", 
             size = 0.7) +
  stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
    geom_pointintervalh(aes(xmin = exp(pi_lower), 
                          xmax = exp(pi_upper), 
                          x = exp(pi_median)),
                      position = position_nudge(y = -0.2),
                      data = pred_df_secondary_dichot, 
                      col = "darkorange",
                      size = 6) +
  scale_x_log10(breaks = c(0.1, 0.25, 0.5, 1, 2, 4, 7), expand = c(0, 0)) +           
  coord_cartesian(xlim=c(0.1, 7), ylim=c(1,6)) +
  annotate("text", x = 0.35, y =5.5, label = "Greater with\ncontrol") +
  annotate("text", x = 3, y = 5.5,  label = "Greater with\ndexmedetomidine") +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Odds ratio (log scale)") +
  ylab(NULL) +
  guides(alpha = "none")

p_estimates_secondary_dichot <- ggplot(aes(y = relevel(outcome, "Arrhythmia", after=Inf)), 
                   data = tabdat_dichot) +
  geom_text(aes(x = 0, label = effect), hjust = 0,
            fontface = ifelse(grepl("Odds ratio|\\[95%CrI\\]", tabdat_dichot$effect), "bold", "plain")) +
  geom_text(aes(x = 1, label = tau), hjust = 0, 
            fontface = ifelse(grepl("τ|\\[95%CrI\\]", tabdat_dichot$tau), "bold", "plain")) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 2))

# Map the fill and color aesthetics to the new column in the ggplot call
p_studies_secondary_dichot <- ggplot(aes(y = relevel(outcome, "Arrhythmia", after=Inf)), 
                          data = tabdat_dichot) +
  geom_text(aes(x = 0, label = outcome), hjust = 0, 
            fontface = ifelse(tabdat_dichot$outcome == "Outcome", "bold","plain")) +
  geom_text(aes(x = 2, label = study_number), hjust = 0, 
            fontface = ifelse(tabdat_dichot$study_number == "No. Studies", "bold", "plain")) +
  geom_text(aes(x = 4, label = dex_number), hjust = 0, 
            fontface = ifelse(tabdat_dichot$dex_number == "n/total" | tabdat_dichot$dex_number == "DEX", "bold", "plain")) +
  geom_text(aes(x = 6, label = control_number), hjust = 0, 
            fontface = ifelse(tabdat_dichot$control_number == "n/total" | tabdat_dichot$control_number == "Control", "bold", "plain")) +
  scale_color_identity() + # Use the specified colors directly for text color
  theme_void() +
  coord_cartesian(xlim = c(0, 8))

## Now let's look at our continous secondary outcomes
df_del_duration <-  spread_draws(m.brm_del_duration, b_Intercept) %>%
  mutate(outcome = "Delirium duration")

df_extub_time <-  spread_draws(m.brm_extub_time, b_Intercept) %>%
  mutate(outcome = "Time to extubation")

df_hosp_stay <-  spread_draws(m.brm_hosp_stay, b_Intercept) %>%
  mutate(outcome = "Hospital stay")

df_icu_stay <-  spread_draws(m.brm_icu_stay, b_Intercept) %>%
  mutate(outcome = "ICU stay")

forest.data_secondary_cont <- rbind(df_del_duration, df_extub_time, df_hosp_stay, df_icu_stay) %>%
  mutate(outcome = as.factor(outcome))

# Combine the prediction intervals into a dataframe
pred_df_secondary_cont <- rbind(pred_fn(m.brm_del_duration, "Delirium duration"), pred_fn(m.brm_extub_time, "Time to extubation"), pred_fn(m.brm_hosp_stay, "Hospital stay"), 
                           pred_fn(m.brm_icu_stay, "ICU stay"))


# Extract MDs and 95% CIs
est_fn_cont <- function(m.brm_secondary) {
  spread_draws(m.brm_secondary, b_Intercept) %>%
    median_qi() %>%
    mutate(effect = paste0(sprintf('%.2f', b_Intercept), 
                  ' [', sprintf('%.2f', .lower),
                  ', ', sprintf('%.2f', .upper), ']'))
}

estimates_secondary_cont <- rbind(est_fn_cont(m.brm_del_duration), est_fn_cont(m.brm_extub_time), 
                             est_fn_cont(m.brm_hosp_stay), est_fn_cont(m.brm_icu_stay))
tau_secondary_cont <- rbind(tau_fn_secondary(m.brm_del_duration), tau_fn_secondary(m.brm_extub_time), 
                       tau_fn_secondary(m.brm_hosp_stay), tau_fn_secondary(m.brm_icu_stay))

comb_estimates_tau_cont <- cbind(estimates_secondary_cont, tau_secondary_cont) %>%
  dplyr::select(effect, tau)

study_number_cont <- c(count_na("delirium_duration_days_mean"), count_na("timetoextubation_mean"), count_na("hospital_days_mean"), count_na("icu_days_mean"))

## And finally get the overall grand mean for each outcome
dex_mean_fn <- function(study_mean, study_sd) {
  return(paste0(sprintf('%.2f', mean(dat[!is.na(dat[[paste0("dex_", study_mean)]]),][[paste0("dex_", study_mean)]], na.rm=TRUE)), " (",
                sprintf('%.2f', mean(dat[!is.na(dat[[paste0("dex_", study_sd)]]),][[paste0("dex_", study_sd)]], na.rm=TRUE)), ")"))
}

control_mean_fn <- function(study_mean, study_sd) {
  return(paste0(sprintf('%.2f', mean(dat[!is.na(dat[[paste0("control_", study_mean)]]),][[paste0("control_", study_mean)]], na.rm=TRUE)), " (",
                sprintf('%.2f', mean(dat[!is.na(dat[[paste0("control_", study_sd)]]),][[paste0("control_", study_sd)]], na.rm=TRUE)), ")"))
}

dex_mean <- c(dex_mean_fn("delirium_duration_days_mean", "delirium_duration_days_sd"), 
              dex_mean_fn("timetoextubation_mean", "timetoextubation_sd"), 
              dex_mean_fn("hospital_days_mean", "hospital_days_sd"), 
              dex_mean_fn("icu_days_mean", "icu_days_sd"))

control_mean <- c(control_mean_fn("delirium_duration_days_mean", "delirium_duration_days_sd"), 
              control_mean_fn("timetoextubation_mean", "timetoextubation_sd"), 
              control_mean_fn("hospital_days_mean", "hospital_days_sd"), 
              control_mean_fn("icu_days_mean", "icu_days_sd"))

## Combine all these into a dataframe
tabdat_pre <- data.frame("outcome" = as.factor(c("Delirium duration", "Time to extubation", "Hospital stay",
                                "ICU stay")),
                     "study_number" = as.character(study_number_cont),
                     "dex_number" = dex_mean,
                     "control_number" = control_mean,
                     comb_estimates_tau_cont)

new_row <- data.frame(outcome = as.factor("Outcome"),
                      effect = "[95%CrI]",
                      tau = "[95%CrI]",
                      dex_number = "Mean (SD)",
                      control_number = "Mean (SD)",
                      study_number = "No. Studies")

## Add an extra row because we need more space at the top
new_row_tabdat <- data.frame(outcome = as.factor(""),
                      effect = "Mean difference",
                      tau = "τ",
                      dex_number = "DEX",
                      control_number = "Control",
                      study_number = "")

tabdat_cont <- bind_rows(tabdat_pre, new_row, new_row_tabdat)

p_forest_secondary_cont <- ggplot(aes(b_Intercept, 
           relevel(outcome, "ICU stay", after=Inf)), 
       data = forest.data_secondary_cont) +
  geom_vline(xintercept = 0, color = "black", 
             size = 0.7) +
  stat_halfeye(interval_colour = "darkblue", slab_colour = "blue", point_colour = "darkblue", fill = "lightblue",
               slab_linewidth = 0.9) +
    geom_pointintervalh(aes(xmin = pi_lower, 
                          xmax = pi_upper, 
                          x = pi_median),
                      position = position_nudge(y = -0.2),
                      data = pred_df_secondary_cont, 
                      col = "darkorange",
                      size = 6) +
  scale_x_continuous(expand = c(0, 0)) +           
  coord_cartesian(xlim=c(-6, 6), ylim=c(1,6)) +
  annotate("text", x = -4, y =5.5, label = "Greater with\ncontrol") +
  annotate("text", x = 4, y = 5.5,  label = "Greater with\ndexmedetomidine") +
  theme_light() +
  theme(axis.text.y = element_blank()) +
  labs(x="Mean difference") +
  ylab(NULL) +
  guides(alpha = "none")

p_estimates_secondary_cont <- ggplot(aes(y = relevel(outcome, "ICU stay", after=Inf)), 
                   data = tabdat_cont) +
  geom_text(aes(x = 0, label = effect), hjust = 0,
            fontface = ifelse(grepl("Mean difference|\\[95%CrI\\]", tabdat_cont$effect), "bold", "plain")) +
  geom_text(aes(x = 1, label = tau), hjust = 0, 
            fontface = ifelse(grepl("τ|\\[95%CrI\\]", tabdat_cont$tau), "bold", "plain")) +
  scale_color_identity() +
  theme_void() +
  coord_cartesian(xlim = c(0, 2))

# Map the fill and color aesthetics to the new column in the ggplot call
p_studies_secondary_cont <- ggplot(aes(y = relevel(outcome, "ICU stay", after=Inf)), 
                          data = tabdat_cont) +
  geom_text(aes(x = 0, label = outcome), hjust = 0, 
            fontface = ifelse(tabdat_cont$outcome == "Outcome", "bold","plain")) +
  geom_text(aes(x = 2, label = study_number), hjust = 0, 
            fontface = ifelse(tabdat_cont$study_number == "No. Studies", "bold", "plain")) +
  geom_text(aes(x = 4, label = dex_number), hjust = 0, 
            fontface = ifelse(tabdat_cont$dex_number == "Mean (SD)" | tabdat_cont$dex_number == "DEX", "bold", "plain")) +
  geom_text(aes(x = 6, label = control_number), hjust = 0, 
            fontface = ifelse(tabdat_cont$control_number == "Mean (SD)" | tabdat_cont$control_number == "Control", "bold", "plain")) +
  scale_color_identity() + # Use the specified colors directly for text color
  theme_void() +
  coord_cartesian(xlim = c(0, 8))

library(patchwork)

layout <- c(
  area(t = 0, l = 0, b = 12, r = 38),
  area(t = 0, l = 38, b = 12, r = 58), 
  area(t = 0, l = 59, b = 12, r = 76),
  area(t = 13, l = 0, b = 25, r = 38),
  area(t = 13, l = 38, b = 25, r = 58), 
  area(t = 13, l = 59, b = 25, r = 76))

p_studies_secondary_dichot + p_forest_secondary_dichot + p_estimates_secondary_dichot + 
  p_studies_secondary_cont + p_forest_secondary_cont + p_estimates_secondary_cont + plot_layout(design = layout)
```

## Probability of harm calculations

Let's also look at the probability of dexmedetomidine causing various levels of harm, in the form of bradycardia and hypotension. This is equal and opposite to @tbl-benefit-probs-all. @tbl-harm-probs-secondary below shows the relevant figures.

```{r}
#| label: tbl-harm-probs-secondary
#| tbl-cap: Probability of harm of dexmedetomidine in causing bradycardia and hypotension, across various subgroups.
#| #| results: 'hide'
#| message: FALSE
#| warning: FALSE
#| errors: FALSE

# First - bradycardia
 # Begin with overall
# First, calculate the median delirium rate in control group
Rc_overall_bradycardia <- median(IVdat_bradycardia$control_bradycardia_rate)


nnt_50_Rt_overall_bradycardia <- Rc_overall_bradycardia + (100/50)/100
nnt_50_or_overall_bradycardia <- (nnt_50_Rt_overall_bradycardia*(Rc_overall_bradycardia - 1))/(Rc_overall_bradycardia*(nnt_50_Rt_overall_bradycardia - 1))

nnt_25_Rt_overall_bradycardia <- Rc_overall_bradycardia + (100/25)/100
nnt_25_or_overall_bradycardia <- (nnt_25_Rt_overall_bradycardia*(Rc_overall_bradycardia - 1))/(Rc_overall_bradycardia*(nnt_25_Rt_overall_bradycardia - 1))

nnt_10_Rt_overall_bradycardia <- Rc_overall_bradycardia + (100/10)/100
nnt_10_or_overall_bradycardia <- (nnt_10_Rt_overall_bradycardia*(Rc_overall_bradycardia - 1))/(Rc_overall_bradycardia*(nnt_10_Rt_overall_bradycardia - 1))

probs_overall_bradycardia = 
  m.brm_bradycardia |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_bradycardia = 100*mean(b_Intercept > log(1)),
            rrr_nnt_50_overall_bradycardia = 100*mean(b_Intercept > log(nnt_50_or_overall_bradycardia)),
            rrr_nnt_25_overall_bradycardia = 100*mean(b_Intercept > log(nnt_25_or_overall_bradycardia)),
            rrr_nnt_10_overall_bradycardia = 100*mean(b_Intercept > log(nnt_10_or_overall_bradycardia)))

## Create the 'age' dataframe
IVdat_bradycardia_age <- IVdat_bradycardia[!is.na(IVdat_bradycardia$age_60),]
m.brm_bradycardia_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_bradycardia_age, prior = priors_metareg)

# First, age below 60
Rc_age_below_60_bradycardia <- median(IVdat_bradycardia[IVdat_bradycardia$age_60=="No" &
                                               !is.na(IVdat_bradycardia$age_60),]$control_bradycardia_rate)
nnt_50_Rt_age_below_60_bradycardia <- Rc_age_below_60_bradycardia + (100/50)/100
nnt_50_or_age_below_60_bradycardia <- (nnt_50_Rt_age_below_60_bradycardia*(Rc_age_below_60_bradycardia - 1))/(Rc_age_below_60_bradycardia*(nnt_50_Rt_age_below_60_bradycardia - 1))

nnt_25_Rt_age_below_60_bradycardia <- Rc_age_below_60_bradycardia + (100/25)/100
nnt_25_or_age_below_60_bradycardia <- (nnt_25_Rt_age_below_60_bradycardia*(Rc_age_below_60_bradycardia - 1))/(Rc_age_below_60_bradycardia*(nnt_25_Rt_age_below_60_bradycardia - 1))

nnt_10_Rt_age_below_60_bradycardia <- Rc_age_below_60_bradycardia + (100/10)/100
nnt_10_or_age_below_60_bradycardia <- (nnt_10_Rt_age_below_60_bradycardia*(Rc_age_below_60_bradycardia - 1))/(Rc_age_below_60_bradycardia*(nnt_10_Rt_age_below_60_bradycardia - 1))

probs_age_below_60_bradycardia = 
  spread_draws(m.brm_bradycardia_age, b_age_60No) |> 
  summarise(any_benefit_age_below_60_bradycardia = 100*mean(b_age_60No > log(1)),
            rrr_nnt_50_age_below_60_bradycardia = 100*mean(b_age_60No > log(nnt_50_or_age_below_60_bradycardia)),
            rrr_nnt_25_age_below_60_bradycardia = 100*mean(b_age_60No > log(nnt_25_or_age_below_60_bradycardia)),
            rrr_nnt_10_age_below_60_bradycardia = 100*mean(b_age_60No > log(nnt_10_or_age_below_60_bradycardia)),
            )

# Then age above 60
Rc_age_above_60_bradycardia <- median(IVdat_bradycardia[IVdat_bradycardia$age_60=="Yes" &
                                               !is.na(IVdat_bradycardia$age_60),]$control_bradycardia_rate)
nnt_50_Rt_age_above_60_bradycardia <- Rc_age_above_60_bradycardia + (100/50)/100
nnt_50_or_age_above_60_bradycardia <- (nnt_50_Rt_age_above_60_bradycardia*(Rc_age_above_60_bradycardia - 1))/(Rc_age_above_60_bradycardia*(nnt_50_Rt_age_above_60_bradycardia - 1))

nnt_25_Rt_age_above_60_bradycardia <- Rc_age_above_60_bradycardia + (100/25)/100
nnt_25_or_age_above_60_bradycardia <- (nnt_25_Rt_age_above_60_bradycardia*(Rc_age_above_60_bradycardia - 1))/(Rc_age_above_60_bradycardia*(nnt_25_Rt_age_above_60_bradycardia - 1))

nnt_10_Rt_age_above_60_bradycardia <- Rc_age_above_60_bradycardia + (100/10)/100
nnt_10_or_age_above_60_bradycardia <- (nnt_10_Rt_age_above_60_bradycardia*(Rc_age_above_60_bradycardia - 1))/(Rc_age_above_60_bradycardia*(nnt_10_Rt_age_above_60_bradycardia - 1))

probs_age_above_60_bradycardia = 
  spread_draws(m.brm_bradycardia_age, b_age_60Yes) |> 
  summarise(any_benefit_age_above_60_bradycardia = 100*mean(b_age_60Yes > log(1)),
            rrr_nnt_50_age_above_60_bradycardia = 100*mean(b_age_60Yes > log(nnt_50_or_age_above_60_bradycardia)),
            rrr_nnt_25_age_above_60_bradycardia = 100*mean(b_age_60Yes > log(nnt_25_or_age_above_60_bradycardia)),
            rrr_nnt_10_age_above_60_bradycardia = 100*mean(b_age_60Yes > log(nnt_10_or_age_above_60_bradycardia)),
            )


## Now dose
## Create the 'dose' dataframe
IVdat_bradycardia_dose <- IVdat_bradycardia[!is.na(IVdat_bradycardia$dex_dose),]
m.brm_bradycardia_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_bradycardia_dose, prior = priors_metareg)

# High dose
Rc_high_dose_bradycardia <- median(IVdat_bradycardia[IVdat_bradycardia$dex_dose=="High" &
                                               !is.na(IVdat_bradycardia$dex_dose),]$control_bradycardia_rate)

nnt_50_Rt_high_dose_bradycardia <- Rc_high_dose_bradycardia + (100/50)/100
nnt_50_or_high_dose_bradycardia <- (nnt_50_Rt_high_dose_bradycardia*(Rc_high_dose_bradycardia - 1))/(Rc_high_dose_bradycardia*(nnt_50_Rt_high_dose_bradycardia - 1))

nnt_25_Rt_high_dose_bradycardia <- Rc_high_dose_bradycardia + (100/25)/100
nnt_25_or_high_dose_bradycardia <- (nnt_25_Rt_high_dose_bradycardia*(Rc_high_dose_bradycardia - 1))/(Rc_high_dose_bradycardia*(nnt_25_Rt_high_dose_bradycardia - 1))

nnt_10_Rt_high_dose_bradycardia <- Rc_high_dose_bradycardia + (100/10)/100
nnt_10_or_high_dose_bradycardia <- (nnt_10_Rt_high_dose_bradycardia*(Rc_high_dose_bradycardia - 1))/(Rc_high_dose_bradycardia*(nnt_10_Rt_high_dose_bradycardia - 1))

probs_high_dose_bradycardia = 
  spread_draws(m.brm_bradycardia_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_high_dose_bradycardia = 100*mean(b_dex_doseHigh > log(1)),
            rrr_nnt_50_high_dose_bradycardia = 100*mean(b_dex_doseHigh > log(nnt_50_or_high_dose_bradycardia)),
            rrr_nnt_25_high_dose_bradycardia = 100*mean(b_dex_doseHigh > log(nnt_25_or_high_dose_bradycardia)),
            rrr_nnt_10_high_dose_bradycardia = 100*mean(b_dex_doseHigh > log(nnt_10_or_high_dose_bradycardia)),
            )

# Low dose
Rc_low_dose_bradycardia <- median(IVdat_bradycardia[IVdat_bradycardia$dex_dose=="Low" &
                                               !is.na(IVdat_bradycardia$dex_dose),]$control_bradycardia_rate)
nnt_50_Rt_low_dose_bradycardia <- Rc_low_dose_bradycardia + (100/50)/100
nnt_50_or_low_dose_bradycardia <- (nnt_50_Rt_low_dose_bradycardia*(Rc_low_dose_bradycardia - 1))/(Rc_low_dose_bradycardia*(nnt_50_Rt_low_dose_bradycardia - 1))

nnt_25_Rt_low_dose_bradycardia <- Rc_low_dose_bradycardia + (100/25)/100
nnt_25_or_low_dose_bradycardia <- (nnt_25_Rt_low_dose_bradycardia*(Rc_low_dose_bradycardia - 1))/(Rc_low_dose_bradycardia*(nnt_25_Rt_low_dose_bradycardia - 1))

nnt_10_Rt_low_dose_bradycardia <- Rc_low_dose_bradycardia + (100/10)/100
nnt_10_or_low_dose_bradycardia <- (nnt_10_Rt_low_dose_bradycardia*(Rc_low_dose_bradycardia - 1))/(Rc_low_dose_bradycardia*(nnt_10_Rt_low_dose_bradycardia - 1))

probs_low_dose_bradycardia = 
  spread_draws(m.brm_bradycardia_dose, b_dex_doseLow) |> 
  summarise(any_benefit_low_dose_bradycardia = 100*mean(b_dex_doseLow > log(1)),
            rrr_nnt_50_low_dose_bradycardia = 100*mean(b_dex_doseLow > log(nnt_50_or_low_dose_bradycardia)),
            rrr_nnt_25_low_dose_bradycardia = 100*mean(b_dex_doseLow > log(nnt_25_or_low_dose_bradycardia)),
            rrr_nnt_10_low_dose_bradycardia = 100*mean(b_dex_doseLow > log(nnt_10_or_low_dose_bradycardia)),
            )

prob_tbl1 <- as.data.frame(rbind("Overall" = c("Overall",  Rc_overall_bradycardia, probs_overall_bradycardia),
     "Age below 60" = c("Age below 60",  Rc_age_below_60_bradycardia, probs_age_below_60_bradycardia),
     "Age above 60" = c("Age above 60",  Rc_age_above_60_bradycardia,probs_age_above_60_bradycardia),
     "High dose" = c("High dose",  Rc_high_dose_bradycardia, probs_high_dose_bradycardia),
     "Low dose" = c("Low dose",  Rc_low_dose_bradycardia, probs_low_dose_bradycardia))) %>%
  mutate_at(vars(2:6), as.numeric) %>%
  rename(`Subgroup` = V1,
          `median_rate` = V2) %>%
  mutate(`median_rate` = `median_rate` * 100) %>%
  mutate_at(vars(2:6), ~sprintf("%.1f", .))

colnames(prob_tbl1) <- c("Subgroup", "Median control group rate (%)", "Probability of any harm (%)", "Probability of NNH = 50 (%)",
                        "Probability of NNH = 25 (%)", "Probability of NNH = 10 (%)")

# First - hypotension
 # Begin with overall
# First, calculate the median delirium rate in control group
Rc_overall_hypotension <- median(IVdat_hypotension$control_hypotension_rate)


nnt_50_Rt_overall_hypotension <- Rc_overall_hypotension + (100/50)/100
nnt_50_or_overall_hypotension <- (nnt_50_Rt_overall_hypotension*(Rc_overall_hypotension - 1))/(Rc_overall_hypotension*(nnt_50_Rt_overall_hypotension - 1))

nnt_25_Rt_overall_hypotension <- Rc_overall_hypotension + (100/25)/100
nnt_25_or_overall_hypotension <- (nnt_25_Rt_overall_hypotension*(Rc_overall_hypotension - 1))/(Rc_overall_hypotension*(nnt_25_Rt_overall_hypotension - 1))

nnt_10_Rt_overall_hypotension <- Rc_overall_hypotension + (100/10)/100
nnt_10_or_overall_hypotension <- (nnt_10_Rt_overall_hypotension*(Rc_overall_hypotension - 1))/(Rc_overall_hypotension*(nnt_10_Rt_overall_hypotension - 1))

probs_overall_hypotension = 
  m.brm_hypotension |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_hypotension = 100*mean(b_Intercept > log(1)),
            rrr_nnt_50_overall_hypotension = 100*mean(b_Intercept > log(nnt_50_or_overall_hypotension)),
            rrr_nnt_25_overall_hypotension = 100*mean(b_Intercept > log(nnt_25_or_overall_hypotension)),
            rrr_nnt_10_overall_hypotension = 100*mean(b_Intercept > log(nnt_10_or_overall_hypotension)),
            )

## Create the 'age' dataframe
IVdat_hypotension_age <- IVdat_hypotension[!is.na(IVdat_hypotension$age_60),]
m.brm_hypotension_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_hypotension_age, prior = priors_metareg)

# First, age below 60
Rc_age_below_60_hypotension <- median(IVdat_hypotension[IVdat_hypotension$age_60=="No" &
                                               !is.na(IVdat_hypotension$age_60),]$control_hypotension_rate)
nnt_50_Rt_age_below_60_hypotension <- Rc_age_below_60_hypotension + (100/50)/100
nnt_50_or_age_below_60_hypotension <- (nnt_50_Rt_age_below_60_hypotension*(Rc_age_below_60_hypotension - 1))/(Rc_age_below_60_hypotension*(nnt_50_Rt_age_below_60_hypotension - 1))

nnt_25_Rt_age_below_60_hypotension <- Rc_age_below_60_hypotension + (100/25)/100
nnt_25_or_age_below_60_hypotension <- (nnt_25_Rt_age_below_60_hypotension*(Rc_age_below_60_hypotension - 1))/(Rc_age_below_60_hypotension*(nnt_25_Rt_age_below_60_hypotension - 1))

nnt_10_Rt_age_below_60_hypotension <- Rc_age_below_60_hypotension + (100/10)/100
nnt_10_or_age_below_60_hypotension <- (nnt_10_Rt_age_below_60_hypotension*(Rc_age_below_60_hypotension - 1))/(Rc_age_below_60_hypotension*(nnt_10_Rt_age_below_60_hypotension - 1))

probs_age_below_60_hypotension = 
  spread_draws(m.brm_hypotension_age, b_age_60No) |> 
  summarise(any_benefit_age_below_60_hypotension = 100*mean(b_age_60No > log(1)),
            rrr_nnt_50_age_below_60_hypotension = 100*mean(b_age_60No > log(nnt_50_or_age_below_60_hypotension)),
            rrr_nnt_25_age_below_60_hypotension = 100*mean(b_age_60No > log(nnt_25_or_age_below_60_hypotension)),
            rrr_nnt_10_age_below_60_hypotension = 100*mean(b_age_60No > log(nnt_10_or_age_below_60_hypotension)),
            )

# Then age above 60
Rc_age_above_60_hypotension <- median(IVdat_hypotension[IVdat_hypotension$age_60=="Yes" &
                                               !is.na(IVdat_hypotension$age_60),]$control_hypotension_rate)
nnt_50_Rt_age_above_60_hypotension <- Rc_age_above_60_hypotension + (100/50)/100
nnt_50_or_age_above_60_hypotension <- (nnt_50_Rt_age_above_60_hypotension*(Rc_age_above_60_hypotension - 1))/(Rc_age_above_60_hypotension*(nnt_50_Rt_age_above_60_hypotension - 1))

nnt_25_Rt_age_above_60_hypotension <- Rc_age_above_60_hypotension + (100/25)/100
nnt_25_or_age_above_60_hypotension <- (nnt_25_Rt_age_above_60_hypotension*(Rc_age_above_60_hypotension - 1))/(Rc_age_above_60_hypotension*(nnt_25_Rt_age_above_60_hypotension - 1))

nnt_10_Rt_age_above_60_hypotension <- Rc_age_above_60_hypotension + (100/10)/100
nnt_10_or_age_above_60_hypotension <- (nnt_10_Rt_age_above_60_hypotension*(Rc_age_above_60_hypotension - 1))/(Rc_age_above_60_hypotension*(nnt_10_Rt_age_above_60_hypotension - 1))

probs_age_above_60_hypotension = 
  spread_draws(m.brm_hypotension_age, b_age_60Yes) |> 
  summarise(any_benefit_age_above_60_hypotension = 100*mean(b_age_60Yes > log(1)),
            rrr_nnt_50_age_above_60_hypotension = 100*mean(b_age_60Yes > log(nnt_50_or_age_above_60_hypotension)),
            rrr_nnt_25_age_above_60_hypotension = 100*mean(b_age_60Yes > log(nnt_25_or_age_above_60_hypotension)),
            rrr_nnt_10_age_above_60_hypotension = 100*mean(b_age_60Yes > log(nnt_10_or_age_above_60_hypotension)),
            )


## Now dose
## Create the 'dose' dataframe
IVdat_hypotension_dose <- IVdat_hypotension[!is.na(IVdat_hypotension$dex_dose),]
m.brm_hypotension_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_hypotension_dose, prior = priors_metareg)

# High dose
Rc_high_dose_hypotension <- median(IVdat_hypotension[IVdat_hypotension$dex_dose=="High" &
                                               !is.na(IVdat_hypotension$dex_dose),]$control_hypotension_rate)

nnt_50_Rt_high_dose_hypotension <- Rc_high_dose_hypotension + (100/50)/100
nnt_50_or_high_dose_hypotension <- (nnt_50_Rt_high_dose_hypotension*(Rc_high_dose_hypotension - 1))/(Rc_high_dose_hypotension*(nnt_50_Rt_high_dose_hypotension - 1))

nnt_25_Rt_high_dose_hypotension <- Rc_high_dose_hypotension + (100/25)/100
nnt_25_or_high_dose_hypotension <- (nnt_25_Rt_high_dose_hypotension*(Rc_high_dose_hypotension - 1))/(Rc_high_dose_hypotension*(nnt_25_Rt_high_dose_hypotension - 1))

nnt_10_Rt_high_dose_hypotension <- Rc_high_dose_hypotension + (100/10)/100
nnt_10_or_high_dose_hypotension <- (nnt_10_Rt_high_dose_hypotension*(Rc_high_dose_hypotension - 1))/(Rc_high_dose_hypotension*(nnt_10_Rt_high_dose_hypotension - 1))

probs_high_dose_hypotension = 
  spread_draws(m.brm_hypotension_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_high_dose_hypotension = 100*mean(b_dex_doseHigh > log(1)),
            rrr_nnt_50_high_dose_hypotension = 100*mean(b_dex_doseHigh > log(nnt_50_or_high_dose_hypotension)),
            rrr_nnt_25_high_dose_hypotension = 100*mean(b_dex_doseHigh > log(nnt_25_or_high_dose_hypotension)),
            rrr_nnt_10_high_dose_hypotension = 100*mean(b_dex_doseHigh > log(nnt_10_or_high_dose_hypotension)),
            )

# Low dose
Rc_low_dose_hypotension <- median(IVdat_hypotension[IVdat_hypotension$dex_dose=="Low" &
                                               !is.na(IVdat_hypotension$dex_dose),]$control_hypotension_rate)
nnt_50_Rt_low_dose_hypotension <- Rc_low_dose_hypotension + (100/50)/100
nnt_50_or_low_dose_hypotension <- (nnt_50_Rt_low_dose_hypotension*(Rc_low_dose_hypotension - 1))/(Rc_low_dose_hypotension*(nnt_50_Rt_low_dose_hypotension - 1))

nnt_25_Rt_low_dose_hypotension <- Rc_low_dose_hypotension + (100/25)/100
nnt_25_or_low_dose_hypotension <- (nnt_25_Rt_low_dose_hypotension*(Rc_low_dose_hypotension - 1))/(Rc_low_dose_hypotension*(nnt_25_Rt_low_dose_hypotension - 1))

nnt_10_Rt_low_dose_hypotension <- Rc_low_dose_hypotension + (100/10)/100
nnt_10_or_low_dose_hypotension <- (nnt_10_Rt_low_dose_hypotension*(Rc_low_dose_hypotension - 1))/(Rc_low_dose_hypotension*(nnt_10_Rt_low_dose_hypotension - 1))

probs_low_dose_hypotension = 
  spread_draws(m.brm_hypotension_dose, b_dex_doseLow) |> 
  summarise(any_benefit_low_dose_hypotension = 100*mean(b_dex_doseLow > log(1)),
            rrr_nnt_50_low_dose_hypotension = 100*mean(b_dex_doseLow > log(nnt_50_or_low_dose_hypotension)),
            rrr_nnt_25_low_dose_hypotension = 100*mean(b_dex_doseLow > log(nnt_25_or_low_dose_hypotension)),
            rrr_nnt_10_low_dose_hypotension = 100*mean(b_dex_doseLow > log(nnt_10_or_low_dose_hypotension)),
            )

prob_tbl2 <- as.data.frame(rbind("Overall" = c("Overall",  Rc_overall_hypotension, probs_overall_hypotension),
     "Age below 60" = c("Age below 60",  Rc_age_below_60_hypotension, probs_age_below_60_hypotension),
     "Age above 60" = c("Age above 60",  Rc_age_above_60_hypotension,probs_age_above_60_hypotension),
     "High dose" = c("High dose",  Rc_high_dose_hypotension, probs_high_dose_hypotension),
     "Low dose" = c("Low dose", Rc_low_dose_hypotension, probs_low_dose_hypotension)
     )) %>%
  mutate_at(vars(2:6), as.numeric) %>%
  rename(`Subgroup` = V1,
         `median_rate` = V2) %>%
  mutate(`median_rate` = `median_rate` * 100) %>%
  mutate_at(vars(2:6), ~sprintf("%.1f", .))

colnames(prob_tbl2) <- c("Subgroup", "Median control group rate (%)", "Probability of any harm (%)", "Probability of NNH = 50 (%)",
                        "Probability of NNH = 25 (%)", "Probability of NNH = 10 (%)")

# combine the two data frames using rbind()
prob_tbl <- rbind(prob_tbl1, prob_tbl2)

# Add a row at the top with "Bradycardia"
prob_tbl$outcome <- rep(c("Bradycardia", "Hypotension"), each = 5)

prob_tbl %>%
  gt(rowname_col = "Subgroup", groupname_col = "outcome") %>%
  tab_style(
    style = list(
      cell_fill(color = "grey"),
      cell_text(weight = "bold")
      ),
    locations = cells_row_groups()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels())

```

## Probability of benefit calculations

### Binary outcomes

For two of our dichotomous secondary outcomes, mortality, and arrhythmia, dexmedetomidine may yield benefit. So, below in @tbl-benefit-probs-secondary-dichot, I've calculated the probability of various NNTs for these outcomes, again divided into subgroups `overall`, `age` and `dose`.

Many of the values for mortality are `NA`. This is because the mortality rate is low (1-2%), so it is impossible to have an absolute risk reduction of \>1-2% (you can't have a negative absolute risk).

```{r}
#| label: tbl-benefit-probs-secondary-dichot
#| tbl-cap: |
#|     Probability of benefit of dexmedetomidine in preventing arrhythmias and mortality, across various subgroups.

# First - arrhythmia
 # Begin with overall
# First, calculate the median delirium rate in control group
Rc_overall_arrhythmia <- median(IVdat_arrhythmia$control_arrhythmia_rate)

nnt_50_Rt_overall_arrhythmia <- Rc_overall_arrhythmia - (100/50)/100
nnt_50_or_overall_arrhythmia <- (nnt_50_Rt_overall_arrhythmia*(Rc_overall_arrhythmia - 1))/(Rc_overall_arrhythmia*(nnt_50_Rt_overall_arrhythmia - 1))

nnt_25_Rt_overall_arrhythmia <- Rc_overall_arrhythmia - (100/25)/100
nnt_25_or_overall_arrhythmia <- (nnt_25_Rt_overall_arrhythmia*(Rc_overall_arrhythmia - 1))/(Rc_overall_arrhythmia*(nnt_25_Rt_overall_arrhythmia - 1))

nnt_10_Rt_overall_arrhythmia <- Rc_overall_arrhythmia - (100/10)/100
nnt_10_or_overall_arrhythmia <- (nnt_10_Rt_overall_arrhythmia*(Rc_overall_arrhythmia - 1))/(Rc_overall_arrhythmia*(nnt_10_Rt_overall_arrhythmia - 1))

probs_overall_arrhythmia = 
  m.brm_arrhythmia |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_arrhythmia = 100*mean(b_Intercept < log(1)),
            rrr_nnt_50_overall_arrhythmia = 100*mean(b_Intercept < log(nnt_50_or_overall_arrhythmia)),
            rrr_nnt_25_overall_arrhythmia = 100*mean(b_Intercept < log(nnt_25_or_overall_arrhythmia)),
            rrr_nnt_10_overall_arrhythmia = 100*mean(b_Intercept < log(nnt_10_or_overall_arrhythmia)),
            )

## Create the 'age' dataframe
IVdat_arrhythmia_age <- IVdat_arrhythmia[!is.na(IVdat_arrhythmia$age_60),]
m.brm_arrhythmia_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_arrhythmia_age, prior = priors_metareg)

# First, age below 60
Rc_age_below_60_arrhythmia <- median(IVdat_arrhythmia[IVdat_arrhythmia$age_60=="No" &
                                               !is.na(IVdat_arrhythmia$age_60),]$control_arrhythmia_rate)
nnt_50_Rt_age_below_60_arrhythmia <- Rc_age_below_60_arrhythmia - (100/50)/100
nnt_50_or_age_below_60_arrhythmia <- (nnt_50_Rt_age_below_60_arrhythmia*(Rc_age_below_60_arrhythmia - 1))/(Rc_age_below_60_arrhythmia*(nnt_50_Rt_age_below_60_arrhythmia - 1))

nnt_25_Rt_age_below_60_arrhythmia <- Rc_age_below_60_arrhythmia - (100/25)/100
nnt_25_or_age_below_60_arrhythmia <- (nnt_25_Rt_age_below_60_arrhythmia*(Rc_age_below_60_arrhythmia - 1))/(Rc_age_below_60_arrhythmia*(nnt_25_Rt_age_below_60_arrhythmia - 1))

nnt_10_Rt_age_below_60_arrhythmia <- Rc_age_below_60_arrhythmia - (100/10)/100
nnt_10_or_age_below_60_arrhythmia <- (nnt_10_Rt_age_below_60_arrhythmia*(Rc_age_below_60_arrhythmia - 1))/(Rc_age_below_60_arrhythmia*(nnt_10_Rt_age_below_60_arrhythmia - 1))

probs_age_below_60_arrhythmia = 
  spread_draws(m.brm_arrhythmia_age, b_age_60No) |> 
  summarise(any_benefit_age_below_60_arrhythmia = 100*mean(b_age_60No < log(1)),
            rrr_nnt_50_age_below_60_arrhythmia = 100*mean(b_age_60No < log(nnt_50_or_age_below_60_arrhythmia)),
            rrr_nnt_25_age_below_60_arrhythmia = 100*mean(b_age_60No < log(nnt_25_or_age_below_60_arrhythmia)),
            rrr_nnt_10_age_below_60_arrhythmia = 100*mean(b_age_60No < log(nnt_10_or_age_below_60_arrhythmia)),
            )

# Then age above 60
Rc_age_above_60_arrhythmia <- median(IVdat_arrhythmia[IVdat_arrhythmia$age_60=="Yes" &
                                               !is.na(IVdat_arrhythmia$age_60),]$control_arrhythmia_rate)
nnt_50_Rt_age_above_60_arrhythmia <- Rc_age_above_60_arrhythmia - (100/50)/100
nnt_50_or_age_above_60_arrhythmia <- (nnt_50_Rt_age_above_60_arrhythmia*(Rc_age_above_60_arrhythmia - 1))/(Rc_age_above_60_arrhythmia*(nnt_50_Rt_age_above_60_arrhythmia - 1))

nnt_25_Rt_age_above_60_arrhythmia <- Rc_age_above_60_arrhythmia - (100/25)/100
nnt_25_or_age_above_60_arrhythmia <- (nnt_25_Rt_age_above_60_arrhythmia*(Rc_age_above_60_arrhythmia - 1))/(Rc_age_above_60_arrhythmia*(nnt_25_Rt_age_above_60_arrhythmia - 1))

nnt_10_Rt_age_above_60_arrhythmia <- Rc_age_above_60_arrhythmia - (100/10)/100
nnt_10_or_age_above_60_arrhythmia <- (nnt_10_Rt_age_above_60_arrhythmia*(Rc_age_above_60_arrhythmia - 1))/(Rc_age_above_60_arrhythmia*(nnt_10_Rt_age_above_60_arrhythmia - 1))

probs_age_above_60_arrhythmia = 
  spread_draws(m.brm_arrhythmia_age, b_age_60Yes) |> 
  summarise(any_benefit_age_above_60_arrhythmia = 100*mean(b_age_60Yes < log(1)),
            rrr_nnt_50_age_above_60_arrhythmia = 100*mean(b_age_60Yes < log(nnt_50_or_age_above_60_arrhythmia)),
            rrr_nnt_25_age_above_60_arrhythmia = 100*mean(b_age_60Yes < log(nnt_25_or_age_above_60_arrhythmia)),
            rrr_nnt_10_age_above_60_arrhythmia = 100*mean(b_age_60Yes < log(nnt_10_or_age_above_60_arrhythmia)),
            )


## Now dose
## Create the 'dose' dataframe
IVdat_arrhythmia_dose <- IVdat_arrhythmia[!is.na(IVdat_arrhythmia$dex_dose),]
m.brm_arrhythmia_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_arrhythmia_dose, prior = priors_metareg)

# High dose
Rc_high_dose_arrhythmia <- median(IVdat_arrhythmia[IVdat_arrhythmia$dex_dose=="High" &
                                               !is.na(IVdat_arrhythmia$dex_dose),]$control_arrhythmia_rate)

nnt_50_Rt_high_dose_arrhythmia <- Rc_high_dose_arrhythmia - (100/50)/100
nnt_50_or_high_dose_arrhythmia <- (nnt_50_Rt_high_dose_arrhythmia*(Rc_high_dose_arrhythmia - 1))/(Rc_high_dose_arrhythmia*(nnt_50_Rt_high_dose_arrhythmia - 1))

nnt_25_Rt_high_dose_arrhythmia <- Rc_high_dose_arrhythmia - (100/25)/100
nnt_25_or_high_dose_arrhythmia <- (nnt_25_Rt_high_dose_arrhythmia*(Rc_high_dose_arrhythmia - 1))/(Rc_high_dose_arrhythmia*(nnt_25_Rt_high_dose_arrhythmia - 1))

nnt_10_Rt_high_dose_arrhythmia <- Rc_high_dose_arrhythmia - (100/10)/100
nnt_10_or_high_dose_arrhythmia <- (nnt_10_Rt_high_dose_arrhythmia*(Rc_high_dose_arrhythmia - 1))/(Rc_high_dose_arrhythmia*(nnt_10_Rt_high_dose_arrhythmia - 1))

probs_high_dose_arrhythmia = 
  spread_draws(m.brm_arrhythmia_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_high_dose_arrhythmia = 100*mean(b_dex_doseHigh < log(1)),
            rrr_nnt_50_high_dose_arrhythmia = 100*mean(b_dex_doseHigh < log(nnt_50_or_high_dose_arrhythmia)),
            rrr_nnt_25_high_dose_arrhythmia = 100*mean(b_dex_doseHigh < log(nnt_25_or_high_dose_arrhythmia)),
            rrr_nnt_10_high_dose_arrhythmia = 100*mean(b_dex_doseHigh < log(nnt_10_or_high_dose_arrhythmia)),
            )

# Low dose
Rc_low_dose_arrhythmia <- median(IVdat_arrhythmia[IVdat_arrhythmia$dex_dose=="Low" &
                                               !is.na(IVdat_arrhythmia$dex_dose),]$control_arrhythmia_rate)
nnt_50_Rt_low_dose_arrhythmia <- Rc_low_dose_arrhythmia - (100/50)/100
nnt_50_or_low_dose_arrhythmia <- (nnt_50_Rt_low_dose_arrhythmia*(Rc_low_dose_arrhythmia - 1))/(Rc_low_dose_arrhythmia*(nnt_50_Rt_low_dose_arrhythmia - 1))

nnt_25_Rt_low_dose_arrhythmia <- Rc_low_dose_arrhythmia - (100/25)/100
nnt_25_or_low_dose_arrhythmia <- (nnt_25_Rt_low_dose_arrhythmia*(Rc_low_dose_arrhythmia - 1))/(Rc_low_dose_arrhythmia*(nnt_25_Rt_low_dose_arrhythmia - 1))

nnt_10_Rt_low_dose_arrhythmia <- Rc_low_dose_arrhythmia - (100/10)/100
nnt_10_or_low_dose_arrhythmia <- (nnt_10_Rt_low_dose_arrhythmia*(Rc_low_dose_arrhythmia - 1))/(Rc_low_dose_arrhythmia*(nnt_10_Rt_low_dose_arrhythmia - 1))

probs_low_dose_arrhythmia = 
  spread_draws(m.brm_arrhythmia_dose, b_dex_doseLow) |> 
  summarise(any_benefit_low_dose_arrhythmia = 100*mean(b_dex_doseLow < log(1)),
            rrr_nnt_50_low_dose_arrhythmia = 100*mean(b_dex_doseLow < log(nnt_50_or_low_dose_arrhythmia)),
            rrr_nnt_25_low_dose_arrhythmia = 100*mean(b_dex_doseLow < log(nnt_25_or_low_dose_arrhythmia)),
            rrr_nnt_10_low_dose_arrhythmia = 100*mean(b_dex_doseLow < log(nnt_10_or_low_dose_arrhythmia)),
            )

prob_tbl1 <- as.data.frame(rbind("Overall" = c("Overall",  Rc_overall_arrhythmia, probs_overall_arrhythmia),
     "Age below 60" = c("Age below 60",  Rc_age_below_60_arrhythmia, probs_age_below_60_arrhythmia),
     "Age above 60" = c("Age above 60",  Rc_age_above_60_arrhythmia,probs_age_above_60_arrhythmia),
     "High dose" = c("High dose",  Rc_high_dose_arrhythmia, probs_high_dose_arrhythmia),
     "Low dose" = c("Low dose",  Rc_low_dose_arrhythmia, probs_low_dose_arrhythmia))) %>%
  mutate_at(vars(2:6), as.numeric) %>%
  rename(`Subgroup` = V1,
          `median_rate` = V2) %>%
  mutate(`median_rate` = `median_rate` * 100) %>%
  mutate_at(vars(2:6), ~sprintf("%.1f", .))

colnames(prob_tbl1) <- c("Subgroup", "Median control group rate (%)", "Probability of any benefit (%)", "Probability of NNT = 50 (%)",
                        "Probability of NNT = 25 (%)", "Probability of NNT = 10 (%)")

# First - mortality
 # Begin with overall
# First, calculate the median delirium rate in control group
Rc_overall_mortality <- median(IVdat_mortality$control_mortality_rate)


nnt_50_Rt_overall_mortality <- Rc_overall_mortality - (100/50)/100
nnt_50_or_overall_mortality <- (nnt_50_Rt_overall_mortality*(Rc_overall_mortality - 1))/(Rc_overall_mortality*(nnt_50_Rt_overall_mortality - 1))

nnt_25_Rt_overall_mortality <- Rc_overall_mortality - (100/25)/100
nnt_25_or_overall_mortality <- (nnt_25_Rt_overall_mortality*(Rc_overall_mortality - 1))/(Rc_overall_mortality*(nnt_25_Rt_overall_mortality - 1))

nnt_10_Rt_overall_mortality <- Rc_overall_mortality - (100/10)/100
nnt_10_or_overall_mortality <- (nnt_10_Rt_overall_mortality*(Rc_overall_mortality - 1))/(Rc_overall_mortality*(nnt_10_Rt_overall_mortality - 1))

probs_overall_mortality = 
  m.brm_mortality |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_mortality = 100*mean(b_Intercept < log(1)),
            rrr_nnt_50_overall_mortality = 100*mean(b_Intercept < log(nnt_50_or_overall_mortality)),
            rrr_nnt_25_overall_mortality = 100*mean(b_Intercept < log(nnt_25_or_overall_mortality)),
            rrr_nnt_10_overall_mortality = 100*mean(b_Intercept < log(nnt_10_or_overall_mortality)))

## Create the 'age' dataframe
IVdat_mortality_age <- IVdat_mortality[!is.na(IVdat_mortality$age_60),]
m.brm_mortality_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_mortality_age, prior = priors_metareg)

# First, age below 60
Rc_age_below_60_mortality <- median(IVdat_mortality[IVdat_mortality$age_60=="No" &
                                               !is.na(IVdat_mortality$age_60),]$control_mortality_rate)
nnt_50_Rt_age_below_60_mortality <- Rc_age_below_60_mortality - (100/50)/100
nnt_50_or_age_below_60_mortality <- (nnt_50_Rt_age_below_60_mortality*(Rc_age_below_60_mortality - 1))/(Rc_age_below_60_mortality*(nnt_50_Rt_age_below_60_mortality - 1))

nnt_25_Rt_age_below_60_mortality <- Rc_age_below_60_mortality - (100/25)/100
nnt_25_or_age_below_60_mortality <- (nnt_25_Rt_age_below_60_mortality*(Rc_age_below_60_mortality - 1))/(Rc_age_below_60_mortality*(nnt_25_Rt_age_below_60_mortality - 1))

nnt_10_Rt_age_below_60_mortality <- Rc_age_below_60_mortality - (100/10)/100
nnt_10_or_age_below_60_mortality <- (nnt_10_Rt_age_below_60_mortality*(Rc_age_below_60_mortality - 1))/(Rc_age_below_60_mortality*(nnt_10_Rt_age_below_60_mortality - 1))

probs_age_below_60_mortality = 
  spread_draws(m.brm_mortality_age, b_age_60No) |> 
  summarise(any_benefit_age_below_60_mortality = 100*mean(b_age_60No < log(1)),
            rrr_nnt_50_age_below_60_mortality = 100*mean(b_age_60No < log(nnt_50_or_age_below_60_mortality)),
            rrr_nnt_25_age_below_60_mortality = 100*mean(b_age_60No < log(nnt_25_or_age_below_60_mortality)),
            rrr_nnt_10_age_below_60_mortality = 100*mean(b_age_60No < log(nnt_10_or_age_below_60_mortality)))

# Then age above 60
Rc_age_above_60_mortality <- median(IVdat_mortality[IVdat_mortality$age_60=="Yes" &
                                               !is.na(IVdat_mortality$age_60),]$control_mortality_rate)
nnt_50_Rt_age_above_60_mortality <- Rc_age_above_60_mortality - (100/50)/100
nnt_50_or_age_above_60_mortality <- (nnt_50_Rt_age_above_60_mortality*(Rc_age_above_60_mortality - 1))/(Rc_age_above_60_mortality*(nnt_50_Rt_age_above_60_mortality - 1))

nnt_25_Rt_age_above_60_mortality <- Rc_age_above_60_mortality - (100/25)/100
nnt_25_or_age_above_60_mortality <- (nnt_25_Rt_age_above_60_mortality*(Rc_age_above_60_mortality - 1))/(Rc_age_above_60_mortality*(nnt_25_Rt_age_above_60_mortality - 1))

nnt_10_Rt_age_above_60_mortality <- Rc_age_above_60_mortality - (100/10)/100
nnt_10_or_age_above_60_mortality <- (nnt_10_Rt_age_above_60_mortality*(Rc_age_above_60_mortality - 1))/(Rc_age_above_60_mortality*(nnt_10_Rt_age_above_60_mortality - 1))

probs_age_above_60_mortality = 
  spread_draws(m.brm_mortality_age, b_age_60Yes) |> 
  summarise(any_benefit_age_above_60_mortality = 100*mean(b_age_60Yes < log(1)),
            rrr_nnt_50_age_above_60_mortality = 100*mean(b_age_60Yes < log(nnt_50_or_age_above_60_mortality)),
            rrr_nnt_25_age_above_60_mortality = 100*mean(b_age_60Yes < log(nnt_25_or_age_above_60_mortality)),
            rrr_nnt_10_age_above_60_mortality = 100*mean(b_age_60Yes < log(nnt_10_or_age_above_60_mortality)))


## Now dose
## Create the 'dose' dataframe
IVdat_mortality_dose <- IVdat_mortality[!is.na(IVdat_mortality$dex_dose),]
m.brm_mortality_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_mortality_dose, prior = priors_metareg)

# High dose
Rc_high_dose_mortality <- median(IVdat_mortality[IVdat_mortality$dex_dose=="High" &
                                               !is.na(IVdat_mortality$dex_dose),]$control_mortality_rate)

nnt_50_Rt_high_dose_mortality <- Rc_high_dose_mortality - (100/50)/100
nnt_50_or_high_dose_mortality <- (nnt_50_Rt_high_dose_mortality*(Rc_high_dose_mortality - 1))/(Rc_high_dose_mortality*(nnt_50_Rt_high_dose_mortality - 1))

nnt_25_Rt_high_dose_mortality <- Rc_high_dose_mortality - (100/25)/100
nnt_25_or_high_dose_mortality <- (nnt_25_Rt_high_dose_mortality*(Rc_high_dose_mortality - 1))/(Rc_high_dose_mortality*(nnt_25_Rt_high_dose_mortality - 1))

nnt_10_Rt_high_dose_mortality <- Rc_high_dose_mortality - (100/10)/100
nnt_10_or_high_dose_mortality <- (nnt_10_Rt_high_dose_mortality*(Rc_high_dose_mortality - 1))/(Rc_high_dose_mortality*(nnt_10_Rt_high_dose_mortality - 1))

probs_high_dose_mortality = 
  spread_draws(m.brm_mortality_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_high_dose_mortality = 100*mean(b_dex_doseHigh < log(1)),
            rrr_nnt_50_high_dose_mortality = 100*mean(b_dex_doseHigh < log(nnt_50_or_high_dose_mortality)),
            rrr_nnt_25_high_dose_mortality = 100*mean(b_dex_doseHigh < log(nnt_25_or_high_dose_mortality)),
            rrr_nnt_10_high_dose_mortality = 100*mean(b_dex_doseHigh < log(nnt_10_or_high_dose_mortality)),
            )

# Low dose
Rc_low_dose_mortality <- median(IVdat_mortality[IVdat_mortality$dex_dose=="Low" &
                                               !is.na(IVdat_mortality$dex_dose),]$control_mortality_rate)
nnt_50_Rt_low_dose_mortality <- Rc_low_dose_mortality - (100/50)/100
nnt_50_or_low_dose_mortality <- (nnt_50_Rt_low_dose_mortality*(Rc_low_dose_mortality - 1))/(Rc_low_dose_mortality*(nnt_50_Rt_low_dose_mortality - 1))

nnt_25_Rt_low_dose_mortality <- Rc_low_dose_mortality - (100/25)/100
nnt_25_or_low_dose_mortality <- (nnt_25_Rt_low_dose_mortality*(Rc_low_dose_mortality - 1))/(Rc_low_dose_mortality*(nnt_25_Rt_low_dose_mortality - 1))

nnt_10_Rt_low_dose_mortality <- Rc_low_dose_mortality - (100/10)/100
nnt_10_or_low_dose_mortality <- (nnt_10_Rt_low_dose_mortality*(Rc_low_dose_mortality - 1))/(Rc_low_dose_mortality*(nnt_10_Rt_low_dose_mortality - 1))

probs_low_dose_mortality = 
  spread_draws(m.brm_mortality_dose, b_dex_doseLow) |> 
  summarise(any_benefit_low_dose_mortality = 100*mean(b_dex_doseLow < log(1)),
            rrr_nnt_50_low_dose_mortality = 100*mean(b_dex_doseLow < log(nnt_50_or_low_dose_mortality)),
            rrr_nnt_25_low_dose_mortality = 100*mean(b_dex_doseLow < log(nnt_25_or_low_dose_mortality)),
            rrr_nnt_10_low_dose_mortality = 100*mean(b_dex_doseLow < log(nnt_10_or_low_dose_mortality)),
            )

prob_tbl2 <- as.data.frame(rbind("Overall" = c("Overall",  Rc_overall_mortality, probs_overall_mortality),
     "Age below 60" = c("Age below 60",  Rc_age_below_60_mortality, probs_age_below_60_mortality),
     "Age above 60" = c("Age above 60",  Rc_age_above_60_mortality,probs_age_above_60_mortality),
     "High dose" = c("High dose",  Rc_high_dose_mortality, probs_high_dose_mortality),
     "Low dose" = c("Low dose", Rc_low_dose_mortality, probs_low_dose_mortality)
     )) %>%
  mutate_at(vars(2:6), as.numeric) %>%
  rename(`Subgroup` = V1,
         `median_rate` = V2) %>%
  mutate(`median_rate` = `median_rate` * 100) %>%
  mutate_at(vars(2:6), ~sprintf("%.1f", .))

colnames(prob_tbl2) <- c("Subgroup", "Median control group rate (%)", "Probability of any benefit (%)", "Probability of NNT = 50 (%)",
                        "Probability of NNT = 25 (%)", "Probability of NNT = 10 (%)")

# combine the two data frames using rbind()
prob_tbl <- rbind(prob_tbl1, prob_tbl2)

# Add a row at the top with "Arrhythmia"
prob_tbl$outcome <- rep(c("Arrhythmia", "Mortality"), each = 5)

prob_tbl %>%
  gt(rowname_col = "Subgroup", groupname_col = "outcome") %>%
  tab_style(
    style = list(
      cell_fill(color = "grey"),
      cell_text(weight = "bold")
      ),
    locations = cells_row_groups()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels())

```

### Continuous outcomes

We also have favourable continuous secondary outcomes. Unlike our binary outcomes, we need to choose specific mean differences that we think would be meaningful. Obviously this would be different for each outcome.

Before we decide on this, let's focus on mean differences on 0.5, 1, and 2. @tbl-benefit-probs-secondary shows the results.

```{r}
#| label: tbl-benefit-probs-secondary-cont
#| tbl-cap: |
#|    Probability of benefit of dexmedetomidine in decreasing delirium duration, time to extubation, hospital stay, and ICU stay, across various subgroups.

# First - delirium duration
 # Begin with overall

probs_overall_del_duration = 
  m.brm_del_duration |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_del_duration = 100*mean(b_Intercept < 0),
            md_0.5_overall_del_duration = 100*mean(b_Intercept < -0.5),
            md_1_overall_del_duration = 100*mean(b_Intercept < -1),
            md_2_overall_del_duration = 100*mean(b_Intercept < -2))

## Create the 'age' dataframe
IVdat_del_duration_age <- IVdat_del_duration[!is.na(IVdat_del_duration$age_60),]
m.brm_del_duration_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_del_duration_age, prior = priors_metareg)

# First, age below 60
probs_age_below_60_del_duration = 
  spread_draws(m.brm_del_duration_age, b_age_60No) |> 
  summarise(any_benefit_overall_del_duration = 100*mean(b_age_60No < 0),
            md_0.5_age_below_60_del_duration = 100*mean(b_age_60No < -0.5),
            md_1_age_below_60_del_duration = 100*mean(b_age_60No < -1),
            md_2_age_below_60_del_duration = 100*mean(b_age_60No < -2))

# Then age above 60
probs_age_above_60_del_duration = 
  spread_draws(m.brm_del_duration_age, b_age_60Yes) |> 
  summarise(any_benefit_overall_del_duration = 100*mean(b_age_60Yes < 0),
            md_0.5_age_above_60_del_duration = 100*mean(b_age_60Yes < -0.5),
            md_1_age_above_60_del_duration = 100*mean(b_age_60Yes < -1),
            md_2_age_above_60_del_duration = 100*mean(b_age_60Yes < -2))

## Now dose
## Create the 'dose' dataframe
IVdat_del_duration_dose <- IVdat_del_duration[!is.na(IVdat_del_duration$dex_dose),]
m.brm_del_duration_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_del_duration_dose, prior = priors_metareg)

# High dose
probs_high_dose_del_duration = 
  spread_draws(m.brm_del_duration_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_dose_high_del_duration = 100*mean(b_dex_doseHigh < 0),
            md_0.5_dose_high_del_duration = 100*mean(b_dex_doseHigh < -0.5),
            md_1_dose_high_del_duration = 100*mean(b_dex_doseHigh < -1),
            md_2_dose_high_del_duration = 100*mean(b_dex_doseHigh < -2))

# Low dose
probs_low_dose_del_duration = 
  spread_draws(m.brm_del_duration_dose, b_dex_doseLow) |> 
  summarise(any_benefit_dose_high_del_duration = 100*mean(b_dex_doseLow < 0),
            md_0.5_dose_high_del_duration = 100*mean(b_dex_doseLow < -0.5),
            md_1_dose_high_del_duration = 100*mean(b_dex_doseLow < -1),
            md_2_dose_high_del_duration = 100*mean(b_dex_doseLow < -2))

prob_tbl1 <- as.data.frame(rbind("Overall" = c("Overall", probs_overall_del_duration),
     "Age below 60" = c("Age below 60",  probs_age_below_60_del_duration),
     "Age above 60" = c("Age above 60", probs_age_above_60_del_duration),
     "High dose" = c("High dose", probs_high_dose_del_duration),
     "Low dose" = c("Low dose", probs_low_dose_del_duration))) %>%
  mutate_at(vars(2:5), as.numeric) %>%
  rename(`Subgroup` = V1) %>%
  mutate_at(vars(2:5), ~sprintf("%.1f", .))

colnames(prob_tbl1) <- c("Subgroup", "Probability any benefit", "Probability of MD >0.5",
                        "Probability of MD >1", "Probability of MD >2")

# Now time to extubation
 # Begin with overall
probs_overall_extub_time = 
  m.brm_extub_time |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_extub_time = 100*mean(b_Intercept < 0),
            md_0.5_overall_extub_time = 100*mean(b_Intercept < -0.5),
            md_1_overall_extub_time = 100*mean(b_Intercept < -1),
            md_2_overall_extub_time = 100*mean(b_Intercept < -2))

## Create the 'age' dataframe
IVdat_extub_time_age <- IVdat_extub_time[!is.na(IVdat_extub_time$age_60),]
m.brm_extub_time_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_extub_time_age, prior = priors_metareg)

# First, age below 60
probs_age_below_60_extub_time = 
  spread_draws(m.brm_extub_time_age, b_age_60No) |> 
  summarise(any_benefit_overall_extub_time = 100*mean(b_age_60No < 0),
            md_0.5_age_below_60_extub_time = 100*mean(b_age_60No < -0.5),
            md_1_age_below_60_extub_time = 100*mean(b_age_60No < -1),
            md_2_age_below_60_extub_time = 100*mean(b_age_60No < -2))

# Then age above 60
probs_age_above_60_extub_time = 
  spread_draws(m.brm_extub_time_age, b_age_60Yes) |> 
  summarise(any_benefit_overall_extub_time = 100*mean(b_age_60Yes < 0),
            md_0.5_age_above_60_extub_time = 100*mean(b_age_60Yes < -0.5),
            md_1_age_above_60_extub_time = 100*mean(b_age_60Yes < -1),
            md_2_age_above_60_extub_time = 100*mean(b_age_60Yes < -2))

## Now dose
## Create the 'dose' dataframe
IVdat_extub_time_dose <- IVdat_extub_time[!is.na(IVdat_extub_time$dex_dose),]
m.brm_extub_time_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_extub_time_dose, prior = priors_metareg)

# High dose
probs_high_dose_extub_time = 
  spread_draws(m.brm_extub_time_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_dose_high_extub_time = 100*mean(b_dex_doseHigh < 0),
            md_0.5_dose_high_extub_time = 100*mean(b_dex_doseHigh < -0.5),
            md_1_dose_high_extub_time = 100*mean(b_dex_doseHigh < -1),
            md_2_dose_high_extub_time = 100*mean(b_dex_doseHigh < -2))

# Low dose
probs_low_dose_extub_time = 
  spread_draws(m.brm_extub_time_dose, b_dex_doseLow) |> 
  summarise(any_benefit_dose_high_extub_time = 100*mean(b_dex_doseLow < 0),
            md_0.5_dose_high_extub_time = 100*mean(b_dex_doseLow < -0.5),
            md_1_dose_high_extub_time = 100*mean(b_dex_doseLow < -1),
            md_2_dose_high_extub_time = 100*mean(b_dex_doseLow < -2))

prob_tbl2 <- as.data.frame(rbind("Overall" = c("Overall", probs_overall_extub_time),
     "Age below 60" = c("Age below 60",  probs_age_below_60_extub_time),
     "Age above 60" = c("Age above 60", probs_age_above_60_extub_time),
     "High dose" = c("High dose", probs_high_dose_extub_time),
     "Low dose" = c("Low dose", probs_low_dose_extub_time))) %>%
  mutate_at(vars(2:5), as.numeric) %>%
  rename(`Subgroup` = V1) %>%
  mutate_at(vars(2:5), ~sprintf("%.1f", .))

colnames(prob_tbl2) <- c("Subgroup", "Probability any benefit", "Probability of MD >0.5",
                        "Probability of MD >1", "Probability of MD >2")

# Now hospital stay
 # Begin with overall
probs_overall_hosp_stay = 
  m.brm_hosp_stay |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_hosp_stay = 100*mean(b_Intercept < 0),
            md_0.5_overall_hosp_stay = 100*mean(b_Intercept < -0.5),
            md_1_overall_hosp_stay = 100*mean(b_Intercept < -1),
            md_2_overall_hosp_stay = 100*mean(b_Intercept < -2))

## Create the 'age' dataframe
IVdat_hosp_stay_age <- IVdat_hosp_stay[!is.na(IVdat_hosp_stay$age_60),]
m.brm_hosp_stay_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_hosp_stay_age, prior = priors_metareg)

# First, age below 60
probs_age_below_60_hosp_stay = 
  spread_draws(m.brm_hosp_stay_age, b_age_60No) |> 
  summarise(any_benefit_overall_hosp_stay = 100*mean(b_age_60No < 0),
            md_0.5_age_below_60_hosp_stay = 100*mean(b_age_60No < -0.5),
            md_1_age_below_60_hosp_stay = 100*mean(b_age_60No < -1),
            md_2_age_below_60_hosp_stay = 100*mean(b_age_60No < -2))

# Then age above 60
probs_age_above_60_hosp_stay = 
  spread_draws(m.brm_hosp_stay_age, b_age_60Yes) |> 
  summarise(any_benefit_overall_hosp_stay = 100*mean(b_age_60Yes < 0),
            md_0.5_age_above_60_hosp_stay = 100*mean(b_age_60Yes < -0.5),
            md_1_age_above_60_hosp_stay = 100*mean(b_age_60Yes < -1),
            md_2_age_above_60_hosp_stay = 100*mean(b_age_60Yes < -2))

## Now dose
## Create the 'dose' dataframe
IVdat_hosp_stay_dose <- IVdat_hosp_stay[!is.na(IVdat_hosp_stay$dex_dose),]
m.brm_hosp_stay_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_hosp_stay_dose, prior = priors_metareg)

# High dose
probs_high_dose_hosp_stay = 
  spread_draws(m.brm_hosp_stay_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_dose_high_hosp_stay = 100*mean(b_dex_doseHigh < 0),
            md_0.5_dose_high_hosp_stay = 100*mean(b_dex_doseHigh < -0.5),
            md_1_dose_high_hosp_stay = 100*mean(b_dex_doseHigh < -1),
            md_2_dose_high_hosp_stay = 100*mean(b_dex_doseHigh < -2))

# Low dose
probs_low_dose_hosp_stay = 
  spread_draws(m.brm_hosp_stay_dose, b_dex_doseLow) |> 
  summarise(any_benefit_dose_high_hosp_stay = 100*mean(b_dex_doseLow < 0),
            md_0.5_dose_high_hosp_stay = 100*mean(b_dex_doseLow < -0.5),
            md_1_dose_high_hosp_stay = 100*mean(b_dex_doseLow < -1),
            md_2_dose_high_hosp_stay = 100*mean(b_dex_doseLow < -2))

prob_tbl3 <- as.data.frame(rbind("Overall" = c("Overall", probs_overall_hosp_stay),
     "Age below 60" = c("Age below 60",  probs_age_below_60_hosp_stay),
     "Age above 60" = c("Age above 60", probs_age_above_60_hosp_stay),
     "High dose" = c("High dose", probs_high_dose_hosp_stay),
     "Low dose" = c("Low dose", probs_low_dose_hosp_stay))) %>%
  mutate_at(vars(2:5), as.numeric) %>%
  rename(`Subgroup` = V1) %>%
  mutate_at(vars(2:5), ~sprintf("%.1f", .))

colnames(prob_tbl3) <- c("Subgroup", "Probability any benefit", "Probability of MD >0.5",
                        "Probability of MD >1", "Probability of MD >2")

# Now ICU stay
 # Begin with overall
probs_overall_icu_stay = 
  m.brm_icu_stay |> 
  tidy_draws() |> 
  summarise(any_benefit_overall_icu_stay = 100*mean(b_Intercept < 0),
            md_0.5_overall_icu_stay = 100*mean(b_Intercept < -0.5),
            md_1_overall_icu_stay = 100*mean(b_Intercept < -1),
            md_2_overall_icu_stay = 100*mean(b_Intercept < -2))

## Create the 'age' dataframe
IVdat_icu_stay_age <- IVdat_icu_stay[!is.na(IVdat_icu_stay$age_60),]
m.brm_icu_stay_age <- update(m.brm, formula. = ~ . - Intercept + age_60, newdata = IVdat_icu_stay_age, prior = priors_metareg)

# First, age below 60
probs_age_below_60_icu_stay = 
  spread_draws(m.brm_icu_stay_age, b_age_60No) |> 
  summarise(any_benefit_overall_icu_stay = 100*mean(b_age_60No < 0),
            md_0.5_age_below_60_icu_stay = 100*mean(b_age_60No < -0.5),
            md_1_age_below_60_icu_stay = 100*mean(b_age_60No < -1),
            md_2_age_below_60_icu_stay = 100*mean(b_age_60No < -2))

# Then age above 60
probs_age_above_60_icu_stay = 
  spread_draws(m.brm_icu_stay_age, b_age_60Yes) |> 
  summarise(any_benefit_overall_icu_stay = 100*mean(b_age_60Yes < 0),
            md_0.5_age_above_60_icu_stay = 100*mean(b_age_60Yes < -0.5),
            md_1_age_above_60_icu_stay = 100*mean(b_age_60Yes < -1),
            md_2_age_above_60_icu_stay = 100*mean(b_age_60Yes < -2))

## Now dose
## Create the 'dose' dataframe
IVdat_icu_stay_dose <- IVdat_icu_stay[!is.na(IVdat_icu_stay$dex_dose),]
m.brm_icu_stay_dose <- update(m.brm, formula. = ~ . - Intercept + dex_dose, newdata = IVdat_icu_stay_dose, prior = priors_metareg)

# High dose
probs_high_dose_icu_stay = 
  spread_draws(m.brm_icu_stay_dose, b_dex_doseHigh) |> 
  summarise(any_benefit_dose_high_icu_stay = 100*mean(b_dex_doseHigh < 0),
            md_0.5_dose_high_icu_stay = 100*mean(b_dex_doseHigh < -0.5),
            md_1_dose_high_icu_stay = 100*mean(b_dex_doseHigh < -1),
            md_2_dose_high_icu_stay = 100*mean(b_dex_doseHigh < -2))

# Low dose
probs_low_dose_icu_stay = 
  spread_draws(m.brm_icu_stay_dose, b_dex_doseLow) |> 
  summarise(any_benefit_dose_high_icu_stay = 100*mean(b_dex_doseLow < 0),
            md_0.5_dose_high_icu_stay = 100*mean(b_dex_doseLow < -0.5),
            md_1_dose_high_icu_stay = 100*mean(b_dex_doseLow < -1),
            md_2_dose_high_icu_stay = 100*mean(b_dex_doseLow < -2))

prob_tbl4 <- as.data.frame(rbind("Overall" = c("Overall", probs_overall_icu_stay),
     "Age below 60" = c("Age below 60",  probs_age_below_60_icu_stay),
     "Age above 60" = c("Age above 60", probs_age_above_60_icu_stay),
     "High dose" = c("High dose", probs_high_dose_icu_stay),
     "Low dose" = c("Low dose", probs_low_dose_icu_stay))) %>%
  mutate_at(vars(2:5), as.numeric) %>%
  rename(`Subgroup` = V1) %>%
  mutate_at(vars(2:5), ~sprintf("%.1f", .))

colnames(prob_tbl4) <- c("Subgroup", "Probability any benefit", "Probability of MD >0.5",
                        "Probability of MD >1", "Probability of MD >2")

# combine the two data frames using rbind()
prob_tbl <- rbind(prob_tbl1, prob_tbl2, prob_tbl3, prob_tbl4)

# Add a row at the top
prob_tbl$outcome <- rep(c("Delirium duration (days)", "Time to extubation (mins)", "Hospital stay (days)", "ICU stay (days)"), each = 5)

prob_tbl %>%
  gt(rowname_col = "Subgroup", groupname_col = "outcome") %>%
  tab_style(
    style = list(
      cell_fill(color = "grey"),
      cell_text(weight = "bold")
      ),
    locations = cells_row_groups()) %>%
  tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels())

```

# Publication bias

We will use two Bayesian methods of publication bias assessment: Bayesian regression testing and Bayesian model averaging. Both of these techniques were only described in the last few years, so we will also use some more well-established frequentist methods further down below.

## Bayesian regression test

Arguably the most popular method of publication bias assessment is through analysis of funnel plot asymmetry, in the form of Egger's regression test (and variants therein). This test involves regressing the effect sizes of studies according to their precision; slopes that are significantly different from 0 suggest publication bias. This can also be applied in a Bayesian framework, and was described recently by [Shi et al.](https://pubmed.ncbi.nlm.nih.gov/32424987/) The interpretation regarding the slope is similar to the frequentist Egger's test. Shi et al. use the latent "true" SEs in the Egger-type regression under the Bayesian framework.

Like many publication bias tests, **this actually assesses funnel plot asymmetry (of which publication bias is only 1 cause).**

There are various options for inputs. We will use multiplicative heterogeneity rather than additive heterogeneity. We will use a half-normal prior with 0.5 scale parameter (Cauchy distributions are not available but the difference is likely trivial). The default priors for the regression intercept and slope are used: $N(0, 10^2)$.

As you can see, the 95%CrI for the slope excludes 0. This suggests the presence of publication bias.

```{r small_study_effects, cache=TRUE, message=FALSE, warning=FALSE, errors=FALSE}
library(altmeta)
df <- pb.bayesian.binary(n00 = dex_no_del, n01 = dex_del, 
                   n10 = control_no_del, n11 = control_del, data = IVdat_del,
  sig.level = 0.05, method = "bay", het = "mul",
  sd.prior = "hn", n.adapt = 1000, n.chains = 3,
  n.burnin = 5000, n.iter = 100000, thin = 10,
  phi = 0.5, coda = FALSE,
  traceplot = FALSE)

dat <- data.frame(estimate = sprintf('%.2f', df[[1]]),
                  confint = paste0(sprintf('%.2f', df$ci.bay[[1]]), ", ", sprintf('%.2f', df$ci.bay[[2]])))

colnames(dat) <- c("Estimate", "95% credible interval")
gt(dat) %>%
    tab_style(
    style = list(
      cell_fill(color = "white"),
      cell_text(weight = "bold")
      ),
    locations = cells_column_labels())
```

## Bayesian model averaging

Bayesian model averaging is based on one central premise: a myriad of arguments can be made for or against using a myriad of possible Bayesian models; rather than choosing one, why not average them all?

The RoBMA packages generates three families of models across 12 models, and averages their performance. For a description see [this link](https://psyarxiv.com/u4cns?ref=hormones-brain-and-behavior). The output is an **inclusion Bayes factor**. This answers the question: Are the observed data more probable under models with a particular effect, than they are under models without that particular effect? Inclusion bayes factors \>3 indicate 'substantial' evidence against the null, while inclusion bayes factors \<1/3 indicate 'substantial' evidence for the null.

We can use Bayesian model averaging for the whole emta-analysis, if we wanted to. But here we're specifically interested in publication bias. The benefit provided by the model averaging approach is that we are provided with an inclusion Bayes factor for publication bias. A high inclusion BF for publication bias would suggest that models accounting for publication bias are more consistent with the observed data. We are also provided with an estimate that accounts for publication bias. 

Publication bias models include selection models and PET-PEESE models. The publication bias adjusment prior described in Bartoš (2021). "Robust Bayesian metaanalysis: Model-averaging across complementary publication bias adjustment methods."

### Summary

```{r pub_bias_model_averaging, fig.height=12, fig.width=9}
library(RoBMA)
library(rjags)

## Model with pub bias models included
robma <- RoBMA(logOR = IVdat_del$yi, v = IVdat_del$vi,
               priors_effect = prior(distribution = "normal", parameters = list(mean = 0, sd = 0.82)),
               priors_heterogeneity = prior(distribution = "normal", parameters = list(mean = 0, sd = 0.5),
                                            truncation = list(lower = 0, upper = Inf)), 
               prior_scale = "logOR",
               effect_direction = "negative")

## Model without pub bias included
robma_no_bias <- RoBMA(logOR = IVdat_del$yi, v = IVdat_del$vi,
               priors_effect = prior(distribution = "normal", parameters = list(mean = 0, sd = 0.82)),
               priors_heterogeneity = prior(distribution = "normal", parameters = list(mean = 0, sd = 0.5),
                                            truncation = list(lower = 0, upper = Inf)),
               prior_scale = "logOR",
               effect_direction = "negative", priors_bias = NULL)
cat("Models including those accounting for publication bias:\n\n")
summary(robma, conditional = TRUE)
cat("Models excluding those accounting for publication bias:\n\n")
summary(robma_no_bias, conditional = TRUE)

```

### Plots

```{r robma_plots, fig.height = 4, fig.width=10}
# Set up the plots with a 1x2 grid
par(mfrow = c(1, 2))

# Plot the first graph with title "A" on the left side
plot(robma, parameter = "mu", prior = TRUE, conditional = TRUE, xlim = c(-1, 1.5))
mtext("A", side = 3, line = 1, adj = 0, cex = 1.5, font = 2)
text(-0.9, 1.25, "Conditional model-averaged \nmean effect including \nmodels accounting \nfor publication bias", 
     pos = 4, cex = 0.75)
arrows(x0 = 0, y0 = 1.25, x1 = 0.2, y1 = 1.25, length = 0.1)
text(-0.8, 0.8, "Prior for mean \neffect (N(0,0.82))", 
     pos = 4, cex = 0.75)
arrows(x0 = -0.5, y0 = 0.75, x1 = -0.5, y1 = 0.5, length = 0.1)

# Plot the second graph with title "B" on the left side
plot(robma_no_bias, parameter = "mu", prior = TRUE, conditional = TRUE, xlim = c(-1, 1.5))
mtext("B", side = 3, line = 1, adj = 0, cex = 1.5, font = 2)
text(0.25, 2, "Conditional model-averaged \nmean effect excluding \nmodels accounting \nfor publication bias", 
     pos = 4, cex = 0.75)
arrows(x0 = 0.2, y0 = 2, x1 = -0.2, y1 = 2, length = 0.1)
text(0, 1, "Prior for mean \neffect (N(0,0.82))", 
     pos = 4, cex = 0.75)
arrows(x0 = 0.25, y0 = 0.75, x1 = 0.25, y1 = 0.2, length = 0.1)

# Reset to the default 1x1 grid
par(mfrow = c(1, 1))

```

## Frequentist methods for publication bias

### Funnel plot

```{r freq_funnel, fig.height=5, fig.width=5}
res_REML <- rma(yi, vi, data=IVdat_del_bayesmeta, 
                        test="knha", method = "REML")
funnel(res_REML, yaxis="sei",
       xlab = "Odds ratio (log scale)", ylab = "Standard error",
       steps=5, level=res_REML$level,
       addtau2=FALSE, type="rstandard",
       back="lightgray", shade="white", hlines="white",
       lty=3,
       label=FALSE, offset=0.4)
```

### Hemni & Copas method

-   Random effects model with fixed effect weights
    -   This is essentially just another way to try and account for publication bias
-   Aims to combat the issue that smaller studies are give relatively larger weight in the RE model with heterogeneity

```{r hemni_copas}
res_REML_hc <- rma(yi, vi, data=IVdat_del, weights = 1/vi,
                        test="knha", method = "REML")
predict(res_REML_hc, transf=exp)
```

### Selection models

-   See the [bookdown selection model page](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html#selection-models)
-   Also see the [metafor selection model page](https://wviechtb.github.io/metafor/reference/selmodel.html)
-   We have used the step function to create the so-called "Three-parameter selection model"
    -   The parameters are mu (mean effect size), tau\^2 (heterogeneity variance), and d2 (likelihood of selection of p-values)
-   We can't really do any more complex models because we only have 21 studies
-   Also see [Dan Quintana's blog](https://www.dsquintana.blog/how-to-perform-a-bayesian-meta-analysis-in-r/)

```{r selection_models}
selmodel(res_REML,
         type = "stepfun",
         steps = c(0.025,1),
         alternative = "less")
```

### Trim and fill

#### Funnel

```{r trim_and_fill_funnel, fig.height=5, fig.width=5}
trimfill(res_REML)
taf <- trimfill(res_REML)
 
### draw funnel plot with missing studies filled in
funnel(taf, legend=TRUE)
```

#### Revised effect size

```{r trim_and_fill_effect}
predict(taf, transf=exp)
```
