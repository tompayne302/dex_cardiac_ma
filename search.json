[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "",
    "text": "Added a section for the average treatment effect (ATE) from DECADE in Table 8 and Figure 13.\nChanged Table 7 to calculate risk differences directly from logistic regression using marginaleffects (rather than converting using the OR). Results are similar.\nChanged Table 1 and all other NNT tables to report absolute risk reductions rather than NNTs. See Frank Harrell’s issues with the NNT. They have convinced me to stop using it (thanks Ben).\nChanged Figure 17 to be a contour-enhanced funnel plot rather than normal funnel plot.\nAll incorrect references to ‘publication bias’ changed to ‘small study effects’. See this link for details.\nPriors specified and justified for the primary analysis and metaregression.\nI had made an error in the fully Bayesian sequential analysis in Table 4. I was mistakenly putting our prior for \\(\\tau\\) on \\(\\tau^2\\) rather than \\(\\tau\\). I have changed this and added a longer explanation of how I created the new model in rjags. It now suggests stopping after 15 studies (including studies at high RoB), which is more congruent with the semi-Bayesian analysis.\nAdded a summary section (at the top) for our Bayesian reanalysis of DECADE.\nAdded plots of our priors in Figure 1 and Figure 2."
  },
  {
    "objectID": "index.html#overall-forest-plot",
    "href": "index.html#overall-forest-plot",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "All studiesExcluding studies at high risk of bias\n\n\n\n\nError in list2(...): object 'merged_df' not found\n\n\nError in list2(...): object 'res_plot' not found\n\n\nError in ggplot(aes(y = relevel(author1, \"Pooled Effect\", after = Inf), : object 'res_plot_overall' not found\n\n\nError in ggplot(aes(y = relevel(author1, \"Pooled Effect\", after = Inf), : object 'res_plot_overall' not found\n\n\nError in ggplot(aes(y = relevel(author1, \"Pooled Effect\", after = Inf)), : object 'res_plot' not found\n\n\nError in eval(expr, envir, enclos): object 'p_studies_overall' not found\n\n\n\n\n\n\n\n\n\nFigure 1: Forest plot excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#posterior-probability-plots",
    "href": "index.html#posterior-probability-plots",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "Now let’s look at the posteriors a bit more closely. We are essentially zooming in on the ‘Pooled Effect’ curves from the above forest plots.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\nFigure 3: Posteriors of A) mean effect and B) heterogeneity across all studies.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Posteriors of A) mean effect and B) heterogeneity when excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#sensitivity-using-other-priors",
    "href": "index.html#sensitivity-using-other-priors",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "Running MCMC with 4 chains, at most 8 in parallel...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.3 seconds.\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.1 seconds.\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.1 seconds.\n\n\n\n\n\n\n\n\n\nStatistical Model\nCredible interval\nPrediction interval\nτ\n\n\nOdds ratio (median [95%CrI])\nOdds ratio (median [95%CrI])\nMedian [95%CrI])\n\n\n\n\nInformative*\n0.64 [0.46, 0.86]\n0.64 [0.25, 1.49]\n0.37 [0.10, 0.70]\n\n\nVague†\n0.62 [0.42, 0.84]\n0.63 [0.22, 1.59]\n0.40 [0.12, 0.75]\n\n\nTurner et al.‡\n0.66 [0.48, 0.85]\n0.67 [0.28, 1.31]\n0.30 [0.00, 0.58]\n\n\n\n* μ prior: normal(0, 0.82); τ prior: cauchy(0, 0.5)\n\n\n† μ prior: normal(0, 4); τ prior: cauchy(0, 4)\n\n\n‡ μ prior: normal(0, 0.82); τ prior: lognormal(-2.49, 1.52)"
  },
  {
    "objectID": "index.html#fully-bayesian-sequential-analysis",
    "href": "index.html#fully-bayesian-sequential-analysis",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "Code from https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.7052?saml_referrer\nIn this, delta (relevant effect size) only matters for the futility calculation\n\nThe ‘benefit’ is when the 95% highest probability density interval excludes 0\nThe ‘futility’ indicates the probability that the intervention can achieve the minimum clinically relevant effect size is small\n\n\n\n\n\n\n$result\n[1] \"Benefit\"\n\n$studies\n[1] 9\n\n$theta\n[1] -0.6594117\n\n$theta.hpd\n    lower     upper \n-1.121320 -0.250375 \n\n$tau.sq\n[1] 0.1601466\n\n$tau.sq.hpd\n       lower        upper \n1.946495e-06 6.159201e-01"
  },
  {
    "objectID": "index.html#prior-plots",
    "href": "index.html#prior-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.1 Prior plots",
    "text": "7.1 Prior plots\nFigure 7 shows the distributions of the priors. The top two panels show the distribution of the (A) reference priors, and (B) meta-analysis priors. The bottom forest plot shows the posterior distribution of DECADE’s results for each prior. The grey region denotes the ROPE. The purple lines show the DECADE’s findings when analysed alone. The grey lines represent the prior. Regions of the posterior that suggest benefit (OR &lt; 1) are shaded in blue and those that suggest harm (OR &gt; 1) are shaded in red. The probabilities of any harm or benefit, and the harm or benefit exceeding the MCID, are shown for each prior. Analyses of all studies and excluding studies at high risk of bias are provided.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\nFigure 7: Plots of the distributions of the reference and meta-analysis priors.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Plots of the distributions of the reference and meta-analysis priors, excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#arr-table",
    "href": "index.html#arr-table",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "8.2 ARR table",
    "text": "8.2 ARR table\nBelow I provide direct probability statements for the DECADE trial’s results given each priors I specified above. I consider absolute risk reductions (ARRs) of up to 8% or more and absolute risk increases (ARIs) of up to 8% or more.\nI have chosen this range of risk changes in order to cover a wide variety of plausible effects (-8% to +8%). This gives clinicians the option to consider whether dexmedetomidine may yield a small benefit/harm (2%) or a very large benefit/harm (8%), based on which prior their personal beliefs align with.\nAll ARRs in Table 7 below use avg_comparisons() function from the marginaleffects package to obtain risk differences as the measure of effect.\n\n\n\n\n\n\nTable 7:  Explanations for priors for DECADE and the corresponding\nprobabilities of changes in absolute risk of delirium. \n  \n    \n    \n      \n      Prior equivalent\n      Rationale for prior\n      \n        Posterior probability that the change in absolute risk from DECADE is above/below a certain threshold\n      \n    \n    \n      ARR &gt;8%\n      ARR &gt;4%\n      ARR &gt;2%\n      ARR &gt;0%\n      ARI &gt;0%\n      ARI &gt;2%\n      ARI &gt;4%\n      ARI &gt;8%\n    \n  \n  \n    \n      Reference priors\n    \n    Vague\nNo information imposed on posterior estimate\nDoes not favour one prior belief over another\n0.0\n0.0\n0.1\n1.7\n98.3\n90.8\n69.5\n13.0\n    Very skeptical\nEquivalent to a hypothetical n = 1000 RCT showing a 31% increase in odds of delirium\nThis effect size is 1.5 times the MCID for harm (logOR of 0.27)\n0.0\n0.0\n0.0\n0.7\n99.4\n90.4\n54.6\n1.3\n    Skeptical\nEquivalent to a hypothetical n = 500 RCT showing a 20% increase in odds of delirium\nThis effect size is the MCID for harm (logOR of 0.18)\n0.0\n0.0\n0.1\n1.8\n98.2\n85.6\n51.5\n2.6\n    Neutral\n95% of the density lies between an odds ratio of 0.5 to 2.0\nPlausible values for the effect are likely, with values closer to the null most likely\n0.0\n0.0\n0.3\n3.5\n96.5\n82.0\n48.9\n2.8\n    Optimistic\nEquivalent to a hypothetical n = 500 RCT showing a 14% decrease in odds of delirium\nThis effect size is the MCID for benefit (logOR of -0.18)\n0.0\n0.0\n0.9\n9.0\n91.0\n63.8\n25.7\n0.5\n    Very optimistic\nEquivalent to a hypothetical n = 1000 RCT showing a 24% decrease in odds of delirium\nThis effect size is the 1.5 times MCID for benefit (logOR of -0.27)\n0.0\n0.2\n4.3\n27.4\n72.6\n28.8\n4.8\n0.0\n    \n      Meta-analysis priors (all studies)\n    \n    MA (100% weight)\nMeta-analysis of n = 3488 participants across 15 trials given its full weight as a prior\nThis is analgous to the result of a standard meta-analysis of all the 15 studies, including the DECADE trial\n0.0\n7.0\n39.8\n83.3\n16.7\n1.2\n0.0\n0.0\n    MA (50% weight)\nMeta-analysis of n = 3488 participants across 15 trials given 50% weight as a prior\nThis downweights the effect of the meta-analysis by 50% to account for percieved issues with the included trials\n0.0\n0.1\n2.6\n15.4\n84.6\n54.3\n20.1\n0.3\n    MA (25% weight)\nMeta-analysis of n = 3488 participants across 15 trials given 25% weight as a prior\nThis downweights the effect of the meta-analysis by 75% to account for percieved issues with the included trials\n0.0\n0.0\n0.4\n3.7\n96.3\n82.6\n53.7\n5.8\n    \n      Meta-analysis priors (excluding high RoB)\n    \n    MA (100% weight)\nMeta-analysis of n = 3128 participants across 10 trials (excluding high RoB) given its full weight as a prior\nThis is analgous to the result of a standard meta-analysis of all the 10 studies, including the DECADE trial\n0.0\n6.2\n44.7\n90.5\n9.5\n0.3\n0.0\n0.0\n    MA (50% weight)\nMeta-analysis of n = 3128 participants across 10 trials (excluding high RoB) given 50% weight as a prior\nThis downweights the effect of the meta-analysis by 50% to account for percieved issues with the included trials\n0.0\n0.2\n3.4\n22.1\n78.0\n42.4\n11.4\n0.2\n    MA (25% weight)\nMeta-analysis of n = 3128 participants across 10 trials (excluding high RoB) given 25% weight as a prior\nThis downweights the effect of the meta-analysis by 75% to account for percieved issues with the included trials\n0.0\n0.0\n0.6\n4.8\n95.2\n80.0\n50.1\n4.4"
  },
  {
    "objectID": "index.html#shrinkage-or-estimation-ecdfs",
    "href": "index.html#shrinkage-or-estimation-ecdfs",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "8.3 Shrinkage OR estimation + ECDFs",
    "text": "8.3 Shrinkage OR estimation + ECDFs\nNow let’s zoom on the relative distributions of the mean effect when we include and exclude the DECADE trial from the meta-analysis. For this we will use empirical cumulative distribution function (ECDF) plots.\nI consider four density functions: meta-analysis including DECADE, meta-analysis excluding DECADE, DECADE alone, and the DECADE shrinkage estimate from the meta-analysis.\nThe x-axis is common to the upper and lower graphs so one is able to read off the probability of any odds ratio for any of the four curves. Figure 11 shows the results. The grey region denotes the ROPE (\\(log(OR) = ± 0.18\\)).\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\nFigure 11: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE.\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE, and excluding studies at high risk of bias."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#semi-bayesian-sequential-analysis",
    "href": "index.html#semi-bayesian-sequential-analysis",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.3 Semi-Bayesian sequential analysis",
    "text": "7.3 Semi-Bayesian sequential analysis\nNow for our semi-Bayesian sequential analysis. This uses the methods of Higgins et al. This method is ‘semi-Bayesian’ because it used restricted Whitehead monitoring boundaries with an (approximate) Bayesian approach to updating heterogeneity, but \\({\\mu}\\) is still estimated using frequentist methods.\nIn the dataframe below, when \\(stopping = 0\\), the benefit is uncertain, but when \\(stopping = 1\\), the conclusion is benefit.\nThis method requires us to specify some inputs: \\({\\mu}_R\\), \\(H\\), and \\(V_{max}\\).\n\\({\\mu}_R\\) is the desired log odds ratio. For this, I have used the MCID of \\(logOR = 0.18\\), which I discussed above in Equation 5.\n\\(±H\\) is the horizontal boundaries of the O’Brien-Fleming stopping boundary. To specify an \\({\\alpha}\\) of 0.05 and \\({\\beta}\\) of 0.1, \\(H\\) is set to 7.461.\n\\(V_{max}\\) is the maximum vertical boundary of the O’Brien-Fleming stopping boundary. To specify an \\({\\alpha}\\) of 0.05 and \\({\\beta}\\) of 0.1, \\(V_{max}\\) is set to 11.079.\nAs can be seen below in Table 5, this method suggests the benefit is uncertain with the current data. The slight discordance with the fully Bayesian model is more stringent criteria for stopping in the Semi-Bayesian model.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\n\nTable 5:  Semi-Bayesian sequential method. \n  \n    \n    \n      y\n      var\n      Z\n      V\n      lower\n      upper\n      tausq\n      stopping\n    \n  \n  \n    NaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n    -1.7912194\n1.64957901\n-1.085865\n0.6062153\nNaN\nNaN\n2.7250232\n0\n    -1.3865991\n0.45259935\n-3.063635\n2.2094596\n-19.812738\n17.0395396\n0.9553009\n0\n    -1.1367357\n0.21889469\n-5.193071\n4.5684068\n-10.013918\n7.7404462\n0.5808954\n0\n    -1.1359832\n0.17601287\n-6.453978\n5.6814027\n-8.323459\n6.0514924\n0.4655212\n0\n    -0.9819443\n0.12230420\n-8.028705\n8.1763343\n-5.938827\n3.9749385\n0.3712988\n0\n    -0.9244125\n0.09224386\n-10.021399\n10.8408299\n-4.660137\n2.8113119\n0.2867822\n0\n    -0.8187784\n0.07361411\n-11.122573\n13.5843516\n-3.798997\n2.1614407\n0.2575227\n0\n    -0.8472971\n0.06286077\n-13.478949\n15.9081724\n-3.397010\n1.7024156\n0.2176412\n0\n    -0.7179026\n0.09378391\n-7.654860\n10.6628102\n-4.605246\n3.1694403\n0.5834056\n0\n    -0.6725615\n0.07185913\n-9.359444\n13.9161163\n-3.575559\n2.2304357\n0.4809831\n0\n    -0.6812025\n0.06231506\n-10.931587\n16.0474845\n-3.211123\n1.8487184\n0.4476738\n0\n    -0.6045404\n0.04972926\n-12.156634\n20.1088860\n-2.607391\n1.3983097\n0.3794249\n0\n    -0.6151152\n0.04456163\n-13.803697\n22.4408302\n-2.422522\n1.1922920\n0.3582217\n0\n    -0.5993631\n0.03796641\n-15.786665\n26.3390689\n-2.129369\n0.9306427\n0.3208831\n0\n    -0.5399903\n0.03134580\n-17.226876\n31.9021963\n-1.796171\n0.7161903\n0.2780266\n0\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6:  Semi-Bayesian sequential method for this meta-analysis, excluding\nstudies at high risk of bias. \n  \n    \n    \n      y\n      var\n      Z\n      V\n      lower\n      upper\n      tausq\n      stopping\n    \n  \n  \n    NaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n    -0.7011444\n0.39925330\n-1.756139\n2.504676\nNaN\nNaN\n0.6666667\n0\n    -0.6339709\n0.22217420\n-2.853485\n4.500973\n-9.660082\n8.392140\n0.5000000\n0\n    -0.5530248\n0.14451299\n-3.826817\n6.919793\n-6.412056\n5.306007\n0.4000000\n0\n    -0.6439467\n0.11344903\n-5.676088\n8.814531\n-5.255367\n3.967473\n0.3333333\n0\n    -0.4246023\n0.12300633\n-3.451874\n8.129663\n-5.523215\n4.674010\n0.5371638\n0\n    -0.4312923\n0.08989502\n-4.797733\n11.124087\n-4.066750\n3.204166\n0.4495936\n0\n    -0.4751186\n0.07545045\n-6.297094\n13.253731\n-3.538347\n2.588110\n0.4170572\n0\n    -0.4178173\n0.05699398\n-7.330902\n17.545714\n-2.711380\n1.875745\n0.3445449\n0\n    -0.4260005\n0.04694213\n-9.075015\n21.302827\n-2.318705\n1.466704\n0.3076497\n0\n    -0.3798029\n0.03691375\n-10.288928\n27.090173\n-1.858106\n1.098500\n0.2603647\n0"
  },
  {
    "objectID": "index.html#explanatory-plots",
    "href": "index.html#explanatory-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.1 Explanatory plots",
    "text": "7.1 Explanatory plots\n\n\n\n\n\nFigure 7: Explanation of the fully Bayesian sequential method described by Spence et al. A) Shows when we stop for effect, B) when we stop for futility."
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.2 Analysis",
    "text": "7.2 Analysis\nSimilar to our model in brms, in rjags we need to specify priors for \\(\\mu\\) and \\(\\tau\\). However, JAGS uses precision rather than variance, where:\n\\[\nPrecision = \\frac{1}{Variance}\n\\]\nThis means our priors look different in rjags and JAGS, but they have the same result. When working with JAGS we are specifying a prior distribution for \\(\\tau^2\\), rather than \\(\\tau\\). It is difficult to specify a prior for \\(\\tau^2\\) that can closely mimic our earlier priors for \\(\\tau\\), and for this reason I have decided to specify a vague prior for \\(\\tau\\) in the sequential analysis model:\n\\[\n\\tau^2 \\sim HalfNormal(0, 10^2)\n\\]\nIn summary, our priors in JAGS are:\n\\[\\hat{\\theta} \\sim Normal(\\theta, \\frac{1}{\\sigma^2})\\] \\[\\theta \\sim Normal(\\mu, \\frac{1}{\\tau^2})\\] \\[\\mu \\sim Normal(0, \\frac{1}{0.82^2})\\] \\[\\tau^2 \\sim HalfNormal(0, \\frac{1}{10^2})\\]\nBelow in Table 4 we present the results of the fully Bayesian sequential analysis. As can be seen, this method suggests stopping for benefit after 15 studies when all studies are included, but the conclusion is uncertain when excluding studies at high risk of bias.\n\n\n\n\n\n\nTable 4:  Fully Bayesian sequential method. \n  \n    \n    \n      Subgroup\n      Conclusion\n      Number of studies to reach conclusion\n      Total number of studies in subgroup\n      Median odds ratio (95%CrI) at stopping\n      τ (95% CrI) at stopping\n    \n  \n  \n    All studies\nBenefit\n15\n16\n0.59 (0.39, 0.83)\n0.49 (0.13, 0.90)\n    Excluding studies at high RoB\nUncertain\n11\n11\n0.72 (0.49, 1.01)\n0.42 (0.02, 0.81)"
  },
  {
    "objectID": "index.html#forest-plot-all-subgroups",
    "href": "index.html#forest-plot-all-subgroups",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "9.1 Forest plot all subgroups",
    "text": "9.1 Forest plot all subgroups\n\n\n\n\n\nFigure 14: Metaregression plots."
  },
  {
    "objectID": "index.html#forest-plot",
    "href": "index.html#forest-plot",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.1 Forest plot",
    "text": "10.1 Forest plot\nFor the ‘Mean (SD)’ columns for the continuous outcomes, I used the unweighted grand mean (means of all the studys’ means and SDs). As such, these numbers as essentially arbitrary (because means from tiny studies are weighted equally to means from massive studies). I just present these numbers to give the reader an idea of the sorts of values that were observed in the included studies.\n\n\n\n\n\nFigure 15: Forest plot of the secondary outcomes."
  },
  {
    "objectID": "index.html#probability-of-harm-calculations",
    "href": "index.html#probability-of-harm-calculations",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.2 Probability of harm calculations",
    "text": "10.2 Probability of harm calculations\nLet’s also look at the probability of dexmedetomidine causing various levels of harm, in the form of bradycardia and hypotension. This is equal and opposite to Table 1. Table 9 below shows the relevant figures.\n\n\n\n\n\n\nTable 9:  Probability of harm of dexmedetomidine in causing bradycardia and\nhypotension, across various subgroups. \n  \n    \n    \n      \n      Median control group rate (%)\n      Probability of any harm (%)\n      Probability of ARI = 2 (%)\n      Probability of ARI = 4 (%)\n      Probability of ARI = 6 (%)\n      Probability of ARI = 8 (%)\n    \n  \n  \n    \n      Bradycardia\n    \n    Overall\n8.3\n77.1\n31.5\n9.8\n3.1\n0.8\n    Age below 60\n7.0\n92.6\n73.8\n47.0\n23.8\n10.4\n    Age above 60\n11.9\n40.0\n15.2\n6.1\n2.0\n0.9\n    High dose\n5.8\n62.8\n33.1\n16.2\n7.8\n3.8\n    Low dose\n11.9\n73.6\n42.4\n18.9\n7.1\n2.6\n    \n      Hypotension\n    \n    Overall\n27.5\n16.4\n9.1\n4.6\n2.6\n1.6\n    Age below 60\n34.5\n10.4\n7.1\n5.1\n3.6\n2.9\n    Age above 60\n27.5\n41.6\n27.4\n16.6\n9.1\n5.1\n    High dose\n29.7\n89.3\n83.2\n76.1\n67.8\n57.0\n    Low dose\n27.5\n1.6\n0.7\n0.2\n0.0\n0.0"
  },
  {
    "objectID": "index.html#probability-of-benefit-calculations-1",
    "href": "index.html#probability-of-benefit-calculations-1",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.3 Probability of benefit calculations",
    "text": "10.3 Probability of benefit calculations\n\n10.3.1 Binary outcomes\nFor two of our dichotomous secondary outcomes, mortality, and arrhythmia, dexmedetomidine may yield benefit. So, below in Table 10, I’ve calculated the probability of various NNTs for these outcomes, again divided into subgroups overall, age and dose.\nMany of the values for mortality are NA. This is because the mortality rate is low (1-2%), so it is impossible to have an absolute risk reduction of &gt;1-2% (you can’t have a negative absolute risk).\n\n\n\n\n\n\nTable 10:  Probability of benefit of dexmedetomidine in preventing arrhythmias\nand mortality, across various subgroups. \n  \n    \n    \n      \n      Median control group rate (%)\n      Probability of any benefit (%)\n      Probability of ARR = 2 (%)\n      Probability of ARR = 4 (%)\n      Probability of ARR = 6 (%)\n      Probability of ARR = 8 (%)\n    \n  \n  \n    \n      Arrhythmia\n    \n    Overall\n45.2\n97.9\n90.5\n67.8\n31.8\n8.3\n    Age below 60\n61.5\n89.0\n73.3\n51.6\n28.8\n12.6\n    Age above 60\n31.7\n96.5\n86.4\n61.7\n29.2\n8.0\n    High dose\n56.3\n97.7\n93.3\n83.9\n67.7\n46.9\n    Low dose\n31.7\n89.0\n65.6\n33.0\n8.9\n1.8\n    \n      Mortality\n    \n    Overall\n1.2\n69.2\nNA\nNA\nNA\nNA\n    Age below 60\n2.0\n77.7\n0.0\nNA\nNA\nNA\n    Age above 60\n1.0\n43.2\nNA\nNA\nNA\nNA\n    High dose\n1.1\n35.3\nNA\nNA\nNA\nNA\n    Low dose\n1.9\n85.0\nNA\nNA\nNA\nNA\n  \n  \n  \n\n\n\n\n\n\n\n10.3.2 Continuous outcomes\nWe also have favourable continuous secondary outcomes. Unlike our binary outcomes, we need to choose specific mean differences that we think would be meaningful. Obviously this would be different for each outcome.\nBefore we decide on this, let’s focus on mean differences on 0.5, 1, and 2. Table 11 shows the results.\n\n\n\n\n\n\nTable 11:  Probability of benefit of dexmedetomidine in decreasing delirium\nduration, time to extubation, hospital stay, and ICU stay, across\nvarious subgroups. \n  \n    \n    \n      \n      Probability any benefit\n      Probability of MD &gt;0.5\n      Probability of MD &gt;1\n      Probability of MD &gt;2\n    \n  \n  \n    \n      Delirium duration (days)\n    \n    Overall\n99.4\n43.8\n0.2\n0.0\n    Age below 60\n99.6\n77.0\n5.8\n0.0\n    Age above 60\n71.9\n8.6\n0.1\n0.0\n    High dose\n81.4\n28.2\n2.7\n0.0\n    Low dose\n98.5\n47.9\n1.1\n0.0\n    \n      Time to extubation (mins)\n    \n    Overall\n88.1\n42.4\n8.9\n0.3\n    Age below 60\n68.9\n30.4\n7.7\n0.0\n    Age above 60\n83.7\n47.3\n13.7\n0.1\n    High dose\n91.4\n64.7\n26.5\n0.9\n    Low dose\n58.7\n17.8\n2.4\n0.0\n    \n      Hospital stay (days)\n    \n    Overall\n63.5\n14.8\n1.2\n0.0\n    Age below 60\n72.0\n30.3\n8.1\n0.0\n    Age above 60\n59.3\n17.0\n2.5\n0.0\n    High dose\n57.6\n17.3\n2.9\n0.0\n    Low dose\n68.2\n27.2\n7.6\n0.0\n    \n      ICU stay (days)\n    \n    Overall\n88.4\n14.7\n0.0\n0.0\n    Age below 60\n86.2\n41.0\n8.9\n0.1\n    Age above 60\n65.8\n11.9\n1.1\n0.0\n    High dose\n52.8\n13.3\n1.9\n0.0\n    Low dose\n87.6\n36.3\n5.6\n0.0"
  },
  {
    "objectID": "index.html#bayesian-regression-test",
    "href": "index.html#bayesian-regression-test",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "11.1 Bayesian regression test",
    "text": "11.1 Bayesian regression test\nArguably the most popular method of sample study effect assessment is through analysis of funnel plot asymmetry, in the form of Egger’s regression test (and variants therein). This test involves regressing the effect sizes of studies according to their precision; slopes that are significantly different from 0 suggest sample sample bias. This can also be applied in a Bayesian framework, and was described recently by Shi et al. The interpretation regarding the slope is similar to the frequentist Egger’s test. Shi et al. use the latent “true” SEs in the Egger-type regression under the Bayesian framework.\nLike many sample bias tests, this actually assesses funnel plot asymmetry (of which publication bias is only 1 cause).\nFor a list of alternative causes of funnel plot asymmetry, see Harrer et al.\nBriefly, they are:\n\nBetween-study effect heterogeneity\nHeterogeneity in types of feasible methods of small vs. large studies\nSmall studies being more poorly run than large studies\nRandom change\n\nThere are various options for inputs. We will use multiplicative heterogeneity rather than additive heterogeneity. We will use a half-normal prior with 0.5 scale parameter (Cauchy distributions are not available but the difference is likely trivial). The default vague priors for the regression intercept and slope are used: \\(N(0, 10^4)\\), as this is the approach use for the model’s validation in Shi et al.\nAs you can see in Table 12, the 95%CrI for the slope excludes 0, whether looking at all studies or excluding studies at high risk of bias. This suggests the presence of publication bias."
  },
  {
    "objectID": "index.html#bayesian-model-averaging",
    "href": "index.html#bayesian-model-averaging",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "11.3 Bayesian model averaging",
    "text": "11.3 Bayesian model averaging\nBayesian model averaging is based on one central premise: a myriad of arguments can be made for or against using a myriad of possible Bayesian models; rather than choosing one, why not average them all?\nAs Bartos et al. put it:\n\n“In practice, researchers seldom have knowledge about the data-generating process nor do they have sufficient information to choose with confidence among the wide variety of proposed methods that aim to adjust for publication bias. Furthermore, this wide range of proposed methods often leads to contradictory conclusions. The combination of uncertainty about the data-generating process and the presence of conflicting conclusions can create a”breeding ground” for confirmation bias: researchers may unintentionally select those methods that support the desired outcome. This freedom to choose can greatly inflate the rate of false positives, which can be a serious problem for conventional meta-analysis methods.”\n\nThe RoBMA packages generates three families of models across 12 models (interfacing with JAGS through rjags), and averages their performance. A total of 32 of these models include publication bias and 4 do not. For a description see this link. The output includes an inclusion Bayes factor. The Bayes factor is the ratio of two marginal likelihoods (i.e., two probabilities of data given certain models). We will produce Bayes factors for the mean effect, heterogeneity, and publication bias.\nThe “inclusion” part of “inclusion Bayes factor” refers to the fact we are comparing the alternative hypothesis relative to the null hypothesis. Hence our notation for the bayes factor is \\(BF_{10}\\) rather than \\(BF_{01}\\), which is another kind of Bayes factor that compares the null hypothesis relative to the alternative hypothesis. So, for the mean effect, our null model is \\(H_0: {\\mu} = 0\\) and alterative model is \\(H_1: {\\mu} ≠ 0\\). Then, the inclusion Bayes factor is:\n\\[\nBF_{10} = \\frac{p(data | H_1)}{p(data | H_0)}\n\\] Bayes factors answers the question: Are the observed data more probable under models with a particular effect, than they are under models without that particular effect? Inclusion Bayes factors &gt;3 indicate ‘substantial’ evidence against the null, while inclusion Bayes factors &lt;1/3 indicate ‘substantial’ evidence for the null.\nWe can use Bayesian model averaging for the whole meta-analysis, if we wanted to. But here we’re specifically interested in publication bias. The benefit provided by the model averaging approach is that we are provided with an inclusion Bayes factor for publication bias. A high inclusion BF for publication bias would suggest that models accounting for publication bias are more consistent with the observed data. We are also provided with an estimate that accounts for publication bias.\nPublication bias models include selection models and PET-PEESE models as methods of publication bias assessment. The publication bias adjusment prior is described in Bartoš (2021).\nTable 13 shows the results.\n\n11.3.1 Summary\n\n\n\n\n\n\nTable 13:  Results of the model-averaging approach to meta-analysis, with\nresults for models including publication bias and excluding publication\nbias. \n  \n    \n    \n      \n      Number of models\n      Prior probability\n      Posterior probability\n      Inclusion Bayes factor\n    \n  \n  \n    \n      Models accounting for publication bias\n    \n    Effect\n18\n0.50\n0.47\n0.90\n    Heterogeneity\n18\n0.50\n0.52\n1.10\n    Publication bias\n32\n0.50\n0.99\n72.67\n    \n      Models not accounting for publication bias\n    \n    Effect\n2\n0.50\n0.95\n19.76\n    Heterogeneity\n2\n0.50\n0.94\n14.49\n    Publication bias\n0\n0.00\n0.00\n0.00\n  \n  \n  \n\n\n\n\n\n\n\n11.3.2 Plots\nNow let’s plot the posterior distributions for the mean effect to visualise the effect of publication bias. In Figure 16 below, (A) shows the distribution of the model-averaged estimate with models including publication bias, and (B) is the average distribution of models that do not include publication bias.\nOur model-averaged estimates in these plots use models that assume the absence of an effect (other options includes models that assume the presence of an effect).\nThe arrows demonstrate the probability of a spike at \\(log(OR) = 0\\) (no effect).\nEvidently, including publication bias models significantly changes the results.\n\n\n\n\n\nFigure 16: Plots of the model-averaged mean effect when considering (A) all models including those accounting for publication bias, and (B) models excluding those accounting for publication bias."
  },
  {
    "objectID": "index.html#frequentist-methods-for-publication-bias",
    "href": "index.html#frequentist-methods-for-publication-bias",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.4 Frequentist methods for publication bias",
    "text": "10.4 Frequentist methods for publication bias\n\n10.4.1 Contour-enhanced funnel plot\nBelow is the funnel plot. As you can see, there is clear asymmetry to the plot, with studies missing from the right hand side (area of no effect or harm).\n\n\n\n\n\n\n\n10.4.2 Henmi & Copas method\nRandom effects meta-analysis is not always better than fixed effect meta-analysis, even when you are certain that your population of trials do not meet the (almost invariably un-meet-able) requirement for a fixed effect meta-analysis, which is that the variability of population effects in your trials should be 0. Once instance where random effects falls down is in the allocation of weight to included trials in the presence of heterogeneity in observed effect sizes.\nFor a random effects inverse variance meta-analysis, each study is weighed according to its variance and the calculated heterogeneity:\n\\[\nw_i = \\frac{1}{v_i + \\tau}\n\\tag{7}\\]\nFor a fixed effect analysis, \\(\\tau\\) is not estimated, and the weight of studies is given by:\n\\[\nw_i = 1/v_i\n\\tag{8}\\]\nIndeed, this is partly why the random effects inverse variance meta-analysis converges to a fixed effect analysis when \\(\\tau\\) is estimated to be 0.\nHowever, in the presence of significant heterogeneity in random effects meta-analysis, Equation 7 results in the relative downweighting of larger studies and upweighting of smaller studies. This means smaller studies have a greater influence on the pooled outcome, which exacerbates any impact of publication bias.\nHenmi and Copas proposed that studies in a random effects meta-analysis be weighted according to fixed effect Equation 8. This means that \\(\\tau\\) is calculated and incorporated into the results but is not used to weight the studies. This approach was later popularised by Doi et al. as the ‘Inverse Variance Heterogeneity Model’.\nTable 14 shows the results.\n\n\n\n\n\n\nTable 14:  Results of the Henmi and Copas method for random effects\nmeta-analysis, where studies are weighed according using fixed effect\nweighting. \n  \n    \n    \n      Odds ratio\n      95% confidence interval\n      95% prediction interval\n    \n  \n  \n    0.74\n0.53, 1.05\n0.31, 1.76\n  \n  \n  \n\n\n\n\n\n\n\n10.4.3 Selection models\nSelection models are another form of publication bias assessment. These revolve around testing the hypothesis that there is a systematic ‘selection’ of low p-values relative to higher p-values. They also provide revised effect sizes that take into account the hypothesized selection model.\nWe have used the step function to create the so-called “three-parameter selection model”. The parameters are \\(\\mu\\) (mean effect size), \\({\\tau}^2\\) (heterogeneity variance), and \\(d^2\\) (likelihood of selection of p-values). We can’t really do any more complex models because we only have 16 studies.\nSee the bookdown selection model page and the metafor selection model page, and Dan Quintana’s blog for more details.\n\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 0.2837 (SE = 0.2216)\ntau (square root of estimated tau^2 value):      0.5326\n\nTest for Heterogeneity:\nLRT(df = 1) = 8.1738, p-val = 0.0043\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub    \n -0.9525  0.4535  -2.1003  0.0357  -1.8413  -0.0636  * \n\nTest for Selection Model Parameters:\nLRT(df = 1) = 2.3021, p-val = 0.1292\n\nSelection Model Results:\n\n                     k  estimate      se    zval    pval   ci.lb    ci.ub    \n0     &lt; p &lt;= 0.025   3    1.0000     ---     ---     ---     ---      ---    \n0.025 &lt; p &lt;= 1      13    6.1976  7.4855  0.6944  0.4875  0.0000  20.8689    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.4.4 Trim and fill\nNow for trim and fill analysis. This is another method of publication bias assessment that has been around a lot longer than the other examples.\nTrim and fill analysis uses the funnel plot to hypothesise the existence of ‘missing’ studies in the plot - i.e., studies that were completed but then put in the ‘file draw’ and not published because they didn’t show a significant result. It also provides a revised effect size accounting for these missing studies.\n\nFunnel\nFirst let’s look at the funnel plot to see how many of these theorised studies there are and where they may sit.\n\n\n\n\n\n\n\nRevised effect size\nNow let’s print the revised effect size from the trim and fill analysis.\n\n\n\n\n\n\n  \n    \n    \n      Odds ratio\n      95% confidence interval\n      95% prediction interval\n    \n  \n  \n    0.79\n0.60, 1.03\n0.33, 1.88"
  },
  {
    "objectID": "index.html#cumulative-meta-analysis-plots",
    "href": "index.html#cumulative-meta-analysis-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.4 Cumulative meta-analysis plots",
    "text": "7.4 Cumulative meta-analysis plots\nThe above analysis give good information but do not present the results succinctly to a reader. Therefore, below I’ve performed cumulative meta-analyses to show the results of the sequential analyses. These plots show the posterior of the pooled effect after adding the next study sequentially. As such, the confidence intervals should shrink with increasing studies towards the ‘true’ population mean. Figure 8 shows the findings.\n\n\n\n\n\nFigure 8: ?(caption)"
  },
  {
    "objectID": "index.html#all-studies-5",
    "href": "index.html#all-studies-5",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.3 All studies",
    "text": "7.3 All studies\n\n\n?(caption)\n\n\n\nError in \"fun(..., .envir = .envir)\": ! Could not evaluate cli `{}` expression: `str_catalog(arrange…`.\nCaused by error in `rep(paste_right(sep, \" \"), item_count - 1)`:\n! invalid 'times' argument"
  },
  {
    "objectID": "index.html#excluding-studies-at-high-risk-of-bias-5",
    "href": "index.html#excluding-studies-at-high-risk-of-bias-5",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.4 Excluding studies at high risk of bias",
    "text": "7.4 Excluding studies at high risk of bias\n\n\n?(caption)\n\n\n\nError in \"fun(..., .envir = .envir)\": ! Could not evaluate cli `{}` expression: `str_catalog(arrange…`.\nCaused by error in `rep(paste_right(sep, \" \"), item_count - 1)`:\n! invalid 'times' argument"
  },
  {
    "objectID": "index.html#all-studies-6",
    "href": "index.html#all-studies-6",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "11.2 All studies",
    "text": "11.2 All studies\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 48\n   Unobserved stochastic nodes: 37\n   Total graph size: 365\n\nInitializing model\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 33\n   Unobserved stochastic nodes: 27\n   Total graph size: 255\n\nInitializing model\n\n\n\n\n\n\nTable 12:  Bayesian regression testing to assess for small study effects. \n  \n    \n    \n      Estimate\n      95% credible interval\n    \n  \n  \n    \n      All studies\n    \n    2.99\n1.74, 4.30\n    \n      Excluding studies at high risk of bias\n    \n    3.36\n1.65, 5.08"
  },
  {
    "objectID": "index.html#excluding-studies-at-high-risk-of-bias-6",
    "href": "index.html#excluding-studies-at-high-risk-of-bias-6",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.7 Excluding studies at high risk of bias",
    "text": "7.7 Excluding studies at high risk of bias\n\n\n\n\n\nFigure 10: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE, and excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#posterior-probabilities-with-different-priors",
    "href": "index.html#posterior-probabilities-with-different-priors",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "8.1 Posterior probabilities with different priors",
    "text": "8.1 Posterior probabilities with different priors\nFigure 9 shows the distributions of the priors. The top two panels show the distribution of the (A) reference priors, and (B) meta-analysis priors. The bottom forest plot shows the posterior distribution of DECADE’s results for each prior. The grey region denotes the ROPE. The purple lines show the DECADE’s findings when analysed alone. The grey lines represent the prior. Regions of the posterior that suggest benefit (OR &lt; 1) are shaded in blue and those that suggest harm (OR &gt; 1) are shaded in red. The probabilities of any harm or benefit, and the harm or benefit exceeding the MCID, are shown for each prior. Analyses of all studies and excluding studies at high risk of bias are provided.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\nFigure 9: Plots of the distributions of the reference and meta-analysis priors.\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Plots of the distributions of the reference and meta-analysis priors, excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#average-treatment-effect-ate-estimation",
    "href": "index.html#average-treatment-effect-ate-estimation",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.4 Average treatment effect (ATE) estimation",
    "text": "7.4 Average treatment effect (ATE) estimation\nTo this point, we have been using odds ratios (from logistic regression) in various fancy ways to generate other helpful statistics (probabilities of harm, benefit, etc.).\nBut odds ratios are not infallible. Firstly, there is the issue of noncollapsibility.\nAs Sander Greenland explains:\n\n“Noncollapsibility is a noncausal phenomenon in which a measurement on a group does not equal a designated average of the same measurement over its constituents, as illustrated by how group odds are not simple averages of individual odds when the odds vary across individuals”\n\nBlogs by Cameron Patrick and Solomun Kurz provide excellent overviews of non-collapsiblity and obtaining non-odds ratio estimates from logistic regression. See also Frank Harrell’s comments for what is the best example of non-collapsbility that I can find.\nBasically, consider an RCT looking at dexmedetomidine vs. placebo for delirium prevention. We can obtain a crude marginal odds ratio from an unadjusted logistic regression model, with delirium as the dependent variable. If we were to adjust the logistic regression for sex, we would obtain conditional odds ratios for men and women, respectively. The issue here is that our overall marginal odds ratio is not the weighted average of our conditional odds ratios. As Harrell shows in an example, you can have conditional odds ratios of 9 for both men and women, but the pooled (overall) marginal odds ratio including both men and women is 5.44 - which is clearly not the weighted average of the two conditional ORs of 9. This is non-collapsibility.\nOn this, Harrell says:\n\n“The marginal OR depends on the distribution of the sex variable in the sample, and does not transport to populations with a different sex ratio than the trial enrollment achieved. It is conditional (adjusted) ORs that generalize to other populations. These calculations illustrate that the sex-conditional OR equals the marginal OR only if the distribution is altered so that the conditioning doesn’t matter (e.g., all the males or all the females are excluded). But what is the exact interpretation of the original marginal OR of 5.44 since it involves hidden conditioning on a 1:1 sex ratio in our example? A definition for this example is that 5.44 is the unconditional OR only when there are equal numbers of males and females because that’s how the sample was constituted. But what is the interpretation when one wants to apply the RCT results to individual patients? It would seem to apply only to those rare situations where the patient is being counseled but for some reason we don’t know the patient’s sex. The marginal estimate needs the physician to conceptualize the clinical population (or at least the sex ratio) from which the patient came since it does not want to take into account the patient’s actual sex. To say this another way, the sample-averaged OR of 5.44 does not apply to males, does not apply to females, and can conceivably only apply to a patient whose sex the physician refuses to know and for which their probability of being female is exactly 0.5. Even then the fact that the treatment OR is the same for both males and females makes the use of the sample averaged value highly questionable.”\n\nFor our purposes, the noncollapsbility of ORs means that our marginal OR from DECADE is not a weighted average of possible hidden subpopulations. Another effect of non-collapsiblity is that the effect estimate will change when adjusting for a variable that is not a confounder, while it does not change for collapsible measures (e.g., relative risk).\nSo, we want conditional ORs from RCTs wherever possible, that condition on important risk factors. Conditioning on any risk factor is better than conditioning on none. Of course, as we don’t have individual participant data from DECADE, we can’t do this.\nAnother issue is that odds ratios are not a true measure of the ‘average treatment effect (ATE)’ (a term used in the causal inference literature). Some have also referred to the ATE as the “difference in probabilities”, “discrete differences,” or “discrete change.” The ATE is a population-level summary of the effect of an intervention. A true measure of the ATE is, for example, the risk difference. This is why I will calculate risk differences; as Cameron Patrick puts it:\n\n“Risk differences (the difference in probability attributable to a particular treatment) and relative risks (the ratio of probabilities due to a particular treatment) avoid both the interpretability and noncollapsibility problems of odds ratios.”\n\nHowever, Harrell does caution us that:\n\n“Absolute risk difference does not fix the problem [of non-collapsibility] in general, as sicker patients will show more absolute treatment benefit.”\n\nSo, let’s get into estimating the ATE, in the form of the risk differences obtained from an ANOVA-type logistic regression model. For this we will use the avg_comparisons() function in the marginaleffects package, which calculates estimates from brms (and frequentist) models.\n\n\n\n\n\n\nTable 8:  Average treatment effect of dexmedetomidine in the DECADE trial \n  \n    \n    \n      \n      Absolute risk change % (DEX - placebo)\n      95% CrI\n    \n  \n  \n    \n      Reference priors\n    \n    Vague\n4.7\n-0.3, 9.6\n    Very sceptical\n4.0\n0.8, 7.2\n    Sceptical\n3.7\n-0.1, 7.8\n    Neutral\n3.5\n-0.7, 7.8\n    Optimistic\n2.3\n-1.7, 6.4\n    Very optimistic\n0.8\n-2.7, 4.2\n    \n      Meta-analysis priors (all studies)\n    \n    MA (100% weight)\n-1.7\n-4.9, 1.4\n    MA (50% weight)\n1.8\n-2.3, 5.9\n    MA (25% weight)\n3.8\n-0.8, 8.4\n    \n      Meta-analysis priors (excluding high RoB)\n    \n    MA (100% weight)\n-1.9\n-4.7, 0.8\n    MA (50% weight)\n1.3\n-2.6, 5.3\n    MA (25% weight)\n3.5\n-1.1, 8.1\n  \n  \n  \n\n\n\n\n\n\n7.4.1 ATE ECDFs\nFinally, let’s look at the risk difference on an ECDF scale for all of our priors. This will be a somewhat busy graph.\nIn Figure 13, the posterior probability of continuous ARRs from the DECADE trial is shown for each prior.\n\n\n\n\n\nFigure 13: Empirical cumulative distribution function (ECDF) plots for the ARR."
  },
  {
    "objectID": "index.html#average-treatment-effect-estimation",
    "href": "index.html#average-treatment-effect-estimation",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "8.4 Average treatment effect estimation",
    "text": "8.4 Average treatment effect estimation\nTo this point, we have been using odds ratios (from logistic regression) in various fancy ways to generate other helpful statistics (probabilities of harm, benefit, etc.).\nBut odds ratios are not infallible. Firstly, there is the issue of noncollapsibility.\nAs Sander Greenland explains:\n\n“Noncollapsibility is a noncausal phenomenon in which a measurement on a group does not equal a designated average of the same measurement over its constituents, as illustrated by how group odds are not simple averages of individual odds when the odds vary across individuals”\n\nBlogs by Cameron Patrick and Solomun Kurz provide excellent overviews of non-collapsiblity and obtaining non-odds ratio estimates from logistic regression. See also Frank Harrell’s comments for what is the best example of non-collapsbility that I can find.\nBasically, consider an RCT looking at dexmedetomidine vs. placebo for delirium prevention. We can obtain a crude marginal odds ratio from an unadjusted logistic regression model, with delirium as the dependent variable. If we were to adjust the logistic regression for sex, we would obtain conditional odds ratios for men and women, respectively. The issue here is that our overall marginal odds ratio is not the weighted average of our conditional odds ratios. As Harrell shows in an example, you can have conditional odds ratios of 9 for both men and women, but the pooled (overall) marginal odds ratio including both men and women is 5.44 - which is clearly not the weighted average of the two conditional ORs of 9. This is non-collapsibility.\nOn this, Harrell says:\n\n“The marginal OR depends on the distribution of the sex variable in the sample, and does not transport to populations with a different sex ratio than the trial enrollment achieved. It is conditional (adjusted) ORs that generalize to other populations. These calculations illustrate that the sex-conditional OR equals the marginal OR only if the distribution is altered so that the conditioning doesn’t matter (e.g., all the males or all the females are excluded). But what is the exact interpretation of the original marginal OR of 5.44 since it involves hidden conditioning on a 1:1 sex ratio in our example? A definition for this example is that 5.44 is the unconditional OR only when there are equal numbers of males and females because that’s how the sample was constituted. But what is the interpretation when one wants to apply the RCT results to individual patients? It would seem to apply only to those rare situations where the patient is being counseled but for some reason we don’t know the patient’s sex. The marginal estimate needs the physician to conceptualize the clinical population (or at least the sex ratio) from which the patient came since it does not want to take into account the patient’s actual sex. To say this another way, the sample-averaged OR of 5.44 does not apply to males, does not apply to females, and can conceivably only apply to a patient whose sex the physician refuses to know and for which their probability of being female is exactly 0.5. Even then the fact that the treatment OR is the same for both males and females makes the use of the sample averaged value highly questionable.”\n\nFor our purposes, the noncollapsbility of ORs means that our marginal OR from DECADE is not a weighted average of possible hidden subpopulations. Another effect of non-collapsiblity is that the effect estimate will change when adjusting for a variable that is not a confounder, while it does not change for collapsible measures (e.g., relative risk).\nSo, we want conditional ORs from RCTs wherever possible, that condition on important risk factors. Conditioning on any risk factor is better than conditioning on none. Of course, as we don’t have individual participant data from DECADE, we can’t do this.\nAnother issue is that odds ratios are not a true measure of the ‘average treatment effect (ATE)’ (a term used in the causal inference literature). Some have also referred to the ATE as the “difference in probabilities”, “discrete differences,” or “discrete change.” The ATE is a population-level summary of the effect of an intervention. A true measure of the ATE is, for example, the risk difference. This is why I will calculate risk differences; as Cameron Patrick puts it:\n\n“Risk differences (the difference in probability attributable to a particular treatment) and relative risks (the ratio of probabilities due to a particular treatment) avoid both the interpretability and noncollapsibility problems of odds ratios.”\n\nHowever, Harrell does caution us that:\n\n“Absolute risk difference does not fix the problem [of non-collapsibility] in general, as sicker patients will show more absolute treatment benefit.”\n\n\n8.4.1 ATE table\nSo, let’s get into estimating the ATE, in the form of the risk differences obtained from an ANOVA-type logistic regression model. For this we will use the avg_comparisons() function in the marginaleffects package, which calculates estimates from brms (and frequentist) models.\n\n\n\n\n\n\nTable 8:  Average treatment effect of dexmedetomidine in the DECADE trial \n  \n    \n    \n      \n      Risk difference % (DEX - placebo)\n      95% CrI\n    \n  \n  \n    \n      Reference priors\n    \n    Vague\n5.2\n0.4, 10.0\n    Very sceptical\n4.2\n0.9, 7.5\n    Sceptical\n4.1\n0.2, 8.0\n    Neutral\n3.9\n-0.3, 8.1\n    Optimistic\n2.7\n-1.2, 6.7\n    Very optimistic\n1.0\n-2.5, 4.5\n    \n      Meta-analysis priors (all studies)\n    \n    MA (100% weight)\n-1.6\n-4.9, 1.5\n    MA (50% weight)\n2.2\n-2.0, 6.4\n    MA (25% weight)\n4.2\n-0.4, 8.9\n    \n      Meta-analysis priors (excluding high RoB)\n    \n    MA (100% weight)\n-1.8\n-4.5, 0.9\n    MA (50% weight)\n1.6\n-2.3, 5.5\n    MA (25% weight)\n4.0\n-0.8, 8.7\n  \n  \n  \n\n\n\n\n\n\n\n8.4.2 ATE ECDFs\nFinally, let’s look at the risk difference on an ECDF scale for all of our priors. This will be a somewhat busy graph.\nIn Figure 13, the posterior probability of continuous risk differences from the DECADE trial is shown for each prior. The posterior density of the draws are also shown, for each respective prior.\n\n\n\n\n\nFigure 13: Empirical cumulative distribution function (ECDF) plots for the ARR."
  },
  {
    "objectID": "index.html#frequentist-methods-for-small-study-effects",
    "href": "index.html#frequentist-methods-for-small-study-effects",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "11.4 Frequentist methods for small study effects",
    "text": "11.4 Frequentist methods for small study effects\n\n11.4.1 Contour-enhanced funnel plot\nBelow in Figure 17 is the contour-enhanced funnel plot. As you can see, there is clear asymmetry to the plot, with small studies missing from the right hand side (area of no effect or harm).\nWe are actually using the meta package for frequentist analysis here (this is the most aesthetically pleasing option for contour-enhanced funnel plots in my opinion), rather than metafor. However the results are the same as the methods I’ve used are identical in both packages.\n\n\n\n\n\nFigure 17: Contour-enhaced funnel plot for the outcome of postoperative delirium.\n\n\n\n\n\n\n11.4.2 Henmi & Copas method\nRandom effects meta-analysis is not always better than fixed effect meta-analysis, even when you are certain that your population of trials do not meet the (almost invariably un-meet-able) requirement for a fixed effect meta-analysis, which is that the variability of population effects in your trials should be 0. Once instance where random effects falls down is in the allocation of weight to included trials in the presence of heterogeneity in observed effect sizes.\nFor a random effects inverse variance meta-analysis, each study is weighed according to its variance and the calculated heterogeneity:\n\\[\nw_i = \\frac{1}{v_i + \\tau}\n\\tag{7}\\]\nFor a fixed effect analysis, \\(\\tau\\) is not estimated, and the weight of studies is given by:\n\\[\nw_i = \\frac{1}{v_i}\n\\tag{8}\\]\nIndeed, this is partly why the random effects inverse variance meta-analysis converges to a fixed effect analysis when \\(\\tau\\) is estimated to be 0.\nHowever, in the presence of significant heterogeneity in random effects meta-analysis, Equation 7 results in the relative downweighting of larger studies and upweighting of smaller studies. This means smaller studies have a greater influence on the pooled outcome, which exacerbates any impact of publication bias.\nHenmi and Copas proposed that studies in a random effects meta-analysis be weighted according to fixed effect Equation 8. This means that \\(\\tau\\) is calculated and incorporated into the results but is not used to weight the studies. This approach was later popularised by Doi et al. as the ‘Inverse Variance Heterogeneity Model’.\nTable 14 shows the results.\n\n\n\n\n\n\nTable 14:  Results of the Henmi and Copas method for random effects\nmeta-analysis, where studies are weighed according using fixed effect\nweighting. \n  \n    \n    \n      Odds ratio\n      95% confidence interval\n      95% prediction interval\n    \n  \n  \n    0.74\n0.53, 1.05\n0.31, 1.76\n  \n  \n  \n\n\n\n\n\n\n\n11.4.3 Selection models\nSelection models are another form of publication bias assessment. These revolve around testing the hypothesis that there is a systematic ‘selection’ of low p-values relative to higher p-values. They also provide revised effect sizes that take into account the hypothesized selection model.\nWe have used the step function to create the so-called “three-parameter selection model”. The parameters are \\(\\mu\\) (mean effect size), \\({\\tau}^2\\) (heterogeneity variance), and \\(d^2\\) (likelihood of selection of p-values). We can’t really do any more complex models because we only have 16 studies.\nSee the bookdown selection model page and the metafor selection model page, and Dan Quintana’s blog for more details.\n\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 0.2837 (SE = 0.2216)\ntau (square root of estimated tau^2 value):      0.5326\n\nTest for Heterogeneity:\nLRT(df = 1) = 8.1738, p-val = 0.0043\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub    \n -0.9525  0.4535  -2.1003  0.0357  -1.8413  -0.0636  * \n\nTest for Selection Model Parameters:\nLRT(df = 1) = 2.3021, p-val = 0.1292\n\nSelection Model Results:\n\n                     k  estimate      se    zval    pval   ci.lb    ci.ub    \n0     &lt; p &lt;= 0.025   3    1.0000     ---     ---     ---     ---      ---    \n0.025 &lt; p &lt;= 1      13    6.1976  7.4855  0.6944  0.4875  0.0000  20.8689    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n11.4.4 Trim and fill\nNow for trim and fill analysis. This is another method of publication bias assessment that has been around a lot longer than the other examples.\nTrim and fill analysis uses the funnel plot to hypothesise the existence of ‘missing’ studies in the plot - i.e., studies that were completed but then put in the ‘file draw’ and not published because they didn’t show a significant result. It also provides a revised effect size accounting for these missing studies.\n\nFunnel\nFirst let’s look at the funnel plot to see how many of these theorised studies there are and where they may sit.\n\n\n\n\n\n\n\nRevised effect size\nNow let’s print the revised effect size from the trim and fill analysis.\n\n\n\n\n\n\n  \n    \n    \n      Odds ratio\n      95% confidence interval\n      95% prediction interval\n    \n  \n  \n    0.79\n0.60, 1.03\n0.33, 1.88"
  }
]