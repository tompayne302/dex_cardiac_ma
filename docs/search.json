[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "",
    "text": "At long last the data analysis is here! Tess, before diving into this I would read Kruschke and Liddell if you’re not across the whole frequentist vs. Bayesian debate. Most of what is below will not make sense in the absence of an understanding of the difference between frequentist and Bayesian approaches to data analysis."
  },
  {
    "objectID": "index.html#overall-forest-plot",
    "href": "index.html#overall-forest-plot",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "All studiesExcluding studies at high risk of bias\n\n\n\n\nError in list2(...): object 'merged_df' not found\n\n\nError in list2(...): object 'res_plot' not found\n\n\nError in ggplot(aes(y = relevel(author1, \"Pooled Effect\", after = Inf), : object 'res_plot_overall' not found\n\n\nError in ggplot(aes(y = relevel(author1, \"Pooled Effect\", after = Inf), : object 'res_plot_overall' not found\n\n\nError in ggplot(aes(y = relevel(author1, \"Pooled Effect\", after = Inf)), : object 'res_plot' not found\n\n\nError in eval(expr, envir, enclos): object 'p_studies_overall' not found\n\n\n\n\n\n\n\n\n\nFigure 1: Forest plot excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#posterior-probability-plots",
    "href": "index.html#posterior-probability-plots",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "Now let’s look at the posteriors a bit more closely. We are essentially zooming in on the ‘Pooled Effect’ curves from the above forest plots.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\nFigure 3: Posteriors of A) mean effect and B) heterogeneity across all studies.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Posteriors of A) mean effect and B) heterogeneity when excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#sensitivity-using-other-priors",
    "href": "index.html#sensitivity-using-other-priors",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "Running MCMC with 4 chains, at most 8 in parallel...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 0.3 seconds.\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.1 seconds.\n\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 1 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 1 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 1 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 1 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 1 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 1 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 1 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 1 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 1 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 1 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 1 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 1 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 1 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 1 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 1 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 1 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 1 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 1 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 1 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 1 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 1 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 1 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 1 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 1 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 1 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 1 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 1 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 1 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 1 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 1 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 1 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 1 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 1 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 1 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 1 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 1 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 1 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 1 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 1 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 1 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 1 finished in 0.2 seconds.\nChain 2 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 2 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 2 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 2 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 2 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 2 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 2 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 2 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 2 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 2 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 2 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 2 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 2 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 2 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 2 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 2 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 2 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 2 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 2 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 2 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 2 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 2 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 2 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 2 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 2 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 2 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 2 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 2 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 2 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 2 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 2 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 2 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 2 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 2 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 2 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 2 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 2 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 2 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 2 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 2 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 2 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 2 finished in 0.2 seconds.\nChain 3 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 3 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 3 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 3 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 3 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 3 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 3 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 3 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 3 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 3 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 3 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 3 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 3 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 3 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 3 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 3 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 3 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 3 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 3 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 3 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 3 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 3 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 3 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 3 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 3 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 3 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 3 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 3 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 3 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 3 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 3 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 3 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 3 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 3 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 3 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 3 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 3 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 3 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 3 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 3 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 3 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 3 finished in 0.2 seconds.\nChain 4 Iteration:    1 / 4000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 4000 [  2%]  (Warmup) \nChain 4 Iteration:  200 / 4000 [  5%]  (Warmup) \nChain 4 Iteration:  300 / 4000 [  7%]  (Warmup) \nChain 4 Iteration:  400 / 4000 [ 10%]  (Warmup) \nChain 4 Iteration:  500 / 4000 [ 12%]  (Warmup) \nChain 4 Iteration:  600 / 4000 [ 15%]  (Warmup) \nChain 4 Iteration:  700 / 4000 [ 17%]  (Warmup) \nChain 4 Iteration:  800 / 4000 [ 20%]  (Warmup) \nChain 4 Iteration:  900 / 4000 [ 22%]  (Warmup) \nChain 4 Iteration: 1000 / 4000 [ 25%]  (Warmup) \nChain 4 Iteration: 1100 / 4000 [ 27%]  (Warmup) \nChain 4 Iteration: 1200 / 4000 [ 30%]  (Warmup) \nChain 4 Iteration: 1300 / 4000 [ 32%]  (Warmup) \nChain 4 Iteration: 1400 / 4000 [ 35%]  (Warmup) \nChain 4 Iteration: 1500 / 4000 [ 37%]  (Warmup) \nChain 4 Iteration: 1600 / 4000 [ 40%]  (Warmup) \nChain 4 Iteration: 1700 / 4000 [ 42%]  (Warmup) \nChain 4 Iteration: 1800 / 4000 [ 45%]  (Warmup) \nChain 4 Iteration: 1900 / 4000 [ 47%]  (Warmup) \nChain 4 Iteration: 2000 / 4000 [ 50%]  (Warmup) \nChain 4 Iteration: 2001 / 4000 [ 50%]  (Sampling) \nChain 4 Iteration: 2100 / 4000 [ 52%]  (Sampling) \nChain 4 Iteration: 2200 / 4000 [ 55%]  (Sampling) \nChain 4 Iteration: 2300 / 4000 [ 57%]  (Sampling) \nChain 4 Iteration: 2400 / 4000 [ 60%]  (Sampling) \nChain 4 Iteration: 2500 / 4000 [ 62%]  (Sampling) \nChain 4 Iteration: 2600 / 4000 [ 65%]  (Sampling) \nChain 4 Iteration: 2700 / 4000 [ 67%]  (Sampling) \nChain 4 Iteration: 2800 / 4000 [ 70%]  (Sampling) \nChain 4 Iteration: 2900 / 4000 [ 72%]  (Sampling) \nChain 4 Iteration: 3000 / 4000 [ 75%]  (Sampling) \nChain 4 Iteration: 3100 / 4000 [ 77%]  (Sampling) \nChain 4 Iteration: 3200 / 4000 [ 80%]  (Sampling) \nChain 4 Iteration: 3300 / 4000 [ 82%]  (Sampling) \nChain 4 Iteration: 3400 / 4000 [ 85%]  (Sampling) \nChain 4 Iteration: 3500 / 4000 [ 87%]  (Sampling) \nChain 4 Iteration: 3600 / 4000 [ 90%]  (Sampling) \nChain 4 Iteration: 3700 / 4000 [ 92%]  (Sampling) \nChain 4 Iteration: 3800 / 4000 [ 95%]  (Sampling) \nChain 4 Iteration: 3900 / 4000 [ 97%]  (Sampling) \nChain 4 Iteration: 4000 / 4000 [100%]  (Sampling) \nChain 4 finished in 0.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.2 seconds.\nTotal execution time: 1.1 seconds.\n\n\n\n\n\n\n\n\n\nStatistical Model\nCredible interval\nPrediction interval\nτ\n\n\nOdds ratio (median [95%CrI])\nOdds ratio (median [95%CrI])\nMedian [95%CrI])\n\n\n\n\nInformative*\n0.64 [0.46, 0.86]\n0.64 [0.25, 1.49]\n0.37 [0.10, 0.70]\n\n\nVague†\n0.62 [0.42, 0.84]\n0.63 [0.22, 1.59]\n0.40 [0.12, 0.75]\n\n\nTurner et al.‡\n0.66 [0.48, 0.85]\n0.67 [0.28, 1.31]\n0.30 [0.00, 0.58]\n\n\n\n* μ prior: normal(0, 0.82); τ prior: cauchy(0, 0.5)\n\n\n† μ prior: normal(0, 4); τ prior: cauchy(0, 4)\n\n\n‡ μ prior: normal(0, 0.82); τ prior: lognormal(-2.49, 1.52)"
  },
  {
    "objectID": "index.html#fully-bayesian-sequential-analysis",
    "href": "index.html#fully-bayesian-sequential-analysis",
    "title": "Dexmedetomidine cardiac surgery meta-analysis",
    "section": "",
    "text": "Code from https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.7052?saml_referrer\nIn this, delta (relevant effect size) only matters for the futility calculation\n\nThe ‘benefit’ is when the 95% highest probability density interval excludes 0\nThe ‘futility’ indicates the probability that the intervention can achieve the minimum clinically relevant effect size is small\n\n\n\n\n\n\n$result\n[1] \"Benefit\"\n\n$studies\n[1] 9\n\n$theta\n[1] -0.6594117\n\n$theta.hpd\n    lower     upper \n-1.121320 -0.250375 \n\n$tau.sq\n[1] 0.1601466\n\n$tau.sq.hpd\n       lower        upper \n1.946495e-06 6.159201e-01"
  },
  {
    "objectID": "index.html#prior-plots",
    "href": "index.html#prior-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.1 Prior plots",
    "text": "7.1 Prior plots\nFigure 7 shows the distributions of the priors. The top two panels show the distribution of the (A) reference priors, and (B) meta-analysis priors. The bottom forest plot shows the posterior distribution of DECADE’s results for each prior. The grey region denotes the ROPE. The purple lines show the DECADE’s findings when analysed alone. The grey lines represent the prior. Regions of the posterior that suggest benefit (OR &lt; 1) are shaded in blue and those that suggest harm (OR &gt; 1) are shaded in red. The probabilities of any harm or benefit, and the harm or benefit exceeding the MCID, are shown for each prior. Analyses of all studies and excluding studies at high risk of bias are provided.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\nFigure 7: Plots of the distributions of the reference and meta-analysis priors.\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Plots of the distributions of the reference and meta-analysis priors, excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#arr-table",
    "href": "index.html#arr-table",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.2 ARR table",
    "text": "7.2 ARR table\nBelow I provide direct probability statements for the DECADE trial’s results given each priors I specified above. I consider absolute risk reductions (ARRs) of up to 8% or more and absolute risk increases (ARIs) of up to 8% or more. All ARRs in Table 6 below use the control group rate for delirium incidence from the DECADE trial (12%). So, an ARR of 8% would be a reduction from 12% in the control group to 4% in the dexmedetomidine group, and an ARI of 8% would be a increase from 12% in the control group to 20% in the dexmedetomidine group.\n\n\n\n\n\n\nTable 6:  Explanations for priors for DECADE and the corresponding\nprobabilities of changes in absolute risk of delirium. \n  \n    \n    \n      \n      Prior equivalent\n      Rationale for prior\n      \n        Posterior probability that the change in absolute risk from DECADE is above/below a certain threshold\n      \n    \n    \n      ARR &gt;8%\n      ARR &gt;4%\n      ARR &gt;2%\n      ARR &gt;0%\n      ARI &gt;0%\n      ARI &gt;2%\n      ARI &gt;4%\n      ARI &gt;8%\n    \n  \n  \n    \n      Reference priors\n    \n    Vague\nNo information imposed on posterior estimate\nDoes not favour one prior belief over another\n0.0\n0.0\n0.4\n3.2\n96.8\n84.2\n58.9\n14.2\n    Very skeptical\nEquivalent to a hypothetical n = 1000 RCT showing a 31% increase in odds of delirium\nThis effect size is 1.5 times the MCID for harm (logOR of 0.27)\n0.0\n0.0\n0.0\n0.6\n99.4\n87.5\n48.8\n1.8\n    Skeptical\nEquivalent to a hypothetical n = 500 RCT showing a 20% increase in odds of delirium\nThis effect size is the MCID for harm (logOR of 0.18)\n0.0\n0.0\n0.1\n2.7\n97.3\n78.2\n43.0\n3.8\n    Neutral\n95% of the density lies between an odds ratio of 0.5 to 2.0\nPlausible values for the effect are likely, with values closer to the null most likely\n0.0\n0.0\n0.4\n5.0\n95.0\n74.2\n38.8\n3.4\n    Optimistic\nEquivalent to a hypothetical n = 500 RCT showing a 14% decrease in odds of delirium\nThis effect size is the MCID for benefit (logOR of -0.18)\n0.0\n0.1\n1.0\n12.8\n87.2\n53.2\n19.4\n0.7\n    Very optimistic\nEquivalent to a hypothetical n = 1000 RCT showing a 24% decrease in odds of delirium\nThis effect size is the 1.5 times MCID for benefit (logOR of -0.27)\n0.0\n0.0\n3.3\n32.0\n68.0\n21.3\n2.7\n0.0\n    \n      Meta-analysis priors (all studies)\n    \n    MA (100% weight)\nMeta-analysis of n = 3488 participants across 15 trials given its full weight as a prior\nThis is analgous to the result of a standard meta-analysis of all the 15 studies, including the DECADE trial\n0.0\n0.7\n31.0\n86.4\n13.6\n0.7\n0.0\n0.0\n    MA (50% weight)\nMeta-analysis of n = 3488 participants across 15 trials given 50% weight as a prior\nThis downweights the effect of the meta-analysis by 50% to account for percieved issues with the included trials\n0.0\n0.0\n2.1\n19.5\n80.5\n42.6\n13.7\n0.3\n    MA (25% weight)\nMeta-analysis of n = 3488 participants across 15 trials given 25% weight as a prior\nThis downweights the effect of the meta-analysis by 75% to account for percieved issues with the included trials\n0.0\n0.0\n0.4\n5.1\n94.9\n75.4\n45.6\n6.2\n    \n      Meta-analysis priors (excluding high RoB)\n    \n    MA (100% weight)\nMeta-analysis of n = 3128 participants across 10 trials (excluding high RoB) given its full weight as a prior\nThis is analgous to the result of a standard meta-analysis of all the 10 studies, including the DECADE trial\n0.0\n0.6\n33.3\n92.0\n8.0\n0.1\n0.0\n0.0\n    MA (50% weight)\nMeta-analysis of n = 3128 participants across 10 trials (excluding high RoB) given 50% weight as a prior\nThis downweights the effect of the meta-analysis by 50% to account for percieved issues with the included trials\n0.0\n0.0\n2.8\n25.6\n74.4\n32.8\n7.8\n0.1\n    MA (25% weight)\nMeta-analysis of n = 3128 participants across 10 trials (excluding high RoB) given 25% weight as a prior\nThis downweights the effect of the meta-analysis by 75% to account for percieved issues with the included trials\n0.0\n0.0\n0.5\n6.8\n93.2\n71.5\n40.0\n4.6"
  },
  {
    "objectID": "index.html#shrinkage-or-estimation-ecdfs",
    "href": "index.html#shrinkage-or-estimation-ecdfs",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.3 Shrinkage OR estimation + ECDFs",
    "text": "7.3 Shrinkage OR estimation + ECDFs\nNow let’s zoom on the relative distributions of the mean effect when we include and exclude the DECADE trial from the meta-analysis. For this we will use empirical cumulative distribution function (ECDF) plots.\nI consider four density functions: meta-analysis including DECADE, meta-analysis excluding DECADE, DECADE alone, and the DECADE shrinkage estimate from the meta-analysis.\nThe x-axis is common to the upper and lower graphs so one is able to read off the probability of any odds ratio for any of the four curves. Figure 9 shows the results. The grey region denotes the ROPE (\\(log(OR) = ± 0.18\\)).\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\nFigure 9: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE.\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE, and excluding studies at high risk of bias."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#semi-bayesian-sequential-analysis",
    "href": "index.html#semi-bayesian-sequential-analysis",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "6.3 Semi-Bayesian sequential analysis",
    "text": "6.3 Semi-Bayesian sequential analysis\nNow for our semi-Bayesian sequential analysis. This uses the methods of Higgins et al. This method is ‘semi-Bayesian’ because it used restricted Whitehead monitoring boundaries with an (approximate) Bayesian approach to updating heterogeneity, but \\({\\mu}\\) is still estimated using frequentist methods.\nIn the dataframe below, when \\(stopping = 0\\), the benefit is uncertain, but when \\(stopping = 1\\), the conclusion is benefit.\nThis method requires us to specify some inputs: \\({\\mu}_R\\), \\(H\\), and \\(V_{max}\\).\n\\({\\mu}_R\\) is the desired log odds ratio. For this, I have used the MCID of \\(logOR = 0.18\\), which I discussed above in Equation 6.\n\\(±H\\) is the horizontal boundaries of the O’Brien-Fleming stopping boundary. To specify an \\({\\alpha}\\) of 0.05 and \\({\\beta}\\) of 0.1, \\(H\\) is set to 7.461.\n\\(V_{max}\\) is the maximum vertical boundary of the O’Brien-Fleming stopping boundary. To specify an \\({\\alpha}\\) of 0.05 and \\({\\beta}\\) of 0.1, \\(V_{max}\\) is set to 11.079.\nAs can be seen below in Table 4, this method suggests the benefit is uncertain with the current data.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\n\nTable 4:  Semi-Bayesian sequential method. \n  \n    \n    \n      y\n      var\n      Z\n      V\n      lower\n      upper\n      tausq\n      stopping\n    \n  \n  \n    NaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n    -1.7912194\n1.64957901\n-1.085865\n0.6062153\nNaN\nNaN\n2.7250232\n0\n    -1.3865991\n0.45259935\n-3.063635\n2.2094596\n-19.812738\n17.0395396\n0.9553009\n0\n    -1.1367357\n0.21889469\n-5.193071\n4.5684068\n-10.013918\n7.7404462\n0.5808954\n0\n    -1.1359832\n0.17601287\n-6.453978\n5.6814027\n-8.323459\n6.0514924\n0.4655212\n0\n    -0.9819443\n0.12230420\n-8.028705\n8.1763343\n-5.938827\n3.9749385\n0.3712988\n0\n    -0.9244125\n0.09224386\n-10.021399\n10.8408299\n-4.660137\n2.8113119\n0.2867822\n0\n    -0.8187784\n0.07361411\n-11.122573\n13.5843516\n-3.798997\n2.1614407\n0.2575227\n0\n    -0.8472971\n0.06286077\n-13.478949\n15.9081724\n-3.397010\n1.7024156\n0.2176412\n0\n    -0.7179026\n0.09378391\n-7.654860\n10.6628102\n-4.605246\n3.1694403\n0.5834056\n0\n    -0.6725615\n0.07185913\n-9.359444\n13.9161163\n-3.575559\n2.2304357\n0.4809831\n0\n    -0.6812025\n0.06231506\n-10.931587\n16.0474845\n-3.211123\n1.8487184\n0.4476738\n0\n    -0.6045404\n0.04972926\n-12.156634\n20.1088860\n-2.607391\n1.3983097\n0.3794249\n0\n    -0.6151152\n0.04456163\n-13.803697\n22.4408302\n-2.422522\n1.1922920\n0.3582217\n0\n    -0.5993631\n0.03796641\n-15.786665\n26.3390689\n-2.129369\n0.9306427\n0.3208831\n0\n    -0.5399903\n0.03134580\n-17.226876\n31.9021963\n-1.796171\n0.7161903\n0.2780266\n0\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 5:  Semi-Bayesian sequential method for this meta-analysis, excluding\nstudies at high risk of bias. \n  \n    \n    \n      y\n      var\n      Z\n      V\n      lower\n      upper\n      tausq\n      stopping\n    \n  \n  \n    NaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n    -0.7011444\n0.39925330\n-1.756139\n2.504676\nNaN\nNaN\n0.6666667\n0\n    -0.6339709\n0.22217420\n-2.853485\n4.500973\n-9.660082\n8.392140\n0.5000000\n0\n    -0.5530248\n0.14451299\n-3.826817\n6.919793\n-6.412056\n5.306007\n0.4000000\n0\n    -0.6439467\n0.11344903\n-5.676088\n8.814531\n-5.255367\n3.967473\n0.3333333\n0\n    -0.4246023\n0.12300633\n-3.451874\n8.129663\n-5.523215\n4.674010\n0.5371638\n0\n    -0.4312923\n0.08989502\n-4.797733\n11.124087\n-4.066750\n3.204166\n0.4495936\n0\n    -0.4751186\n0.07545045\n-6.297094\n13.253731\n-3.538347\n2.588110\n0.4170572\n0\n    -0.4178173\n0.05699398\n-7.330902\n17.545714\n-2.711380\n1.875745\n0.3445449\n0\n    -0.4260005\n0.04694213\n-9.075015\n21.302827\n-2.318705\n1.466704\n0.3076497\n0\n    -0.3798029\n0.03691375\n-10.288928\n27.090173\n-1.858106\n1.098500\n0.2603647\n0"
  },
  {
    "objectID": "index.html#explanatory-plots",
    "href": "index.html#explanatory-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "6.1 Explanatory plots",
    "text": "6.1 Explanatory plots\n\n\n\n\n\nFigure 5: Explanation of the fully Bayesian sequential method described by Spence et al. A) Shows when we stop for effect, B) when we stop for futility."
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "6.2 Analysis",
    "text": "6.2 Analysis\n\n\n\n\n\n\n  \n    \n    \n      Subgroup\n      Conclusion\n      Number of studies to reach conclusion\n      Total number of studies in subgroup\n      Median odds ratio (95%CrI) at stopping\n      τ (95% CrI) at stopping\n    \n  \n  \n    All studies\nBenefit\n7\n16\n0.45 (0.26, 0.75)\n0.41 (0.00, 0.84)\n    Excluding studies at high RoB\nUncertain\n11\n11\n0.72 (0.51, 1.01)\n0.39 (0.00, 0.68)"
  },
  {
    "objectID": "index.html#forest-plot-all-subgroups",
    "href": "index.html#forest-plot-all-subgroups",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "8.1 Forest plot all subgroups",
    "text": "8.1 Forest plot all subgroups\n\n\n\n\n\nFigure 11: Metaregression plots."
  },
  {
    "objectID": "index.html#forest-plot",
    "href": "index.html#forest-plot",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "9.1 Forest plot",
    "text": "9.1 Forest plot\nFor the ‘Mean (SD)’ columns for the continuous outcomes, I used the unweighted grand mean (means of all the studys’ means and SDs). As such, these numbers as essentially arbitrary (because means from tiny studies are weighted equally to means from massive studies). I just present these numbers to give the reader an idea of the sorts of values that were observed in the included studies.\n\n\n\n\n\nFigure 12: Forest plot of the secondary outcomes."
  },
  {
    "objectID": "index.html#probability-of-harm-calculations",
    "href": "index.html#probability-of-harm-calculations",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "9.2 Probability of harm calculations",
    "text": "9.2 Probability of harm calculations\nLet’s also look at the probability of dexmedetomidine causing various levels of harm, in the form of bradycardia and hypotension. This is equal and opposite to Table 1. Table 7 below shows the relevant figures.\n\n\n\n\n\n\nTable 7:  Probability of harm of dexmedetomidine in causing bradycardia and\nhypotension, across various subgroups. \n  \n    \n    \n      \n      Median control group rate (%)\n      Probability of any harm (%)\n      Probability of NNH = 50 (%)\n      Probability of NNH = 25 (%)\n      Probability of NNH = 10 (%)\n    \n  \n  \n    \n      Bradycardia\n    \n    Overall\n8.3\n77.4\n30.9\n8.6\n0.0\n    Age below 60\n7.0\n93.8\n74.6\n46.9\n5.0\n    Age above 60\n11.9\n40.8\n15.5\n6.3\n1.1\n    High dose\n5.8\n63.4\n33.3\n16.1\n1.6\n    Low dose\n11.9\n72.4\n41.4\n19.3\n1.5\n    \n      Hypotension\n    \n    Overall\n27.5\n16.4\n8.6\n4.4\n0.6\n    Age below 60\n34.5\n9.6\n6.7\n4.9\n1.5\n    Age above 60\n27.5\n43.8\n28.9\n17.6\n3.8\n    High dose\n29.7\n89.8\n84.0\n76.1\n45.8\n    Low dose\n27.5\n2.1\n1.1\n0.6\n0.1"
  },
  {
    "objectID": "index.html#probability-of-benefit-calculations-1",
    "href": "index.html#probability-of-benefit-calculations-1",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "9.3 Probability of benefit calculations",
    "text": "9.3 Probability of benefit calculations\n\n9.3.1 Binary outcomes\nFor two of our dichotomous secondary outcomes, mortality, and arrhythmia, dexmedetomidine may yield benefit. So, below in Table 8, I’ve calculated the probability of various NNTs for these outcomes, again divided into subgroups overall, age and dose.\nMany of the values for mortality are NA. This is because the mortality rate is low (1-2%), so it is impossible to have an absolute risk reduction of &gt;1-2% (you can’t have a negative absolute risk).\n\n\n\n\n\n\nTable 8:  Probability of benefit of dexmedetomidine in preventing arrhythmias\nand mortality, across various subgroups. \n  \n    \n    \n      \n      Median control group rate (%)\n      Probability of any benefit (%)\n      Probability of NNT = 50 (%)\n      Probability of NNT = 25 (%)\n      Probability of NNT = 10 (%)\n    \n  \n  \n    \n      Arrhythmia\n    \n    Overall\n45.2\n98.5\n91.7\n68.1\n0.8\n    Age below 60\n61.5\n88.4\n72.8\n51.2\n4.1\n    Age above 60\n31.7\n96.4\n85.4\n61.6\n1.3\n    High dose\n56.3\n97.5\n93.0\n83.7\n27.6\n    Low dose\n31.7\n89.1\n67.0\n33.8\n0.3\n    \n      Mortality\n    \n    Overall\n1.2\n68.5\nNA\nNA\nNA\n    Age below 60\n2.0\n77.8\n0.0\nNA\nNA\n    Age above 60\n1.0\n43.4\nNA\nNA\nNA\n    High dose\n1.1\n35.3\nNA\nNA\nNA\n    Low dose\n1.9\n83.9\nNA\nNA\nNA\n  \n  \n  \n\n\n\n\n\n\n\n9.3.2 Continuous outcomes\nWe also have favourable continuous secondary outcomes. Unlike our binary outcomes, we need to choose specific mean differences that we think would be meaningful. Obviously this would be different for each outcome.\nBefore we decide on this, let’s focus on mean differences on 0.5, 1, and 2. Table 9 shows the results.\n\n\n\n\n\n\nTable 9:  Probability of benefit of dexmedetomidine in decreasing delirium\nduration, time to extubation, hospital stay, and ICU stay, across\nvarious subgroups. \n  \n    \n    \n      \n      Probability any benefit\n      Probability of MD &gt;0.5\n      Probability of MD &gt;1\n      Probability of MD &gt;2\n    \n  \n  \n    \n      Delirium duration (days)\n    \n    Overall\n98.9\n42.6\n0.3\n0.0\n    Age below 60\n99.4\n77.5\n5.5\n0.0\n    Age above 60\n71.9\n8.7\n0.1\n0.0\n    High dose\n81.1\n30.1\n2.6\n0.0\n    Low dose\n98.7\n48.5\n1.2\n0.0\n    \n      Time to extubation (mins)\n    \n    Overall\n88.0\n42.4\n8.2\n0.0\n    Age below 60\n70.1\n31.0\n8.6\n0.1\n    Age above 60\n84.0\n47.4\n14.0\n0.2\n    High dose\n91.6\n65.0\n27.1\n1.0\n    Low dose\n57.0\n18.3\n3.5\n0.0\n    \n      Hospital stay (days)\n    \n    Overall\n65.9\n20.5\n5.6\n0.0\n    Age below 60\n70.2\n31.4\n8.9\n0.1\n    Age above 60\n59.9\n16.7\n2.6\n0.0\n    High dose\n62.2\n23.0\n7.8\n0.1\n    Low dose\n69.8\n29.1\n7.0\n0.0\n    \n      ICU stay (days)\n    \n    Overall\n86.0\n20.2\n1.7\n0.0\n    Age below 60\n85.7\n38.1\n7.9\n0.0\n    Age above 60\n64.5\n10.3\n0.8\n0.0\n    High dose\n54.1\n14.8\n2.2\n0.0\n    Low dose\n87.7\n36.1\n5.6\n0.0"
  },
  {
    "objectID": "index.html#bayesian-regression-test",
    "href": "index.html#bayesian-regression-test",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.1 Bayesian regression test",
    "text": "10.1 Bayesian regression test\nArguably the most popular method of publication bias assessment is through analysis of funnel plot asymmetry, in the form of Egger’s regression test (and variants therein). This test involves regressing the effect sizes of studies according to their precision; slopes that are significantly different from 0 suggest publication bias. This can also be applied in a Bayesian framework, and was described recently by Shi et al. The interpretation regarding the slope is similar to the frequentist Egger’s test. Shi et al. use the latent “true” SEs in the Egger-type regression under the Bayesian framework.\nLike many publication bias tests, this actually assesses funnel plot asymmetry (of which publication bias is only 1 cause).\nThere are various options for inputs. We will use multiplicative heterogeneity rather than additive heterogeneity. We will use a half-normal prior with 0.5 scale parameter (Cauchy distributions are not available but the difference is likely trivial). The default priors for the regression intercept and slope are used: \\(N(0, 10^2)\\).\nAs you can see in Table 10, the 95%CrI for the slope excludes 0, whether looking at all studies or excluding studies at high risk of bias. This suggests the presence of publication bias."
  },
  {
    "objectID": "index.html#bayesian-model-averaging",
    "href": "index.html#bayesian-model-averaging",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.3 Bayesian model averaging",
    "text": "10.3 Bayesian model averaging\nBayesian model averaging is based on one central premise: a myriad of arguments can be made for or against using a myriad of possible Bayesian models; rather than choosing one, why not average them all?\nAs Bartos et al. put it:\n\n“In practice, researchers seldom have knowledge about the data-generating process nor do they have sufficient information to choose with confidence among the wide variety of proposed methods that aim to adjust for publication bias. Furthermore, this wide range of proposed methods often leads to contradictory conclusions. The combination of uncertainty about the data-generating process and the presence of conflicting conclusions can create a “breeding ground” for confirmation bias31: researchers may unintentionally select those methods that support the desired outcome. This freedom to choose can greatly inflate the rate of false positives, which can be a serious problem for conventional meta-analysis methods.”\n\nThe RoBMA packages generates three families of models across 12 models (interfacing with JAGS through rjags), and averages their performance. A total of 32 of these models include publication bias and 4 do not. For a description see this link. The output includes an inclusion Bayes factor. The Bayes factor is the ratio of two marginal likelihoods (i.e., two probabilities of data given certain models). We will produce Bayes factors for the mean effect, heterogeneity, and publication bias.\nThe “inclusion” part of “inclusion Bayes factor” refers to the fact we are comparing the alternative hypothesis relative to the null hypothesis. Hence our notation for the bayes factor is \\(BF_{10}\\) rather than \\(BF_{01}\\), which is another kind of Bayes factor that compares the null hypothesis relative to the alternative hypothesis. So, for the mean effect, our null model is \\(H_0: {\\mu} = 0\\) and alterative model is \\(H_1: {\\mu} ≠ 0\\). Then, the inclusion Bayes factor is:\n\\[\nBF_{10} = \\frac{p(data | H_1)}{p(data | H_0)}\n\\] Bayes factors answers the question: Are the observed data more probable under models with a particular effect, than they are under models without that particular effect? Inclusion Bayes factors &gt;3 indicate ‘substantial’ evidence against the null, while inclusion Bayes factors &lt;1/3 indicate ‘substantial’ evidence for the null.\nWe can use Bayesian model averaging for the whole meta-analysis, if we wanted to. But here we’re specifically interested in publication bias. The benefit provided by the model averaging approach is that we are provided with an inclusion Bayes factor for publication bias. A high inclusion BF for publication bias would suggest that models accounting for publication bias are more consistent with the observed data. We are also provided with an estimate that accounts for publication bias.\nPublication bias models include selection models and PET-PEESE models as methods of publication bias assessment. The publication bias adjusment prior is described in Bartoš (2021).\nTable 11 shows the results.\n\n10.3.1 Summary\n\n\n\n\n\n\nTable 11:  Results of the model-averaging approach to meta-analysis, with\nresults for models including publication bias and excluding publication\nbias. \n  \n    \n    \n      \n      Number of models\n      Prior probability\n      Posterior probability\n      Inclusion Bayes factor\n    \n  \n  \n    \n      Models accounting for publication bias\n    \n    Effect\n18\n0.50\n0.47\n0.90\n    Heterogeneity\n18\n0.50\n0.52\n1.10\n    Publication bias\n32\n0.50\n0.99\n72.67\n    \n      Models not accounting for publication bias\n    \n    Effect\n2\n0.50\n0.95\n19.76\n    Heterogeneity\n2\n0.50\n0.94\n14.49\n    Publication bias\n0\n0.00\n0.00\n0.00\n  \n  \n  \n\n\n\n\n\n\n\n10.3.2 Plots\nNow let’s plot the posterior distributions for the mean effect to visualise the effect of publication bias. In Figure 13 below, (A) shows the distribution of the model-averaged estimate with models including publication bias, and (B) is the average distribution of models that do not include publication bias.\nOur model-averaged estimates in these plots use models that assume the absence of an effect (other options includes models that assume the presence of an effect).\nThe arrows demonstrate the probability of a spike at \\(log(OR) = 0\\) (no effect).\nEvidently, including publication bias models significantly changes the results.\n\n\n\n\n\nFigure 13: ?(caption)"
  },
  {
    "objectID": "index.html#frequentist-methods-for-publication-bias",
    "href": "index.html#frequentist-methods-for-publication-bias",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.4 Frequentist methods for publication bias",
    "text": "10.4 Frequentist methods for publication bias\n\n10.4.1 Funnel plot\nBelow is the funnel plot. As you can see, there is clear asymmetry to the plot, with studies missing from the right hand side (area of no effect or harm).\n\n\n\n\n\n\n\n10.4.2 Henmi & Copas method\nRandom effects meta-analysis is not always better than fixed effect meta-analysis, even when you are certain that your population of trials do not meet the (almost invariably un-meet-able) requirement for a fixed effect meta-analysis, which is that the variability of population effects in your trials should be 0. Once instance where random effects falls down is in the allocation of weight to included trials in the presence of heterogeneity in observed effect sizes.\nFor a random effects inverse variance meta-analysis, each study is weighed according to its variance and the calculated heterogeneity:\n\\[\nw_i = \\frac{1}{v_i + \\tau}\n\\tag{8}\\]\nFor a fixed effect analysis, \\(\\tau\\) is not estimated, and the weight of studies is given by:\n\\[\nw_i = 1/v_i\n\\tag{9}\\]\nIndeed, this is partly why the random effects inverse variance meta-analysis converges to a fixed effect analysis when \\(\\tau\\) is estimated to be 0.\nHowever, in the presence of significant heterogeneity in random effects meta-analysis, Equation 8 results in the relative downweighting of larger studies and upweighting of smaller studies. This means smaller studies have a greater influence on the pooled outcome, which exacerbates any impact of publication bias.\nHenmi and Copas proposed that studies in a random effects meta-analysis be weighted according to fixed effect Equation 9. This means that \\(\\tau\\) is calculated and incorporated into the results but is not used to weight the studies. This approach was later popularised by Doi et al. as the ‘Inverse Variance Heterogeneity Model’.\nTable 12 shows the results.\n\n\n\n\n\n\nTable 12:  Results of the Henmi and Copas method for random effects\nmeta-analysis, where studies are weighed according using fixed effect\nweighting. \n  \n    \n    \n      Odds ratio\n      95% confidence interval\n      95% prediction interval\n    \n  \n  \n    0.74\n0.53, 1.05\n0.31, 1.76\n  \n  \n  \n\n\n\n\n\n\n\n10.4.3 Selection models\nSelection models are another form of publication bias assessment. These revolve around testing the hypothesis that there is a systematic ‘selection’ of low p-values relative to higher p-values. They also provide revised effect sizes that take into account the hypothesized selection model.\nWe have used the step function to create the so-called “three-parameter selection model”. The parameters are \\(\\mu\\) (mean effect size), \\({\\tau}^2\\) (heterogeneity variance), and \\(d^2\\) (likelihood of selection of p-values). We can’t really do any more complex models because we only have 16 studies.\nSee the bookdown selection model page and the metafor selection model page, and Dan Quintana’s blog for more details.\n\n\n\nRandom-Effects Model (k = 16; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 0.2837 (SE = 0.2216)\ntau (square root of estimated tau^2 value):      0.5326\n\nTest for Heterogeneity:\nLRT(df = 1) = 8.1738, p-val = 0.0043\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb    ci.ub    \n -0.9525  0.4535  -2.1003  0.0357  -1.8413  -0.0636  * \n\nTest for Selection Model Parameters:\nLRT(df = 1) = 2.3021, p-val = 0.1292\n\nSelection Model Results:\n\n                     k  estimate      se    zval    pval   ci.lb    ci.ub    \n0     &lt; p &lt;= 0.025   3    1.0000     ---     ---     ---     ---      ---    \n0.025 &lt; p &lt;= 1      13    6.1976  7.4855  0.6944  0.4875  0.0000  20.8689    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n10.4.4 Trim and fill\nNow for trim and fill analysis. This is another method of publication bias assessment that has been around a lot longer than the other examples.\nTrim and fill analysis uses the funnel plot to hypothesise the existence of ‘missing’ studies in the plot - i.e., studies that were completed but then put in the ‘file draw’ and not published because they didn’t show a significant result. It also provides a revised effect size accounting for these missing studies.\n\nFunnel\nFirst let’s look at the funnel plot to see how many of these theorised studies there are and where they may sit.\n\n\n\n\n\n\n\nRevised effect size\nNow let’s print the revised effect size from the trim and fill analysis.\n\n\n\n\n\n\n  \n    \n    \n      Odds ratio\n      95% confidence interval\n      95% prediction interval\n    \n  \n  \n    0.79\n0.60, 1.03\n0.33, 1.88"
  },
  {
    "objectID": "index.html#cumulative-meta-analysis-plots",
    "href": "index.html#cumulative-meta-analysis-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "6.4 Cumulative meta-analysis plots",
    "text": "6.4 Cumulative meta-analysis plots\nThe above analysis give good information but do not present the results succinctly to a reader. Therefore, below I’ve performed cumulative meta-analyses to show the results of the sequential analyses. These plots show the posterior of the pooled effect after adding the next study sequentially. As such, the confidence intervals should shrink with increasing studies towards the ‘true’ population mean. Figure 6 shows the findings.\n\n\n\n\n\nFigure 6: ?(caption)"
  },
  {
    "objectID": "index.html#all-studies-5",
    "href": "index.html#all-studies-5",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.3 All studies",
    "text": "7.3 All studies\n\n\n?(caption)\n\n\n\nError in \"fun(..., .envir = .envir)\": ! Could not evaluate cli `{}` expression: `str_catalog(arrange…`.\nCaused by error in `rep(paste_right(sep, \" \"), item_count - 1)`:\n! invalid 'times' argument"
  },
  {
    "objectID": "index.html#excluding-studies-at-high-risk-of-bias-5",
    "href": "index.html#excluding-studies-at-high-risk-of-bias-5",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.4 Excluding studies at high risk of bias",
    "text": "7.4 Excluding studies at high risk of bias\n\n\n?(caption)\n\n\n\nError in \"fun(..., .envir = .envir)\": ! Could not evaluate cli `{}` expression: `str_catalog(arrange…`.\nCaused by error in `rep(paste_right(sep, \" \"), item_count - 1)`:\n! invalid 'times' argument"
  },
  {
    "objectID": "index.html#all-studies-6",
    "href": "index.html#all-studies-6",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "10.2 All studies",
    "text": "10.2 All studies\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 48\n   Unobserved stochastic nodes: 37\n   Total graph size: 365\n\nInitializing model\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 33\n   Unobserved stochastic nodes: 27\n   Total graph size: 255\n\nInitializing model\n\n\n\n\n\n\nTable 10:  Bayesian regression testing to assess for small study effects. \n  \n    \n    \n      Estimate\n      95% credible interval\n    \n  \n  \n    \n      All studies\n    \n    2.99\n1.74, 4.30\n    \n      Excluding studies at high risk of bias\n    \n    3.36\n1.65, 5.08"
  },
  {
    "objectID": "index.html#excluding-studies-at-high-risk-of-bias-6",
    "href": "index.html#excluding-studies-at-high-risk-of-bias-6",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.7 Excluding studies at high risk of bias",
    "text": "7.7 Excluding studies at high risk of bias\n\n\n\n\n\nFigure 10: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE, and excluding studies at high risk of bias."
  }
]